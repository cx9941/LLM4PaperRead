{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.08008v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Hidden in plain sight: VLMs overlook their visual representations", "authors": "Stephanie Fu, Tyler Bonnen, Devin Guillory, Trevor Darrell", "abstract": "Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs.", "primary_category": "cs.CV", "categories": "cs.CV, cs.AI, cs.LG", "comment": "Project page: https://hidden-plain-sight.github.io/", "pdf_url": "http://arxiv.org/pdf/2506.08008v1", "pdf_filename": "2025-06-09_Hidden_in_plain_sight__VLMs_overlook_their_visual_.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Hidden_in_plain_sight__VLMs_overlook_their_visual_.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.08002v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Aligning Text, Images, and 3D Structure Token-by-Token", "authors": "Aadarsh Sahoo, Vansh Tibrewal, Georgia Gkioxari", "abstract": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/", "primary_category": "cs.CV", "categories": "cs.CV", "comment": "Project webpage: https://glab-caltech.github.io/kyvo/", "pdf_url": "http://arxiv.org/pdf/2506.08002v1", "pdf_filename": "2025-06-09_Aligning_Text__Images__and_3D_Structure_Token-by-T.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Aligning_Text__Images__and_3D_Structure_Token-by-T.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.08001v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation", "authors": "Zeju Qiu, Simon Buchholz, Tim Z. Xiao, Maximilian Dax, Bernhard Sch\u00f6lkopf, Weiyang Liu", "abstract": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.", "primary_category": "cs.LG", "categories": "cs.LG, cs.AI, cs.CL", "comment": "Technical report v1 (36 pages, 24 figures, project page:\n  https://spherelab.ai/poet-site/)", "pdf_url": "http://arxiv.org/pdf/2506.08001v1", "pdf_filename": "2025-06-09_Reparameterized_LLM_Training_via_Orthogonal_Equiva.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Reparameterized_LLM_Training_via_Orthogonal_Equiva.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07997v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System", "authors": "Fan Yang, Yuan Tian, Jiansong Zhang", "abstract": "The construction industry is characterized by both high physical and\npsychological risks, yet supports of mental health remain limited. While\nadvancements in artificial intelligence (AI), particularly large language\nmodels (LLMs), offer promising solutions, their potential in construction\nremains largely underexplored. To bridge this gap, we developed a\nconversational multi-agent system that addresses industry-specific challenges\nthrough an AI-driven approach integrated with domain knowledge. In parallel, it\nfulfills construction workers' basic psychological needs by enabling\ninteractions with multiple agents, each has a distinct persona. This approach\nensures that workers receive both practical problem-solving support and social\nengagement, ultimately contributing to their overall well-being. We evaluate\nits usability and effectiveness through a within-subjects user study with 12\nparticipants. The results show that our system significantly outperforms the\nsingle-agent baseline, achieving improvements of 18% in usability, 40% in\nself-determination, 60% in social presence, and 60% in trust. These findings\nhighlight the promise of LLM-driven AI systems in providing domain-specific\nsupport for construction workers.", "primary_category": "cs.HC", "categories": "cs.HC", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07997v1", "pdf_filename": "2025-06-09_Supporting_Construction_Worker_Well-Being_with_a_M.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Supporting_Construction_Worker_Well-Being_with_a_M.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07972v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization", "authors": "Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang", "abstract": "While Large Language Models (LLMs) have demonstrated significant advancements\nin reasoning and agent-based problem-solving, current evaluation methodologies\nfail to adequately assess their capabilities: existing benchmarks either rely\non closed-ended questions prone to saturation and memorization, or subjective\ncomparisons that lack consistency and rigor. In this work, we introduce\nHeuriGym, an agentic framework designed for evaluating heuristic algorithms\ngenerated by LLMs for combinatorial optimization problems, characterized by\nclearly defined objectives and expansive solution spaces. HeuriGym empowers\nLLMs to propose heuristics, receive evaluative feedback via code execution, and\niteratively refine their solutions. We evaluate nine state-of-the-art models on\nnine problems across domains such as computer systems, logistics, and biology,\nexposing persistent limitations in tool use, planning, and adaptive reasoning.\nTo quantify performance, we propose the Quality-Yield Index (QYI), a metric\nthat captures both solution pass rate and quality. Even top models like\nGPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below\nthe expert baseline of 1. Our open-source benchmark aims to guide the\ndevelopment of LLMs toward more effective and realistic problem-solving in\nscientific and engineering domains.", "primary_category": "cs.LG", "categories": "cs.LG, cs.AI, cs.CL", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07972v1", "pdf_filename": "2025-06-09_HeuriGym__An_Agentic_Benchmark_for_LLM-Crafted_Heu.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_HeuriGym__An_Agentic_Benchmark_for_LLM-Crafted_Heu.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07964v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design", "authors": "Wenxin Tang, Jingyu Xiao, Wenxuan Jiang, Xi Xiao, Yuhang Wang, Xuxin Tang, Qing Li, Yuehe Ma, Junliang Liu, Shisong Tang, Michael R. Lyu", "abstract": "Manual slide creation is labor-intensive and requires expert prior knowledge.\nExisting natural language-based LLM generation methods struggle to capture the\nvisual and structural nuances of slide designs. To address this, we formalize\nthe Reference Image to Slide Generation task and propose Slide2Code, the first\nbenchmark with difficulty-tiered samples based on a novel Slide Complexity\nMetric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework\nfor generating editable slides from reference images. SlideCoder integrates a\nColor Gradient-based Segmentation algorithm and a Hierarchical\nRetrieval-Augmented Generation method to decompose complex tasks and enhance\ncode generation. We also release SlideMaster, a 7B open-source model fine-tuned\nwith improved reverse-engineered data. Experiments show that SlideCoder\noutperforms state-of-the-art baselines by up to 40.5 points, demonstrating\nstrong performance across layout fidelity, execution accuracy, and visual\nconsistency. Our code is available at\nhttps://github.com/vinsontang1/SlideCoder.", "primary_category": "cs.CV", "categories": "cs.CV, cs.AI", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07964v1", "pdf_filename": "2025-06-09_SlideCoder__Layout-aware_RAG-enhanced_Hierarchical.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_SlideCoder__Layout-aware_RAG-enhanced_Hierarchical.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07963v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Reinforcing Multimodal Understanding and Generation with Dual Self-rewards", "authors": "Jixiang Hong, Yiran Zhang, Guanzhong Wang, Yi Liu, Ji-Rong Wen, Rui Yan", "abstract": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate image-text alignment, prone to\ngenerating text responses contradicting the visual input or failing to follow\nthe text-to-image prompts. Current solutions require external supervision\n(e.g., human feedback or reward models) and only address unidirectional\ntasks-either understanding or generation. In this work, based on the\nobservation that understanding and generation are inverse dual tasks, we\nintroduce a self-supervised dual reward mechanism to reinforce the\nunderstanding and generation capabilities of LMMs. Specifically, we sample\nmultiple outputs for a given input in one task domain, then reverse the\ninput-output pairs to compute the dual likelihood of the model as self-rewards\nfor optimization. Extensive experimental results on visual understanding and\ngeneration benchmarks demonstrate that our method can effectively enhance the\nperformance of the model without any external supervision, especially achieving\nremarkable improvements in text-to-image tasks.", "primary_category": "cs.AI", "categories": "cs.AI, cs.CL, cs.CV", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07963v1", "pdf_filename": "2025-06-09_Reinforcing_Multimodal_Understanding_and_Generatio.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Reinforcing_Multimodal_Understanding_and_Generatio.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07962v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Correlated Errors in Large Language Models", "authors": "Elliot Kim, Avi Garg, Kenny Peng, Nikhil Garg", "abstract": "Diversity in training data, architecture, and providers is assumed to\nmitigate homogeneity in LLMs. However, we lack empirical evidence on whether\ndifferent LLMs differ meaningfully. We conduct a large-scale empirical\nevaluation on over 350 LLMs overall, using two popular leaderboards and a\nresume-screening task. We find substantial correlation in model errors -- on\none leaderboard dataset, models agree 60% of the time when both models err. We\nidentify factors driving model correlation, including shared architectures and\nproviders. Crucially, however, larger and more accurate models have highly\ncorrelated errors, even with distinct architectures and providers. Finally, we\nshow the effects of correlation in two downstream tasks: LLM-as-judge\nevaluation and hiring -- the latter reflecting theoretical predictions\nregarding algorithmic monoculture.", "primary_category": "cs.CL", "categories": "cs.CL, cs.AI, cs.CY, stat.ML", "comment": "Accepted to ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.07962v1", "pdf_filename": "2025-06-09_Correlated_Errors_in_Large_Language_Models.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Correlated_Errors_in_Large_Language_Models.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07948v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "TokenBreak: Bypassing Text Classification Models Through Token Manipulation", "authors": "Kasimir Schulz, Kenneth Yeung, Kieran Evans", "abstract": "Natural Language Processing (NLP) models are used for text-related tasks such\nas classification and generation. To complete these tasks, input data is first\ntokenized from human-readable text into a format the model can understand,\nenabling it to make inferences and understand context. Text classification\nmodels can be implemented to guard against threats such as prompt injection\nattacks against Large Language Models (LLMs), toxic input and cybersecurity\nrisks such as spam emails. In this paper, we introduce TokenBreak: a novel\nattack that can bypass these protection models by taking advantage of the\ntokenization strategy they use. This attack technique manipulates input text in\nsuch a way that certain models give an incorrect classification. Importantly,\nthe end target (LLM or email recipient) can still understand and respond to the\nmanipulated text and therefore be vulnerable to the very attack the protection\nmodel was put in place to prevent. The tokenizer is tied to model architecture,\nmeaning it is possible to predict whether or not a model is vulnerable to\nattack based on family. We also present a defensive strategy as an added layer\nof protection that can be implemented without having to retrain the defensive\nmodel.", "primary_category": "cs.LG", "categories": "cs.LG, cs.CR", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07948v1", "pdf_filename": "2025-06-09_TokenBreak__Bypassing_Text_Classification_Models_T.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_TokenBreak__Bypassing_Text_Classification_Models_T.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07947v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Statistical Hypothesis Testing for Auditing Robustness in Language Models", "authors": "Paulius Rauba, Qiyao Wei, Mihaela van der Schaar", "abstract": "Consider the problem of testing whether the outputs of a large language model\n(LLM) system change under an arbitrary intervention, such as an input\nperturbation or changing the model variant. We cannot simply compare two LLM\noutputs since they might differ due to the stochastic nature of the system, nor\ncan we compare the entire output distribution due to computational\nintractability. While existing methods for analyzing text-based outputs exist,\nthey focus on fundamentally different problems, such as measuring bias or\nfairness. To this end, we introduce distribution-based perturbation analysis, a\nframework that reformulates LLM perturbation analysis as a frequentist\nhypothesis testing problem. We construct empirical null and alternative output\ndistributions within a low-dimensional semantic similarity space via Monte\nCarlo sampling, enabling tractable inference without restrictive distributional\nassumptions. The framework is (i) model-agnostic, (ii) supports the evaluation\nof arbitrary input perturbations on any black-box LLM, (iii) yields\ninterpretable p-values; (iv) supports multiple perturbations via controlled\nerror rates; and (v) provides scalar effect sizes. We demonstrate the\nusefulness of the framework across multiple case studies, showing how we can\nquantify response changes, measure true/false positive rates, and evaluate\nalignment with reference models. Above all, we see this as a reliable\nfrequentist hypothesis testing framework for LLM auditing.", "primary_category": "cs.CL", "categories": "cs.CL", "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00868", "pdf_url": "http://arxiv.org/pdf/2506.07947v1", "pdf_filename": "2025-06-09_Statistical_Hypothesis_Testing_for_Auditing_Robust.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Statistical_Hypothesis_Testing_for_Auditing_Robust.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07945v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols", "authors": "Arnav Sheth, Ivaxi Sheth, Mario Fritz", "abstract": "Recent advances in Large Language Models (LLMs) have shown promising\ncapabilities in generating code for general-purpose programming languages. In\ncontrast, their applicability for hardware description languages, particularly\nfor generating synthesizable and functionally correct designs, remains\nsignificantly underexplored. HDLs such as SystemVerilog are logic-oriented and\ndemand strict adherence to timing semantics, concurrency, and synthesizability\nconstraints. Moreover, HDL-based design flows encompass a broad set of tasks\nbeyond structural code generation, including testbench development,\nassertion-based verification, timing closure, and protocol-level integration\nfor on-chip communication. The objective of our paper is to analyze the\ncapabilities of state-of-the-art LLMs in generating SystemVerilog\nimplementations of standard communication protocols, a core component of\nembedded and System-on-Chip (SoC) architectures. This paper introduces the\nfirst benchmark suite targeting four widely used protocols: SPI, I2C, UART, and\nAXI. We define code generation tasks that capture varying levels of design\nabstraction and prompt specificity. The generated designs are assessed for\nsyntactic correctness, synthesizability, and functional fidelity via waveform\nsimulation and test benches.", "primary_category": "cs.AR", "categories": "cs.AR, cs.AI, cs.CL", "comment": "Accepted at MLSysArch@ISCA 2025", "pdf_url": "http://arxiv.org/pdf/2506.07945v1", "pdf_filename": "2025-06-09_ProtocolLLM__RTL_Benchmark_for_SystemVerilog_Gener.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_ProtocolLLM__RTL_Benchmark_for_SystemVerilog_Gener.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07943v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations", "authors": "Yizhen Li, Dell Zhang, Xuelong Li, Yiqing Shen", "abstract": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.", "primary_category": "cs.CV", "categories": "cs.CV, cs.AI", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07943v1", "pdf_filename": "2025-06-09_Decoupling_the_Image_Perception_and_Multimodal_Rea.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Decoupling_the_Image_Perception_and_Multimodal_Rea.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07942v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Adversarial Attack Classification and Robustness Testing for Large Language Models for Code", "authors": "Yang Liu, Armstrong Foundjem, Foutse Khomh, Heng Li", "abstract": "Large Language Models (LLMs) have become vital tools in software development\ntasks such as code generation, completion, and analysis. As their integration\ninto workflows deepens, ensuring robustness against vulnerabilities especially\nthose triggered by diverse or adversarial inputs becomes increasingly\nimportant. Such vulnerabilities may lead to incorrect or insecure code\ngeneration when models encounter perturbed task descriptions, code, or\ncomments. Prior research often overlooks the role of natural language in\nguiding code tasks. This study investigates how adversarial perturbations in\nnatural language inputs including prompts, comments, and descriptions affect\nLLMs for Code (LLM4Code). It examines the effects of perturbations at the\ncharacter, word, and sentence levels to identify the most impactful\nvulnerabilities. We analyzed multiple projects (e.g., ReCode, OpenAttack) and\ndatasets (e.g., HumanEval, MBPP), establishing a taxonomy of adversarial\nattacks. The first dimension classifies the input type code, prompts, or\ncomments while the second dimension focuses on granularity: character, word, or\nsentence-level changes. We adopted a mixed-methods approach, combining\nquantitative performance metrics with qualitative vulnerability analysis.\nLLM4Code models show varying robustness across perturbation types.\nSentence-level attacks were least effective, suggesting models are resilient to\nbroader contextual changes. In contrast, word-level perturbations posed serious\nchallenges, exposing semantic vulnerabilities. Character-level effects varied,\nshowing model sensitivity to subtle syntactic deviations.Our study offers a\nstructured framework for testing LLM4Code robustness and emphasizes the\ncritical role of natural language in adversarial evaluation. Improving model\nresilience to semantic-level disruptions is essential for secure and reliable\ncode-generation systems.", "primary_category": "cs.SE", "categories": "cs.SE", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07942v1", "pdf_filename": "2025-06-09_Adversarial_Attack_Classification_and_Robustness_T.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Adversarial_Attack_Classification_and_Robustness_T.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07927v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Solving Inequality Proofs with Large Language Models", "authors": "Jiayi Sheng, Luna Lyu, Jikai Jin, Tony Xia, Alex Gu, James Zou, Pan Lu", "abstract": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.", "primary_category": "cs.AI", "categories": "cs.AI, cs.CL, cs.LG", "comment": "52 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2506.07927v1", "pdf_filename": "2025-06-09_Solving_Inequality_Proofs_with_Large_Language_Mode.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Solving_Inequality_Proofs_with_Large_Language_Mode.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07915v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement", "authors": "Dimitris Panagopoulos, Adolfo Perrusquia, Weisi Guo", "abstract": "In dynamic environments, the rapid obsolescence of pre-existing environmental\nknowledge creates a gap between an agent's internal model and the evolving\nreality of its operational context. This disparity between prior and updated\nenvironmental valuations fundamentally limits the effectiveness of autonomous\ndecision-making. To bridge this gap, the contextual bias of human domain\nstakeholders, who naturally accumulate insights through direct, real-time\nobservation, becomes indispensable. However, translating their nuanced, and\ncontext-rich input into actionable intelligence for autonomous systems remains\nan open challenge. To address this, we propose LUCIFER (Language Understanding\nand Context-Infused Framework for Exploration and Behavior Refinement), a\ndomain-agnostic framework that integrates a hierarchical decision-making\narchitecture with reinforcement learning (RL) and large language models (LLMs)\ninto a unified system. This architecture mirrors how humans decompose complex\ntasks, enabling a high-level planner to coordinate specialised sub-agents, each\nfocused on distinct objectives and temporally interdependent actions. Unlike\ntraditional applications where LLMs are limited to single role, LUCIFER\nintegrates them in two synergistic roles: as context extractors, structuring\nverbal stakeholder input into domain-aware representations that influence\ndecision-making through an attention space mechanism aligning LLM-derived\ninsights with the agent's learning process, and as zero-shot exploration\nfacilitators guiding the agent's action selection process during exploration.\nWe benchmark various LLMs in both roles and demonstrate that LUCIFER improves\nexploration efficiency and decision quality, outperforming flat,\ngoal-conditioned policies. Our findings show the potential of context-driven\ndecision-making, where autonomous systems leverage human contextual knowledge\nfor operational success.", "primary_category": "cs.AI", "categories": "cs.AI, cs.CL, cs.SY, eess.SY", "comment": "12 pages, 4 Figures, 3 Tables, submitted to the IEEE for possible\n  publication", "pdf_url": "http://arxiv.org/pdf/2506.07915v1", "pdf_filename": "2025-06-09_LUCIFER__Language_Understanding_and_Context-Infuse.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_LUCIFER__Language_Understanding_and_Context-Infuse.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07900v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices", "authors": "MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun", "abstract": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.", "primary_category": "cs.CL", "categories": "cs.CL, cs.AI", "comment": "MiniCPM4 Technical Report", "pdf_url": "http://arxiv.org/pdf/2506.07900v1", "pdf_filename": "2025-06-09_MiniCPM4__Ultra-Efficient_LLMs_on_End_Devices.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_MiniCPM4__Ultra-Efficient_LLMs_on_End_Devices.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07899v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs", "authors": "Ke Wang, Yiming Qin, Nikolaos Dimitriadis, Alessandro Favero, Pascal Frossard", "abstract": "Language models deployed in real-world systems often require post-hoc updates\nto incorporate new or corrected knowledge. However, editing such models\nefficiently and reliably - without retraining or forgetting previous\ninformation - remains a major challenge. Existing methods for lifelong model\nediting either compromise generalization, interfere with past edits, or fail to\nscale to long editing sequences. We propose MEMOIR, a novel scalable framework\nthat injects knowledge through a residual memory, i.e., a dedicated parameter\nmodule, while preserving the core capabilities of the pre-trained model. By\nsparsifying input activations through sample-dependent masks, MEMOIR confines\neach edit to a distinct subset of the memory parameters, minimizing\ninterference among edits. At inference, it identifies relevant edits by\ncomparing the sparse activation patterns of new queries to those stored during\nediting. This enables generalization to rephrased queries by activating only\nthe relevant knowledge while suppressing unnecessary memory activation for\nunrelated prompts. Experiments on question answering, hallucination correction,\nand out-of-distribution generalization benchmarks across LLaMA-3 and Mistral\ndemonstrate that MEMOIR achieves state-of-the-art performance across\nreliability, generalization, and locality metrics, scaling to thousands of\nsequential edits with minimal forgetting.", "primary_category": "cs.CL", "categories": "cs.CL, cs.LG", "comment": "The first two authors contributed equally to this work", "pdf_url": "http://arxiv.org/pdf/2506.07899v1", "pdf_filename": "2025-06-09_MEMOIR__Lifelong_Model_Editing_with_Minimal_Overwr.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_MEMOIR__Lifelong_Model_Editing_with_Minimal_Overwr.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07896v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark", "authors": "Shoko Oka", "abstract": "Recent advancements in large language models (LLMs) have revitalized\nphilosophical debates surrounding artificial intelligence. Two of the most\nfundamental challenges - namely, the Frame Problem and the Symbol Grounding\nProblem - have historically been viewed as unsolvable within traditional\nsymbolic AI systems. This study investigates whether modern LLMs possess the\ncognitive capacities required to address these problems. To do so, I designed\ntwo benchmark tasks reflecting the philosophical core of each problem,\nadministered them under zero-shot conditions to 13 prominent LLMs (both closed\nand open-source), and assessed the quality of the models' outputs across five\ntrials each. Responses were scored along multiple criteria, including\ncontextual reasoning, semantic coherence, and information filtering. The\nresults demonstrate that while open-source models showed variability in\nperformance due to differences in model size, quantization, and instruction\ntuning, several closed models consistently achieved high scores. These findings\nsuggest that select modern LLMs may be acquiring capacities sufficient to\nproduce meaningful and stable responses to these long-standing theoretical\nchallenges.", "primary_category": "cs.AI", "categories": "cs.AI, cs.CL", "comment": "52 pages, Additional resources available on GitHub repository", "pdf_url": "http://arxiv.org/pdf/2506.07896v1", "pdf_filename": "2025-06-09_Evaluating_Large_Language_Models_on_the_Frame_and_.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Evaluating_Large_Language_Models_on_the_Frame_and_.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07888v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark", "authors": "Rui Wen, Yiyong Liu, Michael Backes, Yang Zhang", "abstract": "Data reconstruction attacks, which aim to recover the training dataset of a\ntarget model with limited access, have gained increasing attention in recent\nyears. However, there is currently no consensus on a formal definition of data\nreconstruction attacks or appropriate evaluation metrics for measuring their\nquality. This lack of rigorous definitions and universal metrics has hindered\nfurther advancement in this field. In this paper, we address this issue in the\nvision domain by proposing a unified attack taxonomy and formal definitions of\ndata reconstruction attacks. We first propose a set of quantitative evaluation\nmetrics that consider important criteria such as quantifiability, consistency,\nprecision, and diversity. Additionally, we leverage large language models\n(LLMs) as a substitute for human judgment, enabling visual evaluation with an\nemphasis on high-quality reconstructions. Using our proposed taxonomy and\nmetrics, we present a unified framework for systematically evaluating the\nstrengths and limitations of existing attacks and establishing a benchmark for\nfuture research. Empirical results, primarily from a memorization perspective,\nnot only validate the effectiveness of our metrics but also offer valuable\ninsights for designing new attacks.", "primary_category": "cs.CR", "categories": "cs.CR, cs.LG", "comment": "To Appear in the 34th USENIX Security Symposium, August 13-15, 2025", "pdf_url": "http://arxiv.org/pdf/2506.07888v1", "pdf_filename": "2025-06-09_SoK__Data_Reconstruction_Attacks_Against_Machine_L.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_SoK__Data_Reconstruction_Attacks_Against_Machine_L.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07851v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning", "authors": "Yiju Guo, Wenkai Yang, Zexu Sun, Ning Ding, Zhiyuan Liu, Yankai Lin", "abstract": "Large language models (LLMs) have demonstrated significant improvements in\ncontextual understanding. However, their ability to attend to truly critical\ninformation during long-context reasoning and generation still falls behind the\npace. Specifically, our preliminary experiments reveal that certain distracting\npatterns can misdirect the model's attention during inference, and removing\nthese patterns substantially improves reasoning accuracy and generation\nquality. We attribute this phenomenon to spurious correlations in the training\ndata, which obstruct the model's capacity to infer authentic causal\ninstruction-response relationships. This phenomenon may induce redundant\nreasoning processes, potentially resulting in significant inference overhead\nand, more critically, the generation of erroneous or suboptimal responses. To\nmitigate this, we introduce a two-stage framework called Learning to Focus\n(LeaF) leveraging intervention-based inference to disentangle confounding\nfactors. In the first stage, LeaF employs gradient-based comparisons with an\nadvanced teacher to automatically identify confounding tokens based on causal\nrelationships in the training corpus. Then, in the second stage, it prunes\nthese tokens during distillation to enact intervention, aligning the student's\nattention with the teacher's focus distribution on truly critical context\ntokens. Experimental results demonstrate that LeaF not only achieves an\nabsolute improvement in various mathematical reasoning and code generation\nbenchmarks but also effectively suppresses attention to confounding tokens\nduring inference, yielding a more interpretable and reliable reasoning model.", "primary_category": "cs.CL", "categories": "cs.CL", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07851v1", "pdf_filename": "2025-06-09_Learning_to_Focus__Causal_Attention_Distillation_v.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Learning_to_Focus__Causal_Attention_Distillation_v.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07833v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Improving large language models with concept-aware fine-tuning", "authors": "Michael K. Chen, Xikun Zhang, Jiaxing Huang, Dacheng Tao", "abstract": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm", "primary_category": "cs.LG", "categories": "cs.LG, cs.AI, cs.CL", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07833v1", "pdf_filename": "2025-06-09_Improving_large_language_models_with_concept-aware.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Improving_large_language_models_with_concept-aware.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07824v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs", "authors": "Yao Yan", "abstract": "Multi-digit addition is a clear probe of the computational power of large\nlanguage models. To dissect the internal arithmetic processes in\nLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.\nInspired by the step-by-step manner in which humans perform addition, we\npropose and analyze a coherent four-stage trajectory in the forward\npass:Formula-structure representations become linearly decodable first, while\nthe answer token is still far down the candidate list.Core computational\nfeatures then emerge prominently.At deeper activation layers, numerical\nabstractions of the result become clearer, enabling near-perfect detection and\ndecoding of the individual digits in the sum.Near the output, the model\norganizes and generates the final content, with the correct token reliably\noccupying the top rank.This trajectory suggests a hierarchical process that\nfavors internal computation over rote memorization. We release our code and\ndata to facilitate reproducibility.", "primary_category": "cs.AI", "categories": "cs.AI", "comment": "12 pages, including appendix, 7 figures. EMNLP 2025 submission (ARR\n  May 2025 cycle, reviews pending)", "pdf_url": "http://arxiv.org/pdf/2506.07824v1", "pdf_filename": "2025-06-09_Addition_in_Four_Movements__Mapping_Layer-wise_Inf.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Addition_in_Four_Movements__Mapping_Layer-wise_Inf.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07820v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation", "authors": "Jiaxiang CHen, Zhuo Wang, Mingxi Zou, Qifan Wang, Zenglin Xu", "abstract": "Human reasoning is flexible, adaptive, and grounded in prior\nexperience-qualities that large language models (LLMs) still struggle to\nemulate. Existing methods either explore diverse reasoning paths at inference\ntime or search for optimal workflows through expensive operations, but both\nfall short in leveraging multiple reusable strategies in a structured,\nefficient manner. We propose Guideline Forest, a framework that enhances LLMs\nreasoning by inducing structured reasoning strategies-called guidelines-from\nverified examples and executing them via step-wise aggregation. Unlike\ntest-time search or single-path distillation, our method draws on verified\nreasoning experiences by inducing reusable guidelines and expanding each into\ndiverse variants. Much like human reasoning, these variants reflect alternative\nthought patterns, are executed in parallel, refined via self-correction, and\naggregated step by step-enabling the model to adaptively resolve uncertainty\nand synthesize robust solutions.We evaluate Guideline Forest on four\nbenchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and\nprogrammatic reasoning. Guideline Forest consistently outperforms strong\nbaselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further\nhighlight the effectiveness of multi-path reasoning and stepwise aggregation,\nunderscoring the Guideline Forest's adaptability and generalization potential.", "primary_category": "cs.AI", "categories": "cs.AI", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07820v1", "pdf_filename": "2025-06-09_Guideline_Forest__Experience-Induced_Multi-Guideli.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Guideline_Forest__Experience-Induced_Multi-Guideli.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07795v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "LLM Unlearning Should Be Form-Independent", "authors": "Xiaotian Ye, Mengqi Zhang, Shu Wu", "abstract": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.", "primary_category": "cs.CL", "categories": "cs.CL, cs.CR, cs.LG", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07795v1", "pdf_filename": "2025-06-09_LLM_Unlearning_Should_Be_Form-Independent.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_LLM_Unlearning_Should_Be_Form-Independent.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07778v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Language-Vision Planner and Executor for Text-to-Visual Reasoning", "authors": "Yichang Xu, Gaowen Liu, Ramana Rao Kompella, Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Furkan Tekin, Zachary Yahn, Ling Liu", "abstract": "The advancement in large language models (LLMs) and large vision models has\nfueled the rapid progress in multi-modal visual-text reasoning capabilities.\nHowever, existing vision-language models (VLMs) to date suffer from\ngeneralization performance. Inspired by recent development in LLMs for visual\nreasoning, this paper presents VLAgent, an AI system that can create a\nstep-by-step visual reasoning plan with an easy-to-understand script and\nexecute each step of the plan in real time by integrating planning script with\nexecution verifications via an automated process supported by VLAgent. In the\ntask planning phase, VLAgent fine-tunes an LLM through in-context learning to\ngenerate a step-by-step planner for each user-submitted text-visual reasoning\ntask. During the plan execution phase, VLAgent progressively refines the\ncomposition of neuro-symbolic executable modules to generate high-confidence\nreasoning results. VLAgent has three unique design characteristics: First, we\nimprove the quality of plan generation through in-context learning, improving\nlogic reasoning by reducing erroneous logic steps, incorrect programs, and LLM\nhallucinations. Second, we design a syntax-semantics parser to identify and\ncorrect additional logic errors of the LLM-generated planning script prior to\nlaunching the plan executor. Finally, we employ the ensemble method to improve\nthe generalization performance of our step-executor. Extensive experiments with\nfour visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgent\nachieves significant performance enhancement for multimodal text-visual\nreasoning applications, compared to the exiting representative VLMs and LLM\nbased visual composition approaches like ViperGPT and VisProg, thanks to the\nnovel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer,\nOutput Verifiers). Code and data will be made available upon paper acceptance.", "primary_category": "cs.CV", "categories": "cs.CV", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07778v1", "pdf_filename": "2025-06-09_Language-Vision_Planner_and_Executor_for_Text-to-V.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Language-Vision_Planner_and_Executor_for_Text-to-V.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07759v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models", "authors": "Diego Forni\u00e9s-Tabuenca, Alejandro Uribe, Urtzi Otamendi, Arkaitz Artetxe, Juan Carlos Rivera, Oier Lopez de Lacalle", "abstract": "Multi-objective optimization is fundamental in complex decision-making tasks.\nTraditional algorithms, while effective, often demand extensive\nproblem-specific modeling and struggle to adapt to nonlinear structures. Recent\nadvances in Large Language Models (LLMs) offer enhanced explainability,\nadaptability, and reasoning. This work proposes Reflective Evolution of\nMulti-objective Heuristics (REMoH), a novel framework integrating NSGA-II with\nLLM-based heuristic generation. A key innovation is a reflection mechanism that\nuses clustering and search-space reflection to guide the creation of diverse,\nhigh-quality heuristics, improving convergence and maintaining solution\ndiversity. The approach is evaluated on the Flexible Job Shop Scheduling\nProblem (FJSSP) in-depth benchmarking against state-of-the-art methods using\nthree instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate\nthat REMoH achieves competitive results compared to state-of-the-art approaches\nwith reduced modeling effort and enhanced adaptability. These findings\nunderscore the potential of LLMs to augment traditional optimization, offering\ngreater flexibility, interpretability, and robustness in multi-objective\nscenarios.", "primary_category": "cs.AI", "categories": "cs.AI, cs.NE, I.2.7; I.2.8; F.2.2", "comment": "21 pages, 5 tables, 7 figures and 4 appendixes. Pre-print submitted\n  to IEEE Transactions on Evolutionary Computation", "pdf_url": "http://arxiv.org/pdf/2506.07759v1", "pdf_filename": "2025-06-09_REMoH__A_Reflective_Evolution_of_Multi-objective_H.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_REMoH__A_Reflective_Evolution_of_Multi-objective_H.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07751v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking", "authors": "Silin Gao, Antoine Bosselut, Samy Bengio, Emmanuel Abbe", "abstract": "Recent studies have shown that large language models (LLMs), especially\nsmaller ones, often lack robustness in their reasoning. I.e., they tend to\nexperience performance drops when faced with distribution shifts, such as\nchanges to numerical or nominal variables, or insertions of distracting\nclauses. A possible strategy to address this involves generating synthetic data\nto further \"instantiate\" reasoning problems on potential variations. In\ncontrast, our approach focuses on \"abstracting\" reasoning problems. This not\nonly helps counteract distribution shifts but also facilitates the connection\nto symbolic tools for deriving solutions. We find that this abstraction process\nis better acquired through reinforcement learning (RL) than just supervised\nfine-tuning, which often fails to produce faithful abstractions. Our method,\nAbstraL -- which promotes abstract reasoning in LLMs using RL on granular\nabstraction data -- significantly mitigates performance degradation on recent\nGSM perturbation benchmarks.", "primary_category": "cs.CL", "categories": "cs.CL, cs.AI, cs.SC", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2506.07751v1", "pdf_filename": "2025-06-09_Augmenting_LLMs__Reasoning_by_Reinforcing_Abstract.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Augmenting_LLMs__Reasoning_by_Reinforcing_Abstract.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07748v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Research quality evaluation by AI in the era of Large Language Models: Advantages, disadvantages, and systemic effects", "authors": "Mike Thelwall", "abstract": "Artificial Intelligence (AI) technologies like ChatGPT now threaten\nbibliometrics as the primary generators of research quality indicators. They\nare already used in at least one research quality evaluation system and\nevidence suggests that they are used informally by many peer reviewers. Since\nusing bibliometrics to support research evaluation continues to be\ncontroversial, this article reviews the corresponding advantages and\ndisadvantages of AI-generated quality scores. From a technical perspective,\ngenerative AI based on Large Language Models (LLMs) equals or surpasses\nbibliometrics in most important dimensions, including accuracy (mostly higher\ncorrelations with human scores), and coverage (more fields, more recent years)\nand may reflect more research quality dimensions. Like bibliometrics, current\nLLMs do not \"measure\" research quality, however. On the clearly negative side,\nLLM biases are currently unknown for research evaluation, and LLM scores are\nless transparent than citation counts. From a systemic perspective, the key\nissue is how introducing LLM-based indicators into research evaluation will\nchange the behaviour of researchers. Whilst bibliometrics encourage some\nauthors to target journals with high impact factors or to try to write highly\ncited work, LLM-based indicators may push them towards writing misleading\nabstracts and overselling their work in the hope of impressing the AI.\nMoreover, if AI-generated journal indicators replace impact factors, then this\nwould encourage journals to allow authors to oversell their work in abstracts,\nthreatening the integrity of the academic record.", "primary_category": "cs.DL", "categories": "cs.DL", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07748v1", "pdf_filename": "2025-06-09_Research_quality_evaluation_by_AI_in_the_era_of_La.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Research_quality_evaluation_by_AI_in_the_era_of_La.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07736v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards", "authors": "Jingnan Zheng, Xiangtian Ji, Yijun Lu, Chenhang Cui, Weixiang Zhao, Gelei Deng, Zhenkai Liang, An Zhang, Tat-Seng Chua", "abstract": "Large Language Models (LLMs) continue to exhibit vulnerabilities despite\ndeliberate safety alignment efforts, posing significant risks to users and\nsociety. To safeguard against the risk of policy-violating content,\nsystem-level moderation via external guard models-designed to monitor LLM\ninputs and outputs and block potentially harmful content-has emerged as a\nprevalent mitigation strategy. Existing approaches of training guard models\nrely heavily on extensive human curated datasets and struggle with\nout-of-distribution threats, such as emerging harmful categories or jailbreak\nattacks. To address these limitations, we propose RSafe, an adaptive\nreasoning-based safeguard that conducts guided safety reasoning to provide\nrobust protection within the scope of specified safety policies. RSafe operates\nin two stages: 1) guided reasoning, where it analyzes safety risks of input\ncontent through policy-guided step-by-step reasoning, and 2) reinforced\nalignment, where rule-based RL optimizes its reasoning paths to align with\naccurate safety prediction. This two-stage training paradigm enables RSafe to\ninternalize safety principles to generalize safety protection capability over\nunseen or adversarial safety violation scenarios. During inference, RSafe\naccepts user-specified safety policies to provide enhanced safeguards tailored\nto specific safety requirements.", "primary_category": "cs.AI", "categories": "cs.AI", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07736v1", "pdf_filename": "2025-06-09_RSafe__Incentivizing_proactive_reasoning_to_build_.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_RSafe__Incentivizing_proactive_reasoning_to_build_.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07735v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning", "authors": "Haizhao Jing, Haokui Zhang, Zhenhao Shang, Rong Xiao, Peng Wang, Yanning Zhang", "abstract": "Neural Architecture Representation Learning aims to transform network models\ninto feature representations for predicting network attributes, playing a\ncrucial role in deploying and designing networks for real-world applications.\nRecently, inspired by the success of transformers, transformer-based models\nintegrated with Graph Neural Networks (GNNs) have achieved significant progress\nin representation learning. However, current methods still have some\nlimitations. First, existing methods overlook hardware attribute information,\nwhich conflicts with the current trend of diversified deep learning hardware\nand limits the practical applicability of models. Second, current encoding\napproaches rely on static adjacency matrices to represent topological\nstructures, failing to capture the structural differences between computational\nnodes, which ultimately compromises encoding effectiveness. In this paper, we\nintroduce LeDG-Former, an innovative framework that addresses these limitations\nthrough the synergistic integration of language-based semantic embedding and\ndynamic graph representation learning. Specifically, inspired by large language\nmodels (LLMs), we propose a language embedding framework where both neural\narchitectures and hardware platform specifications are projected into a unified\nsemantic space through tokenization and LLM processing, enabling zero-shot\nprediction across different hardware platforms for the first time. Then, we\npropose a dynamic graph-based transformer for modeling neural architectures,\nresulting in improved neural architecture modeling performance. On the NNLQP\nbenchmark, LeDG-Former surpasses previous methods, establishing a new SOTA\nwhile demonstrating the first successful cross-hardware latency prediction\ncapability. Furthermore, our framework achieves superior performance on the\ncell-structured NAS-Bench-101 and NAS-Bench-201 datasets.", "primary_category": "cs.LG", "categories": "cs.LG, cs.CV", "comment": "9 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2506.07735v1", "pdf_filename": "2025-06-09_Language_Embedding_Meets_Dynamic_Graph__A_New_Expl.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Language_Embedding_Meets_Dynamic_Graph__A_New_Expl.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07731v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models", "authors": "Mouadh Yagoubi, Yasser Dahou, Billel Mokeddem, Younes Belkada, Phuc H. Le-Khac, Basma El Amel Boussaha, Reda Alami, Jingwei Zuo, Damiano Marsili, Mugariya Farooq, Mounia Lalmas, Georgia Gkioxari, Patrick Gallinari, Philip Torr, Hakim Hacid", "abstract": "Existing benchmarks have proven effective for assessing the performance of\nfully trained large language models. However, we find striking differences in\nthe early training stages of small models, where benchmarks often fail to\nprovide meaningful or discriminative signals. To explore how these differences\narise, this competition tackles the challenge of designing scientific knowledge\nevaluation tasks specifically tailored for measuring early training progress of\nlanguage models. Participants are invited to develop novel evaluation\nmethodologies or adapt existing benchmarks to better capture performance\ndifferences among language models. To support this effort, we provide three\npre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate\ncheckpoints sampled during training up to 200B tokens. All experiments and\ndevelopment work can be run on widely available free cloud-based GPU platforms,\nmaking participation accessible to researchers with limited computational\nresources. Submissions will be evaluated based on three criteria: the quality\nof the performance signal they produce, the consistency of model rankings at 1\ntrillion tokens of training, and their relevance to the scientific knowledge\ndomain. By promoting the design of tailored evaluation strategies for early\ntraining, this competition aims to attract a broad range of participants from\nvarious disciplines, including those who may not be machine learning experts or\nhave access to dedicated GPU resources. Ultimately, this initiative seeks to\nmake foundational LLM research more systematic and benchmark-informed from the\nearliest phases of model development.", "primary_category": "cs.AI", "categories": "cs.AI", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07731v1", "pdf_filename": "2025-06-09_NeurIPS_2025_E2LM_Competition___Early_Training_Eva.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_NeurIPS_2025_E2LM_Competition___Early_Training_Eva.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07726v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU", "authors": "Vincenzo Timmel, Manfred Vogel, Daniel Perruchoud, Reza Kakooee", "abstract": "This paper presents a new long-form release of the Swiss Parliaments Corpus,\nconverting entire multi-hour Swiss German debate sessions (each aligned with\nthe official session protocols) into high-quality speech-text pairs. Our\npipeline starts by transcribing all session audio into Standard German using\nWhisper Large-v3 under high-compute settings. We then apply a two-step GPT-4o\ncorrection process: first, GPT-4o ingests the raw Whisper output alongside the\nofficial protocols to refine misrecognitions, mainly named entities. Second, a\nseparate GPT-4o pass evaluates each refined segment for semantic completeness.\nWe filter out any segments whose Predicted BLEU score (derived from Whisper's\naverage token log-probability) and GPT-4o evaluation score fall below a certain\nthreshold. The final corpus contains 801 hours of audio, of which 751 hours\npass our quality control. Compared to the original sentence-level SPC release,\nour long-form dataset achieves a 6-point BLEU improvement, demonstrating the\npower of combining robust ASR, LLM-based correction, and data-driven filtering\nfor low-resource, domain-specific speech corpora.", "primary_category": "cs.CL", "categories": "cs.CL", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07726v1", "pdf_filename": "2025-06-09_Swiss_Parliaments_Corpus_Re-Imagined__SPC_R___Enha.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Swiss_Parliaments_Corpus_Re-Imagined__SPC_R___Enha.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07707v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Interaction Analysis by Humans and AI: A Comparative Perspective", "authors": "Maryam Teimouri, Filip Ginter, Tomi \"bgt\" Suovuo", "abstract": "This paper explores how Mixed Reality (MR) and 2D video conferencing\ninfluence children's communication during a gesture-based guessing game.\nFinnish-speaking participants engaged in a short collaborative task using two\ndifferent setups: Microsoft HoloLens MR and Zoom. Audio-video recordings were\ntranscribed and analyzed using Large Language Models (LLMs), enabling iterative\ncorrection, translation, and annotation. Despite limitations in annotations'\naccuracy and agreement, automated approaches significantly reduced processing\ntime and allowed non-Finnish-speaking researchers to participate in data\nanalysis. Evaluations highlight both the efficiency and constraints of\nLLM-based analyses for capturing children's interactions across these\nplatforms. Initial findings indicate that MR fosters richer interaction,\nevidenced by higher emotional expression during annotation, and heightened\nengagement, while Zoom offers simplicity and accessibility. This study\nunderscores the potential of MR to enhance collaborative learning experiences\nfor children in distributed settings.", "primary_category": "cs.HC", "categories": "cs.HC", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07707v1", "pdf_filename": "2025-06-09_Interaction_Analysis_by_Humans_and_AI__A_Comparati.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Interaction_Analysis_by_Humans_and_AI__A_Comparati.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07691v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Training Superior Sparse Autoencoders for Instruct Models", "authors": "Jiaming Li, Haoran Ye, Yukun Chen, Xinyue Li, Lei Zhang, Hamid Alinejad-Rokny, Jimmy Chih-Hsien Peng, Min Yang", "abstract": "As large language models (LLMs) grow in scale and capability, understanding\ntheir internal mechanisms becomes increasingly critical. Sparse autoencoders\n(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the\nextraction of human-interpretable features from LLMs. However, existing SAE\ntraining methods are primarily designed for base models, resulting in reduced\nreconstruction quality and interpretability when applied to instruct models. To\nbridge this gap, we propose\n$\\underline{\\textbf{F}}$inetuning-$\\underline{\\textbf{a}}$ligned\n$\\underline{\\textbf{S}}$equential $\\underline{\\textbf{T}}$raining\n($\\textit{FAST}$), a novel training method specifically tailored for instruct\nmodels. $\\textit{FAST}$ aligns the training process with the data distribution\nand activation patterns characteristic of instruct models, resulting in\nsubstantial improvements in both reconstruction and feature interpretability.\nOn Qwen2.5-7B-Instruct, $\\textit{FAST}$ achieves a mean squared error of 0.6468\nin token reconstruction, significantly outperforming baseline methods with\nerrors of 5.1985 and 1.5096. In feature interpretability, $\\textit{FAST}$\nyields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,\n$21.1\\%$ scored in the top range, compared to $7.0\\%$ and $10.2\\%$ for\n$\\textit{BT(P)}$ and $\\textit{BT(F)}$. Surprisingly, we discover that\nintervening on the activations of special tokens via the SAEs leads to\nimprovements in output quality, suggesting new opportunities for fine-grained\ncontrol of model behavior. Code, data, and 240 trained SAEs are available at\nhttps://github.com/Geaming2002/FAST.", "primary_category": "cs.CL", "categories": "cs.CL, cs.LG", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07691v1", "pdf_filename": "2025-06-09_Training_Superior_Sparse_Autoencoders_for_Instruct.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Training_Superior_Sparse_Autoencoders_for_Instruct.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07675v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents", "authors": "Yuyang Song, Hanxu Yan, Jiale Lao, Yibo Wang, Yufei Li, Yuanchun Zhou, Jianguo Wang, Mingjie Tang", "abstract": "Query rewrite transforms SQL queries into semantically equivalent forms that\nrun more efficiently. Existing approaches mainly rely on predefined rewrite\nrules, but they handle a limited subset of queries and can cause performance\nregressions. This limitation stems from three challenges of rule-based query\nrewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite\nrules do not generalize to new query patterns, and (3) some rewrite techniques\ncannot be expressed as fixed rules. Motivated by the fact that human experts\nexhibit significantly better rewrite ability but suffer from scalability, and\nLarge Language Models (LLMs) have demonstrated nearly human-level semantic and\nreasoning abilities, we propose a new approach of using LLMs to rewrite SQL\nqueries beyond rules. Due to the hallucination problems in LLMs, directly\napplying LLMs often leads to nonequivalent and suboptimal queries. To address\nthis issue, we propose QUITE (query rewrite), a training-free and\nfeedback-aware system based on LLM agents that rewrites SQL queries into\nsemantically equivalent forms with significantly better performance, covering a\nbroader range of query patterns and rewrite strategies compared to rule-based\nmethods. Firstly, we design a multi-agent framework controlled by a finite\nstate machine (FSM) to equip LLMs with the ability to use external tools and\nenhance the rewrite process with real-time database feedback. Secondly, we\ndevelop a rewrite middleware to enhance the ability of LLMs to generate\noptimized query equivalents. Finally, we employ a novel hint injection\ntechnique to improve execution plans for rewritten queries. Extensive\nexperiments show that QUITE reduces query execution time by up to 35.8% over\nstate-of-the-art approaches and produces 24.1% more rewrites than prior\nmethods, covering query cases that earlier systems did not handle.", "primary_category": "cs.DB", "categories": "cs.DB", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07675v1", "pdf_filename": "2025-06-09_QUITE__A_Query_Rewrite_System_Beyond_Rules_with_LL.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_QUITE__A_Query_Rewrite_System_Beyond_Rules_with_LL.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07673v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "How Benchmark Prediction from Fewer Data Misses the Mark", "authors": "Guanhua Zhang, Florian E. Dorner, Moritz Hardt", "abstract": "Large language model (LLM) evaluation is increasingly costly, prompting\ninterest in methods that speed up evaluation by shrinking benchmark datasets.\nBenchmark prediction (also called efficient LLM evaluation) aims to select a\nsmall subset of evaluation points and predict overall benchmark performance\nfrom that subset. In this paper, we systematically assess the strengths and\nlimitations of 11 benchmark prediction methods across 19 diverse benchmarks.\nFirst, we identify a highly competitive baseline: Take a random sample and fit\na regression model on the sample to predict missing entries. Outperforming most\nexisting methods, this baseline challenges the assumption that careful subset\nselection is necessary for benchmark prediction. Second, we discover that all\nexisting methods crucially depend on model similarity. They work best when\ninterpolating scores among similar models. The effectiveness of benchmark\nprediction sharply declines when new models have higher accuracy than\npreviously seen models. In this setting of extrapolation, none of the previous\nmethods consistently beat a simple average over random samples. To improve over\nthe sample average, we introduce a new method inspired by augmented inverse\npropensity weighting. This method consistently outperforms the random sample\naverage even for extrapolation. However, its performance still relies on model\nsimilarity and the gains are modest in general. This shows that benchmark\nprediction fails just when it is most needed: at the evaluation frontier, where\nthe goal is to evaluate new models of unknown capabilities.", "primary_category": "cs.LG", "categories": "cs.LG", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07673v1", "pdf_filename": "2025-06-09_How_Benchmark_Prediction_from_Fewer_Data_Misses_th.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_How_Benchmark_Prediction_from_Fewer_Data_Misses_th.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07672v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents", "authors": "Yunhe Yan, Shihe Wang, Jiajun Du, Yexuan Yang, Yuxuan Shan, Qichen Qiu, Xianqing Jia, Xinge Wang, Xin Yuan, Xu Han, Mao Qin, Yinxiao Chen, Chen Peng, Shangguang Wang, Mengwei Xu", "abstract": "(M)LLM-powered computer use agents (CUA) are emerging as a transformative\ntechnique to automate human-computer interaction. However, existing CUA\nbenchmarks predominantly target GUI agents, whose evaluation methods are\nsusceptible to UI changes and ignore function interactions exposed by\napplication APIs, e.g., Model Context Protocol (MCP). To this end, we propose\nMCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid\nagents. A key principle of MCPWorld is the use of \"white-box apps\", i.e., those\nwith source code availability and can be revised/re-compiled as needed (e.g.,\nadding MCP support), with two notable advantages:\n  (1) It greatly broadens the design space of CUA, such as what and how the app\nfeatures to be exposed/extracted as CUA-callable APIs.\n  (2) It allows MCPWorld to programmatically verify task completion by directly\nmonitoring application behavior through techniques like dynamic code\ninstrumentation, offering robust, accurate CUA evaluation decoupled from\nspecific agent implementations or UI states.\n  Currently, MCPWorld includes 201 well curated and annotated user tasks,\ncovering diversified use cases and difficulty levels. MCPWorld is also fully\ncontainerized with GPU acceleration support for flexible adoption on different\nOS/hardware environments. Our preliminary experiments, using a representative\nLLM-powered CUA framework, achieve 75.12% task completion accuracy,\nsimultaneously providing initial evidence on the practical effectiveness of\nagent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate\nand standardize the benchmarking of next-generation computer use agents that\ncan leverage rich external tools. Our code and dataset are publicly available\nat https://github.com/SAAgent/MCPWorld.", "primary_category": "cs.AI", "categories": "cs.AI", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07672v1", "pdf_filename": "2025-06-09_MCPWorld__A_Unified_Benchmarking_Testbed_for_API__.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_MCPWorld__A_Unified_Benchmarking_Testbed_for_API__.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07671v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation", "authors": "Ionut-Teodor Sorodoc, Leonardo F. R. Ribeiro, Rexhina Blloshmi, Christopher Davis, Adri\u00e0 de Gispert", "abstract": "We present GaRAGe, a large RAG benchmark with human-curated long-form answers\nand annotations of each grounding passage, allowing a fine-grained evaluation\nof whether LLMs can identify relevant grounding when generating RAG answers.\nOur benchmark contains 2366 questions of diverse complexity, dynamism, and\ntopics, and includes over 35K annotated passages retrieved from both private\ndocument sets and the Web, to reflect real-world RAG use cases. This makes it\nan ideal test bed to evaluate an LLM's ability to identify only the relevant\ninformation necessary to compose a response, or provide a deflective response\nwhen there is insufficient information. Evaluations of multiple\nstate-of-the-art LLMs on GaRAGe show that the models tend to over-summarise\nrather than (a) ground their answers strictly on the annotated relevant\npassages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)\ndeflect when no relevant grounding is available (reaching at most 31% true\npositive rate in deflections). The F1 in attribution to relevant sources is at\nmost 58.9%, and we show that performance is particularly reduced when answering\ntime-sensitive questions and when having to draw knowledge from sparser private\ngrounding sources.", "primary_category": "cs.CL", "categories": "cs.CL, cs.AI", "comment": "ACL 2025 (Findings)", "pdf_url": "http://arxiv.org/pdf/2506.07671v1", "pdf_filename": "2025-06-09_GaRAGe__A_Benchmark_with_Grounding_Annotations_for.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_GaRAGe__A_Benchmark_with_Grounding_Annotations_for.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07664v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Synthesis by Design: Controlled Data Generation via Structural Guidance", "authors": "Lei Xu, Sirui Chen, Yuxuan Huang, Chaochao Lu", "abstract": "Mathematical reasoning remains challenging for LLMs due to complex logic and\nthe need for precise computation. Existing methods enhance LLM reasoning by\nsynthesizing datasets through problem rephrasing, but face issues with\ngeneration quality and problem complexity. To address this, we propose to\nextract structural information with generated problem-solving code from\nmathematical reasoning and guide data generation with structured solutions.\nApplied to MATH and GSM8K, our approach produces 39K problems with labeled\nintermediate steps and a 6.1K-problem benchmark of higher difficulty. Results\non our benchmark show that model performance declines as reasoning length\nincreases. Additionally, we conducted fine-tuning experiments using the\nproposed training data on a range of LLMs, and the results validate the\neffectiveness of our dataset. We hope the proposed method and dataset will\ncontribute to future research in enhancing LLM reasoning capabilities.", "primary_category": "cs.CL", "categories": "cs.CL, cs.AI", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07664v1", "pdf_filename": "2025-06-09_Synthesis_by_Design__Controlled_Data_Generation_vi.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Synthesis_by_Design__Controlled_Data_Generation_vi.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07658v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping", "authors": "Nitin Sharma, Thomas Wolfers, \u00c7a\u011fatay Y\u0131ld\u0131z", "abstract": "The paper addresses two critical challenges in language model (LM)\nevaluation: creating reliable domain-specific benchmarks and understanding\nknowledge representation during domain adaptation. We introduce a deterministic\npipeline that converts raw domain corpora into completion-type benchmarks\nwithout relying on LMs or human curation, eliminating benchmark contamination\nissues while enabling evaluation on the latest domain data. Our approach\ngenerates domain-specific keywords and related word lists using TF and Term\nTF-IDF methods and constructs prompt-target pairs. We evaluate models by\nmeasuring their ability to complete these prompts with the correct\ndomain-specific targets, providing a direct assessment of domain knowledge with\nlow computational cost. Through comprehensive experiments across multiple\nmodels (GPT-2 medium/XL, Llama-2/3.1, OLMo-2, Qwen-2, Mistral) and domains, we\ndemonstrate that our benchmark strongly correlates with expert-generated\nbenchmarks while providing a more accurate measure of domain knowledge than\ntraditional perplexity metrics. We reveal that domain adaptation happens\nrapidly in smaller models (within 500 steps) and illustrate a new approach to\ndomain knowledge evaluation in base models during training for early stopping.\nBy extending mechanistic analysis to domain adaptation, we discover that\ninitial-to-mid layers are primarily responsible for attribute extraction, while\nlater layers focus on next token prediction. Furthermore, we show that during\nadaptation, forgetting begins in the middle layers, where attribute extraction\nhappens and is amplified in later layers. Our work provides both a practical\nevaluation methodology for domain-specific LMs and novel insights into\nknowledge representation during adaptation, with implications for more\nefficient fine-tuning strategies and targeted approaches to mitigate\ncatastrophic forgetting.", "primary_category": "cs.CL", "categories": "cs.CL, I.2.7; I.2.6", "comment": "35 pages, 24 figures. First submission", "pdf_url": "http://arxiv.org/pdf/2506.07658v1", "pdf_filename": "2025-06-09_Beyond_Benchmarks__A_Novel_Framework_for_Domain-Sp.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Beyond_Benchmarks__A_Novel_Framework_for_Domain-Sp.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07647v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Foundation Model Empowered Synesthesia of Machines (SoM): AI-native Intelligent Multi-Modal Sensing-Communication Integration", "authors": "Xiang Cheng, Boxun Liu, Xuanyu Liu, Ensong Liu, Ziwei Huang", "abstract": "To support future intelligent multifunctional sixth-generation (6G) wireless\ncommunication networks, Synesthesia of Machines (SoM) is proposed as a novel\nparadigm for artificial intelligence (AI)-native intelligent multi-modal\nsensing-communication integration. However, existing SoM system designs rely on\ntask-specific AI models and face challenges such as scarcity of massive\nhigh-quality datasets, constrained modeling capability, poor generalization,\nand limited universality. Recently, foundation models (FMs) have emerged as a\nnew deep learning paradigm and have been preliminarily applied to SoM-related\ntasks, but a systematic design framework is still lacking. In this paper, we\nfor the first time present a systematic categorization of FMs for SoM system\ndesign, dividing them into general-purpose FMs, specifically large language\nmodels (LLMs), and SoM domain-specific FMs, referred to as wireless foundation\nmodels. Furthermore, we derive key characteristics of FMs in addressing\nexisting challenges in SoM systems and propose two corresponding roadmaps,\ni.e., LLM-based and wireless foundation model-based design. For each roadmap,\nwe provide a framework containing key design steps as a guiding pipeline and\nseveral representative case studies of FM-empowered SoM system design.\nSpecifically, we propose LLM-based path loss generation (LLM4PG) and scatterer\ngeneration (LLM4SG) schemes, and wireless channel foundation model (WiCo) for\nSoM mechanism exploration, LLM-based wireless multi-task SoM transceiver\n(LLM4WM) and wireless foundation model (WiFo) for SoM-enhanced transceiver\ndesign, and wireless cooperative perception foundation model (WiPo) for\nSoM-enhanced cooperative perception, demonstrating the significant superiority\nof FMs over task-specific models. Finally, we summarize and highlight potential\ndirections for future research.", "primary_category": "eess.SP", "categories": "eess.SP", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07647v1", "pdf_filename": "2025-06-09_Foundation_Model_Empowered_Synesthesia_of_Machines.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Foundation_Model_Empowered_Synesthesia_of_Machines.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07645v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models", "authors": "Maciej Chrab\u0105szcz, Katarzyna Lorenc, Karolina Seweryn", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious natural language processing (NLP) tasks in recent years. However, their\nsusceptibility to jailbreaks and perturbations necessitates additional\nevaluations. Many LLMs are multilingual, but safety-related training data\ncontains mainly high-resource languages like English. This can leave them\nvulnerable to perturbations in low-resource languages such as Polish. We show\nhow surprisingly strong attacks can be cheaply created by altering just a few\ncharacters and using a small proxy model for word importance calculation. We\nfind that these character and word-level attacks drastically alter the\npredictions of different LLMs, suggesting a potential vulnerability that can be\nused to circumvent their internal safety mechanisms. We validate our attack\nconstruction methodology on Polish, a low-resource language, and find potential\nvulnerabilities of LLMs in this language. Additionally, we show how it can be\nextended to other languages. We release the created datasets and code for\nfurther research.", "primary_category": "cs.CL", "categories": "cs.CL", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07645v1", "pdf_filename": "2025-06-09_Evaluating_LLMs_Robustness_in_Less_Resourced_Langu.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Evaluating_LLMs_Robustness_in_Less_Resourced_Langu.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07642v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review", "authors": "Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Zhijiang Guo, Ngai Wong", "abstract": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review.", "primary_category": "cs.CL", "categories": "cs.CL", "comment": "30 pages, 17 figures", "pdf_url": "http://arxiv.org/pdf/2506.07642v1", "pdf_filename": "2025-06-09_TreeReview__A_Dynamic_Tree_of_Questions_Framework_.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_TreeReview__A_Dynamic_Tree_of_Questions_Framework_.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07636v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling", "authors": "Haoran Wang, Zhenyu Hou, Yao Wei, Jie Tang, Yuxiao Dong", "abstract": "Large language models (LLMs) have advanced rapidly from conversational\nproblem solving to addressing real-world tasks involving tool use, such as\nsoftware engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex\nand Cursor, have offered end-to-end automation of the software development\nprocess. However, building effective SWE agents remains challenging due to the\nlack of high-quality training data and effective test cases. To address this\nissue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we\ndevelop a robust pipeline to synthesize test cases for patch evaluation.\nSecond, we scale up agent trajectories to construct the training data for\nbuilding SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the\nSWE-Dev models can achieve top performance among all open SWE agents.\nSpecifically, the success rates of the SWE-Dev 7B and 32B parameter models\nreach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source\nmodels. All code, models, and datasets are publicly available at\nhttps://github.com/THUDM/SWE-Dev.", "primary_category": "cs.AI", "categories": "cs.AI", "comment": "Accepted to Findings of ACL'25", "pdf_url": "http://arxiv.org/pdf/2506.07636v1", "pdf_filename": "2025-06-09_SWE-Dev__Building_Software_Engineering_Agents_with.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_SWE-Dev__Building_Software_Engineering_Agents_with.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07631v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline", "authors": "Brian Gordon, Yonatan Bitton, Andreea Marzoca, Yasumasa Onoe, Xiao Wang, Daniel Cohen-Or, Idan Szpektor", "abstract": "Large Vision-Language Models (VLMs) now generate highly detailed,\nparagraphlength image captions, yet evaluating their factual accuracy remains\nchallenging. Current methods often miss fine-grained errors, being designed for\nshorter texts or lacking datasets with verified inaccuracies. We introduce\nDOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100\nimages, 14 VLMs) featuring over 10,216 sentence-level human annotations of\nfactual correctness and explanatory rationales for errors, all within paragraph\ncontext. Building on this, we develop VNLI-Critique, a model for automated\nsentence-level factuality classification and critique generation. We highlight\nthree key applications: (1) VNLI-Critique demonstrates robust generalization,\nvalidated by state-of-the-art performance on the M-HalDetect benchmark and\nstrong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven\nAutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent\nalignment with human factuality judgments (e.g., 0.98 Spearman). (3) An\ninnovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide\nLLM-based corrections, achieves substantial improvements in caption factuality\n(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark\nalongside practical tools, designed to significantly elevate the standards for\nfine-grained evaluation and foster the improvement of VLM image understanding.\nProject page: https://google.github.io/unblocking-detail-caption", "primary_category": "cs.CL", "categories": "cs.CL, cs.CV", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07631v1", "pdf_filename": "2025-06-09_Unblocking_Fine-Grained_Evaluation_of_Detailed_Cap.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Unblocking_Fine-Grained_Evaluation_of_Detailed_Cap.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07627v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Event-Priori-Based Vision-Language Model for Efficient Visual Understanding", "authors": "Haotong Qin, Cheng Hu, Michele Magno", "abstract": "Large Language Model (LLM)-based Vision-Language Models (VLMs) have\nsubstantially extended the boundaries of visual understanding capabilities.\nHowever, their high computational demands hinder deployment on\nresource-constrained edge devices. A key source of inefficiency stems from the\nVLM's need to process dense and redundant visual information. Visual inputs\ncontain significant regions irrelevant to text semantics, rendering the\nassociated computations ineffective for inference. This paper introduces a\nnovel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core\ncontribution is a novel mechanism leveraging motion priors derived from dynamic\nevent vision to enhance VLM efficiency. Inspired by human visual cognition,\nEP-VLM first employs event data to guide the patch-wise sparsification of RGB\nvisual inputs, progressively concentrating VLM computation on salient regions\nof the visual input. Subsequently, we construct a position-preserving\ntokenization strategy for the visual encoder within the VLM architecture. This\nstrategy processes the event-guided, unstructured, sparse visual input while\naccurately preserving positional understanding within the visual input.\nExperimental results demonstrate that EP-VLM achieves significant efficiency\nimprovements while maintaining nearly lossless accuracy compared to baseline\nmodels from the Qwen2-VL series. For instance, against the original\nQwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the\noriginal accuracy on the RealWorldQA dataset. This work demonstrates the\npotential of event-based vision priors for improving VLM inference efficiency,\npaving the way for creating more efficient and deployable VLMs for sustainable\nvisual understanding at the edge.", "primary_category": "cs.CV", "categories": "cs.CV", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07627v1", "pdf_filename": "2025-06-09_Event-Priori-Based_Vision-Language_Model_for_Effic.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Event-Priori-Based_Vision-Language_Model_for_Effic.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07626v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation", "authors": "Kseniia Petukhova, Ekaterina Kochmar", "abstract": "Large language models (LLMs) hold great promise for educational applications,\nparticularly in intelligent tutoring systems. However, effective tutoring\nrequires alignment with pedagogical strategies - something current LLMs lack\nwithout task-specific adaptation. In this work, we explore whether fine-grained\nannotation of teacher intents can improve the quality of LLM-generated tutoring\nresponses. We focus on MathDial, a dialog dataset for math instruction, and\napply an automated annotation framework to re-annotate a portion of the dataset\nusing a detailed taxonomy of eleven pedagogical intents. We then fine-tune an\nLLM using these new annotations and compare its performance to models trained\non the original four-category taxonomy. Both automatic and qualitative\nevaluations show that the fine-grained model produces more pedagogically\naligned and effective responses. Our findings highlight the value of intent\nspecificity for controlled text generation in educational settings, and we\nrelease our annotated data and code to facilitate further research.", "primary_category": "cs.CL", "categories": "cs.CL", "comment": "", "pdf_url": "http://arxiv.org/pdf/2506.07626v1", "pdf_filename": "2025-06-09_Intent_Matters__Enhancing_AI_Tutoring_with_Fine-Gr.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Intent_Matters__Enhancing_AI_Tutoring_with_Fine-Gr.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07621v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "LoRMA: Low-Rank Multiplicative Adaptation for LLMs", "authors": "Harsh Bihany, Shubham Patel, Ashutosh Modi", "abstract": "Large Language Models have shown remarkable capabilities in the NLP domain.\nTheir effectiveness can mainly be attributed to their ability to adapt to an\narray of downstream tasks. However, generally, full fine-tuning is a\ncomputationally expensive job. To mitigate this, many techniques have been\ndeveloped that prime efficiency, a prominent one being Low-Rank Adaptation\n(LoRA). However, LoRA and its variants employ re-parametrized additive updates.\nIn this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which\nshifts the paradigm of additive updates to a richer space of matrix\nmultiplicative transformations. We tackle challenges such as computational\ncomplexity and rank bottleneck of matrix multiplication by effectively\nre-ordering operations and introducing rank inflation strategies. We conduct\nextensive experiments to demonstrate the effectiveness of our approach in terms\nof various evaluation metrics.", "primary_category": "cs.CL", "categories": "cs.CL, cs.AI, cs.LG", "comment": "Accepted at ACL Findings 2025; 21 pages (9 main paper + 5 pages\n  references + 7 pages appendix)", "pdf_url": "http://arxiv.org/pdf/2506.07621v1", "pdf_filename": "2025-06-09_LoRMA__Low-Rank_Multiplicative_Adaptation_for_LLMs.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_LoRMA__Low-Rank_Multiplicative_Adaptation_for_LLMs.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07617v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation", "authors": "Roman Kyslyi, Yuliia Maksymiuk, Ihor Pysmennyi", "abstract": "In this paper we introduce the first effort to adapt large language models\n(LLMs) to the Ukrainian dialect (in our case Hutsul), a low-resource and\nmorphologically complex dialect spoken in the Carpathian Highlands. We created\na parallel corpus of 9852 dialect-to-standard Ukrainian sentence pairs and a\ndictionary of 7320 dialectal word mappings. We also addressed data shortage by\nproposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate\nsynthetic parallel translation pairs, expanding the corpus with 52142 examples.\nWe have fine-tuned multiple open-source LLMs using LoRA and evaluated them on a\nstandard-to-dialect translation task, also comparing with few-shot GPT-4o\ntranslation. In the absence of human annotators, we adopt a multi-metric\nevaluation strategy combining BLEU, chrF++, TER, and LLM-based judgment\n(GPT-4o). The results show that even small(7B) finetuned models outperform\nzero-shot baselines such as GPT-4o across both automatic and LLM-evaluated\nmetrics. All data, models, and code are publicly released at:\nhttps://github.com/woters/vuyko-hutsul", "primary_category": "cs.CL", "categories": "cs.CL", "comment": "Preprint. Will be published at Proceedings of the Fourth Ukrainian\n  Natural Language Processing Workshop (UNLP)", "pdf_url": "http://arxiv.org/pdf/2506.07617v1", "pdf_filename": "2025-06-09_Vuyko_Mistral__Adapting_LLMs_for_Low-Resource_Dial.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_Vuyko_Mistral__Adapting_LLMs_for_Low-Resource_Dial.md"}
{"query_date": "2025-06-10", "query_keyword": "Multi-Agent LLM", "arxiv_id": "2506.07606v1", "publication_date": "2025-06-09", "updated_date": "2025-06-09", "title": "PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels", "authors": "Peyman Rostami, Vahid Rahimzadeh, Ali Adibi, Azadeh Shakery", "abstract": "Stance detection identifies the viewpoint expressed in text toward a specific\ntarget, such as a political figure. While previous datasets have focused\nprimarily on tweet-level stances from established platforms, user-level stance\nresources, especially on emerging platforms like Bluesky remain scarce.\nUser-level stance detection provides a more holistic view by considering a\nuser's complete posting history rather than isolated posts. We present the\nfirst stance detection dataset for the 2024 U.S. presidential election,\ncollected from Bluesky and centered on Kamala Harris and Donald Trump. The\ndataset comprises 16,044 user-target stance pairs enriched with engagement\nmetadata, interaction graphs, and user posting histories. PolitiSky24 was\ncreated using a carefully evaluated pipeline combining advanced information\nretrieval and large language models, which generates stance labels with\nsupporting rationales and text spans for transparency. The labeling approach\nachieves 81\\% accuracy with scalable LLMs. This resource addresses gaps in\npolitical stance analysis through its timeliness, open-data nature, and\nuser-level perspective. The dataset is available at\nhttps://doi.org/10.5281/zenodo.15616911", "primary_category": "cs.CL", "categories": "cs.CL, cs.AI, cs.IR, cs.SI, I.2.7", "comment": "The dataset is available at https://doi.org/10.5281/zenodo.15616911", "pdf_url": "http://arxiv.org/pdf/2506.07606v1", "pdf_filename": "2025-06-09_PolitiSky24__U_S__Political_Bluesky_Dataset_with_U.pdf", "markdown_path": "data/arxiv/markdown_files/2025-06-09_PolitiSky24__U_S__Political_Bluesky_Dataset_with_U.md"}
