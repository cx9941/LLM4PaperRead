[
    {
        "timestamp": "2025-06-13 16:14:40",
        "task_name": "research_task",
        "task": "阅读论文《Unsupervised Recognition of Unknown Objects for Open-World Object Detection》，论文于2023-08-31发布于arXiv.org内容如下：\\n# Unsupervised Recognition of Unknown Objects for Open-World Object Detection\n\nRuohuan Fang, Guansong Pang, Lei Zhou, Xiao Bai, Jin Zheng\n\nSeptember 1, 2023\n\nAbstract Open-World Object Detection (OWOD) extends object detection problem to a realistic and dynamic scenario, where a detection model is required to be capable of detecting both known and unknown objects and incrementally learning newly introduced knowledge. Current OWOD models, such as ORE and OW-DETR, focus on pseudo-labeling regions with high objectness scores as unknowns, whose performance relies heavily on the supervision of known objects. While they can detect the unknowns that exhibit similar features to the known objects, they suffer from a severe label bias problem that they tend to detect all regions (including unknown object regions) that are dissimilar to the known objects as part of the background. To eliminate the label bias, this paper proposes a novel approach that learns an unsupervised discriminative model to recognize true unknown objects from raw pseudo labels generated by unsupervised region proposal methods. The resulting model can be further refined by a classification-free self-training method which iteratively extends pseudo unknown objects to the unlabeled re-\n\nGuansong Pang\n\nSchool of Computing and Information Systems, Singapore Management University E-mail: gspang@smu.edu.sg\n\nLei Zhou\n\nSchool of Computer Science and Artificial Intelligence, Wuhan University of Technology E-mail: leizhou@whut.edu.cn\n\nCorresponding Authors: G. Pang and X. Bai.\n\ngions. Experimental results show that our method 1) significantly outperforms the prior SOTA in detecting unknown objects while maintaining competitive performance of detecting known object classes on the MS COCO dataset, and 2) achieves better generalization ability on the LVIS and Objects365 datasets. Code is available at <https://github.com/frh23333/mepu-owod>.\n\nKeywords Open world · object detection · unsupervised learning · self-training\n\n#### <span id=\"page-0-0\"></span>1 Introduction\n\nRecent years have witnessed tremendous progress in deep learning-based object detection [\\[1–](#page-15-0)[5\\]](#page-15-1). However, traditional object detection models typically adopt a closed-world setting, meaning that they only consider manually annotated known objects and ignore all other unlabeled objects. But there are scenarios where the recognition of unknown objects is critical. For instance, an autonomous car or robot needs to detect unknown obstacles to avoid collisions and ensure safety.\n\nDhamija et al. [\\[6\\]](#page-15-2) is an early work that explores the problem of Open-Set Object Detection (OSOD), where the detector is trained using known object labels but is required to identify unknown objects during testing. Joseph et al. [\\[7\\]](#page-15-3) further extend the OSOD task to a more dynamic scenario, Open-World Object Detection (OWOD), where the model is required to recognize both known and unknown objects and can be incrementally trained with newly introduced knowledge. Previous methods [\\[7](#page-15-3)[–11\\]](#page-16-0) tackle the OWOD problem by pseudo-labeling regions that do not overlap with known objects and have high objectness scores as unknowns. Such objectness scores indicate the probability of the regions belonging to the foreground, which can\n\nRuohuan Fang, Xiao Bai, Jin Zheng\n\nSchool of Computer Science and Engineering, State Key Laboratory of Software Development Environment, Jiangxi Research Institute, Beihang University\n\nE-mail: ruohuanfang@gmail.com, baixiao@buaa.edu.cn, jinzheng@buaa.edu.cn\n\nbe obtained via a class-agnostic detector (e.g., RPN [\\[1\\]](#page-15-0)) trained using known object labels. These methods can successfully detect unknown objects that exhibit similar features to the known objects. However, they suffer from a severe label bias problem of the known classes, i.e., they tend to detect all regions (including unknown object regions) that are dissimilar to the known objects as part of the background.\n\nSome previous methods [\\[10,](#page-16-1)[12\\]](#page-16-2) have explored using unsupervised region proposal generation methods [\\[13–](#page-16-3) [17\\]](#page-16-4) to improve the generalization ability of OWOD models. Such unsupervised region proposals are usually generated using hand-crafted low-level features (e.g., color, texture, shape, and contour). Such region proposals provide the prior knowledge and physical constraints about the regions where unknown objects may exist. However, they still require objectness-based pseudo labels to calibrate such region proposals produced by unsupervised methods. Therefore, the label bias problem that obstructs the detection of unknown objects remains unsolved in the OWOD task.\n\nTo tackle the aforementioned label bias problem, we propose a novel OWOD framework, named MEPU, to model and extend pseudo unknown objects generated by unsupervised region proposal generators. MEPU first learns an unsupervised discriminative model to recognize unknown objects from a large set of raw pseudo labels generated by the unsupervised proposal generators. The key insight here is that the common background regions (e.g., sky, sea, grassland, and white wall) frequently appear in different images and usually have repeated and simple low-level patterns (e.g., color, texture). In contrast, the foreground regions have much more diverse visual patterns since there are various object categories. As a result, the background and foreground regions respectively form two different distributions from the perspective of the feature frequency domain. Inspired by the reconstruction-based out-ofdistribution (OOD) detection models [\\[18](#page-16-5)[–21\\]](#page-16-6), we find that the encoder-decoder framework (e.g., autoencoders) is an ideal tool for modeling such regional features with different appearance frequencies. Therefore, we implement the unsupervised modeling of foreground and background using a new module, called Reconstruction Errorbased Weibull Model (REW), which models and identifies the unknown objects by a Weibull modeling of the reconstruction errors of the regions. Based on the modeled Weibull distributions of foreground and background regions, all pseudo labels are assigned with soft labels to estimate the likelihood of being true unknown objects. Our REW module can effectively address the label bias problem since we leverage the feature frequencies, rather than similarity to known objects like\n\n![](_page_1_Figure_5.jpeg)\n\nFig. 1: Previous methods vs. ours. (a) Previous OWOD methods [\\[7,](#page-15-3) [8\\]](#page-16-7) suffer from a severe label bias problem that they tend detect all regions (including unknown object regions) that are dissimilar to the known objects as part of the background, while (b) our approach eliminates this label bias problem by modeling and extending pseudo unknown objects in an unsupervised manner, achieving significantly improved ability to detect unknown objects, without affecting the performance of known object detection.\n\nthe objectness scores, to recognize the unknown objects. Note that the reconstruction-based OOD detectors fail to work in OWOD, since they are difficult to adaptively determine a decision threshold for the unknown objects per training image (see Sec. [4.3\\)](#page-11-0). We address this issue via the Weibull modeling in the REW module.\n\nMEPU further trains a REW-enhanced Object Localization Network (ROLNet) for extending pseudo unknown labels to the unlabeled regions. More specifically, the soft labels yielded from REW are incorporated in the training loss of a classification-free detector Object Localization Network (OLN) [\\[22\\]](#page-16-8), so that the pseudo labels of unknown objects can be effectively and accurately extended in each round of self-training, which further alleviates the impact of label bias problem.\n\nOur contributions can be summarized as follows:\n\n- We propose a novel approach named MEPU that decouples the unknown object recognition problem into two sub-problems: unsupervised modeling of unknown objects in the raw pseudo labels generated by unsupervised region generators, and extending new unknown objects that are not covered in these pseudo labels.\n- We introduce a novel module, namely Reconstruction Error-based Weibull Model (REW), that utilizes the prior knowledge of object occurrence frequency via Weibull modeling to have an effective\n\nunsupervised recognition of unknown objects and tackle the label bias problem in OWOD task.\n\n- We further train a REW-enhanced Object Localization Network (ROLNet) that synthesizes soft labels yielded from REW and classification-free self training to effectively and accurately extend the coverage of pseudo labels.\n- Experiments show that our proposed method MEPU 1) significantly outperforms the prior SOTA in detecting unknown objects (e.g., having up to ∼13.9 increase in unknown recall rate) while maintaining competitive performance of detecting known object classes on the MS COCO dataset, and 2) achieves better generalization ability on the LVIS and Objects365 datasets.\n\n# 2 Related Work\n\nOpen-Set Object Detection. Dhamija et al. [\\[6\\]](#page-15-2) is the first work to explicitly explore the OSOD problem, where the detector is required to identify unknown objects during testing. [\\[23,](#page-16-9) [24\\]](#page-16-10) use Dropout Sampling (DS) to measure the uncertainty of the object detector and reject the unknown objects. VOS [\\[25\\]](#page-16-11) synthesizes virtual outliers that can regularize the model's decision boundary during training. OpenDet [\\[11\\]](#page-16-0) identifies unknown objects by separating high/low-density regions in the latent space, and it uses Contrastive Feature Learner (CFL) and Unknown Probability Learner (UPL) to achieve this goal.\n\nSome other works [\\[1,](#page-15-0) [22,](#page-16-8) [26](#page-16-12)[–28\\]](#page-16-13) focus on generating class-agnostic region proposals with higher generalization ability for objects of unknown categories. Kim et al. [\\[22\\]](#page-16-8) propose an Object Localization Network (OLN) which replaces the classification heads of Faster-RCNN [\\[1\\]](#page-15-0) with localization quality prediction heads in order that the detector will not overfit the known classes and depress the scores of unknown objects. [\\[26\\]](#page-16-12) propose a new augmentation method Back-Erase that pastes known objects on a background image sampled from a small region of the original image, so that potential unknown objects will not be suppressed as negatives. [\\[27\\]](#page-16-14) propose the Generic Grouping Networks (GGNs) that learn the Pairwise Affinities (PA) of pixels and use the PA predictions to construct pseudo labels which generalize well to unknown objects. [\\[28\\]](#page-16-13) formulate the entity segmentation task which requires segmenting all visual entities within an image, including things (object) and stuff. They devise an entity segmentation framework based on the unified center-based representation and propose the global kernel bank and overlap suppression module to generate high-quality class-agnostic segmentation masks.\n\nOpen-World Object Detection. Joseph et al. [\\[7\\]](#page-15-3) extend the OSOD task to a more dynamic scenario and formulate the OWOD problem, where the model is required to recognize both known and unknown objects and can be incrementally trained with newly introduced knowledge. They propose the ORE model, which uses the objectness scores of RPN for pseudo-labeling unknown objects and adopts an energy-based classifier to separate the known and unknown classes. OW-DETR [\\[8\\]](#page-16-7) adopts Deformable DETR (detection transformer) as the base detector and uses its attention maps obtained from the intermediate features as scores to assign pseudo labels of unknown classes. PROB [\\[9\\]](#page-16-15) further incorporates a probabilistic objectness head into the standard Deformable DETR model, which iteratively estimates the objectness probability distribution and maximizes the likelihood of known objects to learn more general features for both known and unknown objects. CAT [\\[10\\]](#page-16-1) decouples the detection process via the shared decoder in the cascade decoding way and adopts the self-adaptive pseudo-labelling mechanism which combines the model-driven and input-driven pseudo-labels and self-adaptively to generate robust pseudo-labels for unknown objects. Wu et al. [\\[29\\]](#page-16-16) propose Unknown-Classified Open World Object Detection (UC-OWOD) which requires the model to classify unknown objects into different unknown classes. They design the similaritybased unknown classification (SUC) to detect unknown objects as different classes, and the unknown clustering refinement (UCR) to refine the classification of unknown objects.\n\nAlthough previous methods have made remarkable progress in the recognition of unknown objects for the OWOD task, they still utilize the objectness scores for unknown object pseudo-labeling, which depends on the supervision of known object labels. Therefore, they suffer from the aforementioned label bias problem that limits their recognition ability of unknown objects that are semantically irrelevant to known ones. Instead, our model enables the unsupervised discriminative recognition of unknown objects, which effectively addresses the label bias problem and improves the OWOD model's detection performance of unknown objects.\n\nUnsupervised Region Proposal Generation Methods. Before the deep learning era, many works [\\[13–](#page-16-3) [15,](#page-16-17) [30–](#page-16-18)[32\\]](#page-16-19) focus on generating region proposals as object candidates based on hand-crafted low-level features (e.g., color, texture, and contour). Selective Search [\\[14\\]](#page-16-20) greedily merges superpixels to generate proposals. Edge-Boxes [\\[13\\]](#page-16-3) produces proposals by scoring bounding boxes based on the number of edge contours. But they are later replaced by deep learning-based supervised methods [\\[1\\]](#page-15-0) due to low precision and high time cost. Geodesic\n\nObject Proposal [\\[15\\]](#page-16-17) identifies a set of candidate objects based on critical level sets in geodesic distance transforms computed for seeds placed in the image. Recently, some works [\\[16,](#page-16-21) [17\\]](#page-16-4) have explored generating deep network-based region proposals in an unsupervised manner. FreeSOLO [\\[16\\]](#page-16-21) learns class-agnostic instance segmentation without any manual annotations and its proposal quality significantly outperforms previous unsupervised methods. Detreg [\\[17\\]](#page-16-4) is trained on ImageNet [\\[33\\]](#page-16-22) using Selective Search to provide pseudo ground truth labels.\n\nAlthough these unsupervised region proposal generation methods can not accurately locate each object, they provide additional knowledge and geometric constraints about the regions where unknown objects may exist. In our proposed method, we leverage those unsupervised region proposal generators to produce raw pseudo-labels of unknown objects.\n\nReconstruction-based OOD Detection. OOD Detection aims to detect and reject testing samples that do not belong to the distribution the model has been trained on. The core idea of reconstruction-based methods is that the encoder-decoder framework trained on the ID (In Distribution) data has larger reconstruction errors for OOD data, so we can separate ID and OOD samples based on their reconstruction errors during inference. [\\[18\\]](#page-16-5) incorporates the Mahalanobis distance in latent space to better capture OOD samples that lie far from ID samples in latent space but near the latent dimension manifold of the model. [\\[19\\]](#page-16-23) formulates the essence of the reconstruction-based approach as a quadruplet domain translation with an intrinsic bias to only query for a proxy of conditional data uncertainty. Accordingly, they adopt strategies including semantic reconstruction, data certainty decomposition, and normalized L2 distance to substantially improve OOD detection performance. READ [\\[20\\]](#page-16-24) incorporates auto-encoder into classifier-based OOD detection model by transforming the reconstruction error of raw pixels to the latent space of the classifier.\n\nOur REW module is inspired by the reconstructionbased method in OOD detection. The key difference is that the auto-encoder in our method reconstructs all regional features including frequently-appeared background regions and rarely-appeared foreground regions, so that their reconstruction errors can be used to model the probability distributions for background-foreground recognition; whereas the OOD detection methods are focused on reconstructing image-level ID samples. Moreover, REW learns a soft label indicating the probability of being a true object for each pseudo unknown object via our Weibull modeling. While the reconstructionbased OOD detection models simply utilize reconstruction errors as the OOD scores to reject testing samples whose scores that are higher than a pre-defined user threshold. In our problem setting, such a hard thresholding strategy cannot be adaptive to each training image, leading to high false negative errors, i.e., true unknown objects are filtered out by the hard threshold as background regions.\n\n# 3 Proposed Method\n\n## 3.1 Problem definition\n\nIn OWOD, we are given a streaming set of object detection tasks T <sup>t</sup> = {T1, T2, · · · }. For each task Tt, a set of known classes K<sup>t</sup> = {C 1 t , C<sup>2</sup> t , · · · , C<sup>K</sup> <sup>t</sup> } and a set of unknown classes U <sup>t</sup> = {C K+1 t , CK+2 t , · · · } are presented. The training dataset D<sup>t</sup> = {I<sup>t</sup> , Y <sup>t</sup>} contains N images I <sup>t</sup> = {I1, I2, · · · , I<sup>N</sup> } with the corresponding known class labels Y <sup>t</sup> = {Y1, Y2, · · · , Y<sup>N</sup> }. Each Y<sup>i</sup> = {y1, y2, · · · , yK} denotes the annotations of K instances in one image. The k-th instance label is y<sup>k</sup> = [lk, xk, yk, wk, hk], where l<sup>k</sup> ∈ K<sup>t</sup> denotes the class label, and [xk, yk, wk, hk] denotes the coordinates, width, and height of the bounding box.\n\nOWOD has two main problems, including open-set object detection and incremental object detection [\\[34–](#page-16-25) [37\\]](#page-16-26). For the former problem, the goal is to train an object detector M<sup>t</sup> at time step t with D<sup>t</sup> that contain only the annotations of known object classes. At testing time, the model should be able to detect known objects by classifying them into one of the K<sup>t</sup> classes, while identifying objects beyond known classes by labeling them as 'unknown'. In the incremental object detection, a new task Tt+1 at the next time step is given, where a new set of known classes Kt+1 = {C 1 t+1, C<sup>2</sup> <sup>t</sup>+1, · · · , C<sup>M</sup> <sup>t</sup>+1} is presented, then the goal is to adapt the model M<sup>t</sup> to the new knowledge in Kt+1 and obtain a new model Mt+1, instead of retraining from scratch on the whole dataset. Mt+1 is required to be capable of correctly identifying all the newly added known classes, as well as the previously known and unknown classes, i.e., without catastrophic forgetting of the previously learned knowledge [\\[38](#page-16-27)[–40\\]](#page-17-0).\n\n#### 3.2 Overview of The Proposed Approach\n\nTo address the label bias towards the known classes, our proposed approach MEPU aims to first have unsupervised modeling of unknown objects presented in the raw pseudo labels generated by unsupervised region proposal generators, and then extend to new unknown objects that are not covered in these pseudo labels.\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\nFig. 2: Our proposed approach. It consists of two main components, including Reconstruction Error-based Weibull Model (REW) and REW-enhanced Object Localization Network (ROLNet). REW is designed for modeling unknown objects hidden in the pseudo labels generated by unsupervised proposal generators, while ROLNet is employed to iteratively extend new unknown objects based on the likelihood scores produced by REW.\n\nTo this end, our approach introduces two main modules, consisting of Reconstruction Error-based Weibull Model (REW) and REW-enhanced Object Localization Network (ROLNet), as illustrated in Fig. [2.](#page-4-0) REW can be considered as an unsupervised unknown object identification model that is trained to recognize various types of unknown objects from common background regions in an unsupervised manner and assign all pseudo labels with soft labels to estimate the likelihood of being true unknown objects. While ROLNet leverages the REW scores and classification-free OLN in the selftraining procedure to effectively extend the set of unknown objects that exist in the unlabeled regions. Faster-RCNN [\\[1\\]](#page-15-0) is used as the base detector in our approach. Below we introduce these two modules in detail.\n\n# 3.3 REW: Reconstruction Error-based Weibull Model for Unsupervised Unknown Object Identification\n\nExisting unsupervised region proposal methods can generate massive regions that may contain various types of unknown objects, and thus these region proposals that do not overlap with known instances can be used as pseudo labels for unknown objects. However, these raw unknown pseudo labels may also include many nonobject bounding boxes on the background. The key challenge is to distinguish the true unknown objects from these raw pseudo labels while preventing the label\n\nbias problem. The proposed REW module (Reconstruction Error-based Weibull Model) is specifically designed for learning an unsupervised unknown object identification model to tackle this challenge.\n\nREW is motivated by the observation that the common background regions (e.g., sky, sea, grassland, and white wall) frequently appear in different images and usually share repeated and simple low-level patterns (e.g., color, texture). In contrast, the foreground regions have much more diverse visual patterns since there are various object categories. As a result, the background and foreground regions respectively form two different distributions from the perspective of the feature frequency domain. Therefore, REW is devised to learn the frequency information by a data reconstruction-based auto-encoder and model the two different distributions of the reconstruction errors by a prior probability distribution. Since Weibull distribution is superior in fitting a wide range of distribution shapes, it is used as the prior model in REW.\n\n# <span id=\"page-4-1\"></span>3.3.1 Reconstruction Error-based Weibull Modeling of Foreground and Background Regions\n\nAs demonstrated in Fig. [3,](#page-5-0) given an input image I ∈ R HI×WI×3 , we first use a backbone network to extract the feature map F ∈ R <sup>H</sup><sup>F</sup> <sup>×</sup>W<sup>F</sup> <sup>×</sup><sup>C</sup> . The feature vector of each pixel in the feature map represents the features of\n\n<span id=\"page-5-0\"></span>![](_page_5_Figure_1.jpeg)\n\nFig. 3: The pipeline of our reconstruction error-based Weibull modeling (REW) module. REW first extracts the regional features from the backbone and then feeds all feature vectors to train an auto-encoder by minimizing the errors of reconstructing the regional features. The distribution of the ℓ<sup>2</sup> distance-based reconstruction errors for the known objects (red boxes) and background regions are modeled using a Weibull model. The pseudo unknown objects (green boxes) would be assigned with soft labels using Eq. [\\(5\\)](#page-6-0) to provide knowledge of true unknown objects for training generalized open-world object detectors that are able to detect different types of unknown objects while preventing label bias problem.\n\nthe regions within the receptive field. Therefore, we utilize the auto-encoder to reconstruct such regional features. The encoder and decoder of the auto-encoder are built as multiple convolutional layers with 1 × 1 convolutional kernels, noted as Enc() and Dec() respectively. The encoder first maps the feature map F into a latent space Flatent ∈ R <sup>H</sup><sup>F</sup> <sup>×</sup>W<sup>F</sup> <sup>×</sup>Clatent with fewer dimensions (channels). Then, the decoder reconstructs the latent feature into the original dimensions, obtaining the reconstructed features Frec ∈ R <sup>H</sup><sup>F</sup> <sup>×</sup>W<sup>F</sup> <sup>×</sup><sup>C</sup> . We measure the per-pixel reconstruction error by an ℓ<sup>2</sup> distance and utilize it as the training loss for the auto-encoder. The process can be formulated as follows:\n\n<span id=\"page-5-1\"></span>\n$$\nF_{rec} = Dec(Enc(F))\n$$\n\\n<sup>(1)</sup>\n\n$$\nL_{rew} = \\frac{1}{H_F \\times W_F} \\sum_{i=1}^{H_F} \\sum_{j=1}^{W_F} L_2(F_{rec}[i, j], F[i, j]), \\tag{2}\n$$\n\nwhere Frec[i, j] and F[i, j] denote the regional features with C dimensions in the location [i, j]; L<sup>2</sup> means the ℓ2-norm loss.\n\nThe regional feature of each pixel represents the features of the anchor in the corresponding location, so we can assign each pixel with a foreground/background label based on its corresponding anchor box (e.g., in RPN, an anchor box belongs to the foreground when it overlaps any object with IoU larger than 0.7). As discussed in Sec[.1,](#page-0-0) background regions often share frequently-occurred features, making them better reconstructed and resulting in smaller reconstruction errors compared to the infrequent features of various foreground object regions.\n\nWhen the auto-encoder is trained to convergence, we obtain the reconstruction error map E ∈ R H<sup>F</sup> ×W<sup>F</sup> ×1 via calculating per-pixel ℓ<sup>2</sup> distance, i.e., E[i, j] = L2(Frec[i, j], F[i, j]). By randomly sampling pixels from known objects and background regions, we collect a set of reconstruction errors (noted as Ekn and Ebg respectively). Fig. [4](#page-6-1) visualizes the histogram of randomly sampled reconstruction errors of the annotated known objects regions and background regions from the training set of MS COCO [\\[41\\]](#page-17-1). It is evident that the reconstruction errors of the known object regions are generally much larger compared to those of the background regions. Although unknown objects may have different appearances from known objects, we can assume that they have similar low occurrence frequencies and high reconstruction errors, as there are various types of unknown objects. Therefore, we utilize the reconstruction errors from the sampled annotated known objects to estimate the distribution of all foreground regions.\n\nSince Weibull distribution is superior in fitting a wide range of distribution shapes for many real-world scenarios, it is used as the prior model in REW. The Weibull distributions of known regions and background regions (noted as fkn and fbg respectively) are in the form as follows:\n\n$$\nf(re; a, c) = ac[1 - \\exp(-re^c)]^{a-1} \\exp(-re^c)re^{c-1}, (3)\n$$\n\nwhere re denotes the reconstruction error value of a sampled pixel; f is a probability density function of exponentiated Weibull Distribution and a and c are its shape parameters. The optimal a and c are calculated using Maximum Likelihood Estimation (MLE) based on the sampled reconstruction errors of foreground (Ekn) and background regions (Ebg).\n\n<span id=\"page-6-1\"></span>![](_page_6_Figure_1.jpeg)\n\nFig. 4: Visualization of reconstruction error distributions on the MS COCO dataset. The known and background regions are randomly sampled from the training images of MS COCO.\n\n## <span id=\"page-6-4\"></span>3.3.2 Soft Labeling for Unknown Object Recognition\n\nAfter modeling the distributions of foreground and background regions, we then use the probability function (fkn and fbg) to estimate the likelihood of a pseudo unknown object being a true unknown object. Given one pseudo unknown object punk ∈ R 4 in the image I, it may cover multiple pixels in the feature map. Therefore, we calculate its reconstruction error by averaging the values of all corresponding pixels in that location. To be specific, we utilize the RoIAlign [\\[42\\]](#page-17-2) operation which pools the reconstruction errors within punk into a scalar value, as formulated below:\n\n$$\nre(p_{unk}) = RoIAlign(E, p_{unk}),\n$$\n\\n(4)\n\nwhere re(punk) is the reconstruction error value of punk; RoIAlign means the RoIAlign operation; E ∈ R H<sup>F</sup> ×W<sup>F</sup> ×1 denotes the calculated reconstruction error map. Furthermore, we can calculate the soft label, which estimates the likelihood score of a pseudo object being a true unknown object, using the following equation:\n\n$$\ns(p_{unk}) = \\left(\\frac{f_{kn}(re(p_{unk}))}{f_{bg}(re(p_{unk})) + f_{kn}(re(p_{unk}))}\\right)^{\\gamma},\\tag{5}\n$$\n\nwhere fkn and fbg are the Weibull probability density functions of the known object and background regions, respectively, and γ is a hyperparameter to scale the value of the likelihood score. All pseudo labels will be discarded when γ → ∞ and all pseudo labels will be treated as true objects when γ → 0.\n\n# 3.3.3 Incorporating the Soft Labels into Object Detection Loss Functions\n\nThe unknown object likelihood scores s estimated in Eq. [\\(5\\)](#page-6-0) can then be incorporated into the classification\n\nloss (Lcls) of the detector Faster-RCNN as a weight term to learn to recognize unknown objects while detecting known objects. The modified classification loss equation is as follows:\n\n<span id=\"page-6-2\"></span>\n$$\nL_{cls} = \\frac{1}{N_{cls}} \\sum_{i} w_{r_i} L_{CE}(p_{r_i}, p_{r_i}^*),\n$$\n\\n(6)\n\nwhere wr<sup>i</sup> is the loss weight of a region proposal r<sup>i</sup> which equals to s(ri) when r<sup>i</sup> belongs to the region of a pseudo unknown object, or otherwise it equals to 1; pr<sup>i</sup> and p ∗ ri denote the predicted probability and the ground truth of the region proposal r<sup>i</sup> respectively and LCE means the cross-entropy loss.\n\n# 3.4 ROLNet: REW-enhanced Object Localization Network for Extending the Set of Unknown Objects\n\nAlthough our proposed REW module can effectively identify true unknown objects from the pseudo unknown object labels generated by unsupervised region proposal methods, there are still unknown objects not covered by the pseudo labels. Current semi-supervised object detection methods [\\[43](#page-17-3)[–47\\]](#page-17-4) adopt self-training to extend the pseudo labels to the unlabeled data. However, it cannot work well in our problem setting, because traditional object detectors typically adopt cross-entropy or focal loss as the classification loss for proposal classification, which results in a bias toward the known classes. Since all unlabeled regions are treated as negative samples in the classification loss, minimizing such loss functions would compress the objectness scores of the unlabeled regions as close as zero.\n\n<span id=\"page-6-3\"></span><span id=\"page-6-0\"></span>In our ROLNet module, we aim to utilize REW soft labels and the self-training strategy based on classificationfree detectors, such as object localization networks (OLN), to address this problem. Classification-free detectors aim to estimate the objectness scores by predicting how well the anchors or proposals are located, such as IoU or centerness with their corresponding ground truth instances. This turns the classification problem into a localization quality prediction problem, where only positive anchors or proposals are sampled for training. So the scores of the unlabeled regions will not be compressed, which helps generalize to the unknown objects missed by the pseudo labels. OLN adopts the same architecture as Faster-RCNN but replaces the classification heads in both stages with the localization quality prediction heads. The localization quality heads are trained with the following loss function:\n\n$$\nL_{oln} = \\frac{1}{N_{oln}} \\sum_{i} w_{r_i} L_1(q_{r_i}, q_{r_i}^*),\n$$\n\\n(7)\n\n<span id=\"page-7-0\"></span>![](_page_7_Figure_0.jpeg)\n\nFig. 5: The network architecture of our model. The networks of Faster-RCNN (yellow) and OLN (green) are combined with a unified RPN and RoI Align structure.\n\nwhere qr<sup>i</sup> and q ∗ ri denote the predicted localization and target scores of the region proposal r<sup>i</sup> respectively; L<sup>1</sup> is an ℓ1-norm loss; wr<sup>i</sup> is the loss weight term as in Eq[.6.](#page-6-2) The unknown object likelihood score s(ri) predicted by REW (i.e., wr<sup>i</sup> ) is incorporated in Loln so that OLN will not be misled by the noisy labels and extend the new pseudo labels more accurately. The positive region proposals in Loln are the proposals that have an IoU larger than 0.3 with the matched ground-truth boxes. By default, centerness and IoU are used as targets of localization scores in the RPN and RoI stages, respectively.\n\nThe overall detection architecture (OLN and Faster-RCNN) is first trained using both known object labels and unknown pseudo labels generated by unsupervised proposal methods. We then apply OLN-based selftraining to refine our detection model. The trained OLN is adopted to generate class-agnostic proposals based on its predictions of the localization quality scores. Only the top P % high-score proposals are selected as new pseudo labels for the unknown objects. After that, the detection model is trained using the updated unknown object labels. This self-training procedure is repeated for l iterations, which can gradually include new unknown objects to further refine the detection model.\n\n# 3.5 Training and Inference\n\nTrain. The training process of REW is separate from the object detection model because we have found that features extracted by the self-supervised pre-trained model contain more generalized semantics for recognizing unknown objects, as discussed in [\\[48,](#page-17-5) [49\\]](#page-17-6). In REW, the auto-encoder is trained using regional features extracted by the SoCo backbone [\\[50\\]](#page-17-7) that is pre-trained using\n\n<span id=\"page-7-1\"></span>\n\n|                                        | Task 1           | Task 2                   | Task 3            | Task 4              |\n|----------------------------------------|------------------|--------------------------|-------------------|---------------------|\n| Semantic split                         | Animals, Person, | Appliances, Accessories, | Sports,           | Electronic, Indoor, |\n|                                        | Vehicles         | Outdoor, Furniture       | Food              | Kitchen             |\n| # training images                      | 89,490           | 55,870                   | 39,402            | 38,903              |\n| # train instances                      | 421,243          | 163,512                  | 114,452           | 160,794             |\n| # test images                          |                  | 4,952                    |                   |                     |\n| # test instances                       |                  | 36,781                   |                   |                     |\n|                                        | Task 1           | Task 2                   | Task 3            | Task 4              |\n|                                        |                  |                          |                   |                     |\n|                                        | VOC              |                          | Sports,           |                     |\n| Semantic split                         | Classes          | Outdoor, Accessories,    | Food              | Electronic, Indoor, |\n|                                        | 16,551           | Appliance, Truck         |                   | Kitchen, Furniture  |\n| # training images<br># train instances | 47,223           | 45,520<br>113,741        | 39,402<br>114,452 | 40,260<br>138,996   |\n| # test images<br># test instances      | 4,952            | 1,914<br>4,966           | 1,642<br>4,826    | 1,738<br>6,039      |\n\nTable 1: Dataset splits of S-OWODB (superclassseparated OWOD benchmark) (Top) and M-OWODB (superclass-mixed OWOD benchmark) (Bottom).\n\nobject-level contrastive learning. When training the autoencoder, the pre-trained backbone is frozen as it is only used for feature extraction. Subsequently, the trained REW module can calculate likelihood scores for pseudo unknown objects using Eq[.5.](#page-6-0) These scores are then utilized in Lcls and Loln to train the object detection model. Note that only the known object labels are used in the box regression loss since the pseudo labels are mostly poorly localized. As shown in Fig. [5,](#page-7-0) we combine the network architecture of Faster-RCNN and OLN with a unified RPN and RoI Align structure. For a task T<sup>i</sup> with Ckn known classes, the Faster-RCNN-based detector is trained as a (Ckn + 2)-class classifier, where the extra (+2) classes are the 'unknown' and 'background' classes. Additionally, we employ localization quality prediction heads in both stages to generate classagnostic proposals for self-training.\n\nFor the incremental learning procedure, we adopt the approach of exemplar replay [\\[51](#page-17-8)[–54\\]](#page-17-9). Following the setting of [\\[7,](#page-15-3)[8\\]](#page-16-7), 50 exemplars are stored for each known class, i.e., a small subset of training images containing 50 objects for each known class. The detector is then finetuned using the stored exemplars to recover the previously known knowledge.\n\nInference. During inference, the trained Faster-RCNNbased detector is used to classify each bounding box into (Ckn + 2) classes. When an unknown object is predicted, we utilize the class-agnostic box regression head for its box localization, because it can produce welllocalized unknown object predictions. The inference of known objects is the same as standard Faster-RCNN.\n\nUnsupervised Recognition of Unknown Objects for Open-World Object Detection 9\n\n<span id=\"page-8-1\"></span>\n\n| Task IDs (→)                                 |                      | Task 1                                        |                      | Task 2               |                             |                      |                      | Task 3               |                             |                      |                          | Task 4                      |                      |\n|----------------------------------------------|----------------------|-----------------------------------------------|----------------------|----------------------|-----------------------------|----------------------|----------------------|----------------------|-----------------------------|----------------------|--------------------------|-----------------------------|----------------------|\n|                                              | (↑)                  | U-Recall mAP (↑) U-Recall<br>Current<br>known | (↑)                  | Previously<br>known  | mAP (↑)<br>Current<br>known | Both                 | U-Recall<br>(↑)      | Previously<br>known  | mAP (↑)<br>Current<br>known |                      | Both Previously<br>known | mAP (↑)<br>Current<br>known | Both                 |\n| Faster-RCNN [1]<br>Faster-RCNN               | 0.0                  | 74.4                                          | 0.0                  | 0.42                 | 44.8                        | 24.7                 | 0.0                  | 0.23                 | 43.1                        | 14.5                 | 0.15                     | 41.6                        | 10.5                 |\n| + Finetuning<br>ORE − EBUI [7]               | 0.0<br>1.5           | 74.4<br>71.4                                  | 0.0<br>3.9           | 68.2<br>61.0         | 42.2<br>30.9                | 54.6<br>45.6         | 0.0<br>3.6           | 50.1<br>43.1         | 38.7<br>32.2                | 46.4<br>39.5         | 43.9<br>33.6             | 35.6<br>26.3                | 41.9<br>31.8         |\n| OW-DETR [8]<br>PROB [9]                      | 5.7<br>17.6          | 73.1<br>73.5                                  | 6.2<br>22.3          | 65.0<br>66.3         | 29.0<br>36.0                | 46.0<br>50.4         | 6.9<br>24.8          | 46.7<br>47.8         | 25.7<br>30.4                | 39.7<br>42.0         | 38.2<br>42.6             | 28.1<br>31.7                | 33.1<br>39.9         |\n| CAT [10]<br>MEPU-FS (Ours)<br>MEPU-SS (Ours) | 24.0<br>37.9<br>33.3 | 74.2<br>74.3<br>74.2                          | 23.0<br>35.8<br>34.2 | 67.6<br>68.0<br>67.5 | 35.5<br>41.9<br>41.0        | 50.7<br>54.3<br>53.6 | 24.6<br>35.7<br>33.6 | 51.2<br>50.2<br>50.0 | 32.6<br>38.3<br>37.5        | 45.0<br>46.2<br>45.8 | 45.4<br>43.7<br>43.2     | 35.1<br>33.7<br>33.5        | 42.8<br>41.2<br>40.8 |\n| Faster-RCNN [1]                              | 0.0                  | 60.3                                          | 0.0                  | 0.69                 | 35.2                        | 17.9                 | 0.0                  | 0.32                 | 23.5                        | 8.0                  | 0.65                     | 20.1                        | 5.5                  |\n| Faster-RCNN<br>+ Finetuning                  | 0.0                  | 60.3                                          | 0.0                  | 57.6                 | 34.0                        | 45.3                 | 0.0                  | 43.8                 | 22.3                        | 36.6                 | 35.6                     | 19.5                        | 31.5                 |\n| ORE − EBUI [7]<br>OW-DETR [8]<br>PROB [9]    | 4.9<br>7.5<br>19.4   | 56.0<br>59.2<br>59.5                          | 2.9<br>6.2<br>17.4   | 52.7<br>53.6<br>55.7 | 26.0<br>33.5<br>32.2        | 39.4<br>42.9<br>44.0 | 3.9<br>5.7<br>19.6   | 38.2<br>38.3<br>43.0 | 12.7<br>15.8<br>22.2        | 29.7<br>30.8<br>36.0 | 29.6<br>31.4<br>35.7     | 12.4<br>17.1<br>18.9        | 25.3<br>27.8<br>31.5 |\n| CAT [10]<br>MEPU-FS (Ours)                   | 23.7<br>31.6         | 60.0<br>60.2                                  | 19.1<br>30.9         | 55.5<br>57.3         | 32.7<br>33.3                | 44.1<br>44.8         | 24.4<br>30.1         | 42.8<br>42.6         | 18.7<br>21.0                | 34.8<br>35.4         | 34.4<br>34.8             | 16.6<br>19.1                | 29.9<br>30.9         |\n| MEPU-SS (Ours)                               | 30.3                 | 60.0                                          | 30.6                 | 57.0                 | 33.1                        | 44.5                 | 30.0                 | 42.2                 | 20.5                        | 35.0                 | 34.3                     | 18.9                        | 30.4                 |\n\nTable 2: Comparison to state-of-the-art OWOD models on S-OWODB (top) and M-OWODB (bottom). We adopt mAP of known classes (Known-mAP) and recall of unknown classes (U-Recall) as the closed-set and open-set evaluation metrics. U-Recall is not available in Task 4 because there are no unknown annotations during inference. Results of our MEPU method using two proposal generation methods, FreeSOLO (MEPU-FS) and Selective-Search (MEPU-SS), are reported.\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_3.jpeg)\n\nFig. 6: Visualizations of reconstruction error distributions in different feature levels on S-OWODB Task 1. The Weibull models formed for FPN feature maps {P3, P4, P5, P6} are shown here.\n\n# 4 Experiments\n\n# 4.1 Experimental Settings\n\n# 4.1.1 Datasets.\n\nOWOD Benchmark. We evaluate our MEPU on two OWOD benchmarks proposed by [\\[8\\]](#page-16-7) and [\\[7\\]](#page-15-3) respectively. Following [\\[9\\]](#page-16-15), we refer to these two benchmarks as S-OWODB (superclass-separated OWOD benchmark) and M-OWODB (superclass-mixed OWOD benchmark), respectively. As shown in Tab. [1,](#page-7-1) the 80 classes of COCO are divided into four groups, and each group of data is treated as the dataset for one of the streaming tasks. During the training procedure of task Tt, all classes in {T<sup>τ</sup> : τ ≤ t} are known classes, and all classes within {T<sup>τ</sup> : τ > t} are unknown classes. The models are incrementally trained using images with only the known class annotations of each task, instead of using the entire dataset.\n\nCross Dataset Evaluation. The OWOD benchmark focuses on open-set detection within one dataset. Following [\\[22,](#page-16-8) [27\\]](#page-16-14), we also evaluate the detection ability under the cross-dataset setting, using the LVIS v0.5 [\\[55\\]](#page-17-10) and Objects365 [\\[56\\]](#page-17-11) datasets. LVIS is a large vocabulary instance segmentation dataset with 1,203 classes in a long-tailed distribution, while Objects365 is a largescale object detection dataset consisting of 365 object categories. Both datasets include the 80 COCO classes that are treated as known classes and other classes are labeled as unknown. We train our object detector using the training set of MS COCO and evaluate on the testing set of LVIS and Objects365 to measure our generalization performance across datasets.\n\n## 4.1.2 Evaluation Metrics.\n\nWe employ mAP (mean Average Precision) as the metric to evaluate the detection performance of known categories. As for unknown classes, we utilize unknown object recall (U-recall) as the primary evaluation metric, following the approaches in [\\[8](#page-16-7)[–10\\]](#page-16-1). The U-recall metric in our model is calculated using unknown object predictions with scores higher than 0.05. Additionally, we include the results of Recall@K for a more detailed analysis, in accordance with [\\[22,](#page-16-8) [26,](#page-16-12) [27\\]](#page-16-14). This metric evaluates the performance based on the top K detected unknown objects, ranked by unknown classification scores. To quantify the extent of misclassification of unknown objects as known ones, we report the Absolute Open-Set Error (A-OSE) and Wilderness Impact (WI) metrics. A-OSE represents the total number of unknown objects misclassified as known objects. WI is calculated using the formula:\n\n$$\nWI = \\frac{A - OSE}{TP_{\\mathcal{K}} + FP_{\\mathcal{K}}}\n$$\n\\n(8)\n\nwhere T P<sup>K</sup> and F P<sup>K</sup> are the true positive and false positive predictions of known classes, respectively.\n\n#### 4.1.3 Implementation Details.\n\nDetailed Implementation of REW. The encoder and decoder in REW are constructed using a single convolutional layer with 1 × 1 kernels. We adopt the frozen ResNet50-FPN pre-trained by SoCo [\\[50\\]](#page-17-7) as the backbone network for regional feature extraction. Since FPN is employed as the backbone, the multi-level feature maps corresponding to anchor boxes of different sizes exhibit significant variations in the number of region features. To prevent overfitting or underfitting the regional data in different levels, we build auto-encoders with different intermediate layer dimensions for the multilevel feature maps. For FPN feature maps {P3, P4, P5, P6}, the intermediate layer dimensions of auto-encoders are set to {32, 16, 8, 4}. The feature maps {P3, P4, P5, P6} are corresponding to objects with sizes ranging from [32<sup>2</sup> , 64<sup>2</sup> ], [64<sup>2</sup> , 128<sup>2</sup> ], [128<sup>2</sup> , 256<sup>2</sup> ], [256<sup>2</sup> , 512<sup>2</sup> ] respectively, when calculating their reconstruction errors using Eq. [4.](#page-6-3) Specifically, different Weibull models are formed for the region features at different levels as shown in Fig. [3.5,](#page-8-0) thereby enabling our REW model to effectively recognize unknown objects with varying sizes. We train REW using the MS-COCO training images, using\n\nEq. [2](#page-5-1) for 12 epochs. This training process does not require any object labels. For each task in OWOD, we model the Weibull distributions of the foreground and background based on the known object annotations, and subsequently assign soft labels to each pseudo label, as elaborated in Sec. [3.3.1](#page-4-1) and Sec. [3.3.2.](#page-6-4)\n\nDetailed Implementation of Object Detector. Our method is implemented based on Detectron2 [\\[57\\]](#page-17-12). We use ResNet-50 [\\[58\\]](#page-17-13) with FPN [\\[59\\]](#page-17-14) as the backbone network and Faster-RCNN [\\[1\\]](#page-15-0) as the base detector. We adopt the SGD optimizer with an initial learning rate of 0.02, a momentum of 0.9, and a weight decay of 1e −4 . All models are trained for 12 epochs under the standard schedule, and the self-training process lasts for 4 epochs for each round. All experiments are conducted using eight GPUs with a batch size of 16. We set the hyperparameters γ = 4, P = 30, and l = 1 by default in our experiments.\n\nPseudo Label Processing. We apply the same data processing strategy for the class-agnostic proposals generated by unsupervised region proposal generators and OLN to filter out low-quality pseudo labels. The proposals are first processed by Non-Maximum Suppression (NMS) at the IoU threshold of 0.3 to prevent overlapping bounding boxes. Then, only the proposals which satisfy the following requirements are used as pseudo labels for unknown objects:\n\n- The box size should be larger than 2000 pixels.\n- The aspect ratio should be between 0.25 and 4.0.\n- The IoU with known objects should be less than 0.3.\n\nThe first two rules aim to filter out low-quality boxes, while the purpose of the third rule is to prevent confusion with known instances.\n\n### 4.1.4 Competing Methods.\n\nOur method is compared with two baselines – Faster-RCNN and Faster-RCNN + Finetuning – and four stateof-the-art (SOTA) open-world object detectors, including ORE [\\[7\\]](#page-15-3), OW-DETR [\\[8\\]](#page-16-7), CAT [\\[10\\]](#page-16-1) and PROB [\\[9\\]](#page-16-15). Faster-RCNN is only trained with labels of known classes. Faster-RCNN + Finetuning denotes that Faster-RCNN is finetuned using exemplar replay. They can only detect known objects, so their results of unknown recall are all zeros. The energy-based unknown identifier (EBUI) of ORE is not applied here since it needs weak unknown label supervision. The results of ORE, OW-DETR, CAT, and PROB are obtained from their paper. For the cross-dataset evaluation and label bias evaluation experiments, we also compete with the SOTA open-set object detection model, OpenDet [\\[11\\]](#page-16-0).\n\nUnsupervised Recognition of Unknown Objects for Open-World Object Detection 11\n\n<span id=\"page-10-0\"></span>\n\n| Task IDs (→)                |      |                | Task 1                |       |                  |      |                | Task 2                |       |                  |      |                | Task 3                |                  |       |\n|-----------------------------|------|----------------|-----------------------|-------|------------------|------|----------------|-----------------------|-------|------------------|------|----------------|-----------------------|------------------|-------|\n|                             |      | Unknown Recall |                       |       | Confusion Metric |      | Unknown Recall |                       |       | Confusion Metric |      | Unknown Recall |                       | Confusion Metric |       |\n|                             |      |                | R@10 R@30 R@100 A-OSE |       | WI               |      |                | R@10 R@30 R@100 A-OSE |       | WI               |      |                | R@10 R@30 R@100 A-OSE |                  | WI    |\n|                             | (↑)  | (↑)            | (↑)                   | (↓)   | (↓)              | (↑)  | (↑)            | (↑)                   | (↓)   | (↓)              | (↑)  | (↑)            | (↑)                   | (↓)              | (↓)   |\n| Faster-RCNN<br>+ Finetuning | 0.0  | 0.0            | 0.0                   | 1807  | 0.022            | 0.0  | 0.0            | 0.0                   | 4007  | 0.033            | 0.0  | 0.0            | 0.0                   | 4010             | 0.025 |\n| ORE − EBUI [7]              | 3.6  | 6.3            | 11.9                  | 2486  | 0.024            | 5.6  | 10.5           | 14.5                  | 6608  | 0.040            | 6.8  | 10.7           | 13.1                  | 6896             | 0.026 |\n| OW-DETR [8]                 | 2.3  | 5.6            | 14.7                  | 12721 | 0.029            | 4.3  | 9.0            | 19.2                  | 14970 | 0.041            | 4.4  | 13.5           | 24.2                  | 9197             | 0.024 |\n| PROB [9]                    | 11.6 | 23.2           | 37.8                  | 2003  | 0.021            | 14.6 | 26.8           | 40.5                  | 3358  | 0.031            | 15.2 | 27.1           | 43.5                  | 1546             | 0.018 |\n| CAT [10]                    | 13.5 | 26.1           | 40.5                  | 2097  | 0.023            | 13.3 | 25.6           | 40.1                  | 5784  | 0.040            | 14.1 | 26.5           | 42.7                  | 3545             | 0.021 |\n| MEPU-FS (Ours)              | 25.1 | 39.4           | 54.8                  | 1710  | 0.020            | 25.0 | 40.1           | 54.4                  | 3197  | 0.027            | 24.8 | 39.7           | 55.5                  | 2862             | 0.020 |\n| MEPU-SS (Ours)              | 21.2 | 36.2           | 54.5                  | 1753  | 0.020            | 22.3 | 36.6           | 53.1                  | 3352  | 0.028            | 23.3 | 36.5           | 52.7                  | 2883             | 0.020 |\n| Faster-RCNN<br>+ Finetuning | 0.0  | 0.0            | 0.0                   | 13396 | 0.070            | 0.0  | 0.0            | 0.0                   | 12291 | 0.037            | 0.0  | 0.0            | 0.0                   | 9622             | 0.028 |\n| ORE − EBUI [7]              | 3.8  | 9.4            | 17.5                  | 10459 | 0.062            | 4.4  | 9.8            | 15.3                  | 10445 | 0.028            | 4.2  | 8.5            | 16.8                  | 7990             | 0.021 |\n| OW-DETR [8]                 | 6.2  | 12.3           | 19.9                  | 10240 | 0.057            | 5.1  | 11.3           | 19.1                  | 8441  | 0.028            | 6.9  | 12.3           | 18.5                  | 6803             | 0.016 |\n| PROB [9]                    | 10.7 | 19.8           | 32.2                  | 5195  | 0.057            | 12.3 | 17.8           | 32.5                  | 6452  | 0.034            | 11.2 | 18.1           | 33.0                  | 2641             | 0.015 |\n| CAT [10]                    | 11.8 | 20.5           | 34.0                  | 20364 | 0.066            | 12.1 | 18.7           | 33.2                  | 16768 | 0.032            | 12.9 | 19.5           | 33.7                  | 7515             | 0.020 |\n| MEPU-FS (Ours)              | 18.9 | 27.6           | 38.9                  | 6050  | 0.056            | 18.4 | 26.1           | 37.5                  | 5925  | 0.023            | 19.2 | 27.6           | 36.9                  | 5159             | 0.016 |\n| MEPU-SS (Ours)              | 16.7 | 25.5           | 38.5                  | 6235  | 0.057            | 17.0 | 24.3           | 37.0                  | 6042  | 0.023            | 18.8 | 26.3           | 36.2                  | 5369             | 0.016 |\n\nTable 3: Additional results for OWOD on S-OWODB (top) and M-OWODB (bottom). We report the top K recall of unknown objects (noted as R@K) and the confusion metrics (including A-OSE and WI) for a more detailed analysis.\n\n## 4.2 Comparison to State-of-the-art\n\n#### 4.2.1 Open-World Object Detection Performance\n\nThe mean Average Precision (mAP) for known classes and U-Recall for unknown classes of our method and state-of-the-art models on S-OWODB and M-OWODB are presented in Tab. [2.](#page-8-1) The results of our model using FreeSOLO [\\[16\\]](#page-16-21) and Selective Search [\\[14\\]](#page-16-20) as proposal generators, denoted as MEPU-FS and MEPU-SS repetitively, are reported here.\n\nOverall, both variants of our method, MEPU-FS, and MEPU-SS, consistently and substantially outperform the four SOTA open-world methods in terms of both unknown and known object detection across the four tasks, while retaining almost the same detection performance as the closed-set baseline model Faster-RCNN + Finetuning on detecting the known objects. Detailed analyses of the results are presented separately as follows.\n\nUnknown Object Detection. Our method, MEPU-FS, exhibits significantly better performance in detecting unknown objects compared to the top competitor CAT. On S-OWODB, MEPU-FS surpasses CAT by 13.9 (Task 1), 12.8 (Task 2), and 11.1 (Task 3) in U-Recall. MEPU-SS, another variant of our method using the traditional proposal generator, also achieves notable improvements in U-Recall performance, with gains of 9.3 (Task 1), 11.2 (Task 2), and 9.0 (Task 3) over CAT. As for M-OWODB, our MEPU-FS model achieves gains of 7.9 (Task 1), 11.8 (Task 2), and 5.7 (Task 3) in U-Recall due to our strong capability to overcome the label bias problem. Furthermore, as shown in Tab. [3,](#page-10-0) our\n\nMEPU-FS outperforms CAT and PROB in Unknown R@10 by 6.3 - 13.5, which indicates that our model can identify unknown objects with higher accuracy. MEPU-FS also largely improves Unknown R@100 by 3.2 - 17.0, demonstrating our advantage in retrieving unknown objects. Besides, we also achieve SOTA performance in terms of WI and A-OSE on S-OWODB Task 1-2 and M-OWODB Task 2, which shows that our method can effectively reduce confusion between unknown and known classes.\n\nKnown Object Detection. The four competing methods sacrifice the close-set performance for the openset detection, leading to large performance drops in Known-mAP across four tasks, e.g., maximally a decrease of 7.5 for ORE, 6.3 for OW-DETR, 3.9 for CAT, and 4.2 for PROB on S-OWODB. By contrast, our methods have nearly the same, or even better, closeset detection ability as the closed-set baseline, Faster-RCNN + Finetuning, on all four tasks. The consistently superior performance of our methods across the tasks is mainly because our unsupervised unknown object modeling is able to generate accurate pseudo labels.\n\nIncremental Detection. In task-incremental learning, MEPU-FS demonstrates superior performance compared to ORE and OW-DETR in detecting both previously and currently known objects. It achieves improvements ranging from 1.2 to 6.7 in previously-knownmAP and 7.0 to 12.6 in currently-known-mAP. Although our model may be relatively less effective on M-OWODB Tasks 3-4, it still outperforms CAT and PROB on other tasks. These results highlight the substantially enhanced ability of our methods to acquire new knowledge with-\n\n<span id=\"page-11-1\"></span>\n\n|                |         | Known | Unknown |      |            |  |\n|----------------|---------|-------|---------|------|------------|--|\n| Method         | Data    | mAP   | R@10    |      | R@30 R@100 |  |\n| Faster-RCNN    |         | 39.3  | 0.0     | 0.0  | 0.0        |  |\n| CAT            |         | 35.5  | 13.3    | 23.7 | 36.5       |  |\n| OpenDet        | LVIS    | 35.2  | 14.1    | 26.8 | 41.8       |  |\n| MEPU-SS (Ours) |         | 37.0  | 15.0    | 29.5 | 45.3       |  |\n| MEPU-FS (Ours) |         | 37.5  | 16.2    | 30.1 | 45.6       |  |\n| Faster-RCNN    |         | 38.2  | 0.0     | 0.0  | 0.0        |  |\n| CAT            |         | 35.1  | 11.9    | 21.7 | 38.8       |  |\n| OpenDet        | Objects | 34.0  | 13.6    | 26.9 | 42.0       |  |\n| MEPU-SS (Ours) | 365     | 35.5  | 16.5    | 30.1 | 45.8       |  |\n| MEPU-FS (Ours) |         | 35.7  | 17.2    | 30.5 | 46.0       |  |\n\nTable 4: Results of cross-dataset generalization.\n\nout catastrophically forgetting previously obtained knowledge.\n\n### 4.2.2 Cross-dataset Generalization\n\nTo evaluate the generalization ability for large datasets which is closer to real-world situations, the models are trained on COCO train2017 with 80 known classes, and evaluated on the validation set of LVIS and Objects365. The classes on LVIS and Objects365 beyond the 80 COCO classes are treated as unknown object classes. The results of our method MEPU-FS, MEPU-FS, the baseline Faster-RCNN, SOTA OSOD model OpenDet [\\[11\\]](#page-16-0), and SOTA OWOD model CAT are reported in Tab. [4.](#page-11-1) Our method MEPU-FS outperforms OpneDet in detecting unknown objects for both datasets in terms of R@10, R@30, and R@100, which shows our superiority in recognizing cross-dataset unknown objects. Further, MEPU-FS substantially outperforms OpneDet in Known-mAP by 2.3 on LVIS and 1.7 on Objects365, indicating a much stronger generalization ability in detecting known objects under the cross-dataset setting.\n\n# 4.2.3 Label Bias Evaluation\n\nTo further analyze the label bias problem discussed in Sec. [1,](#page-0-0) we propose a new dataset split that divides COCO into three parts: Known, Related Unknown, and Unrelated Unknown, as shown in Tab. [5.](#page-12-0) Classes within 'Related Unknown' have semantic relations with known ones. For example, there are five known animal classes and another five that belong to the 'Related Unknown' category. The 'Unrelated Unknown' classes belong to separate super-categories from the 'Known' ones. The models are trained only using the known object annotations in COCO train2017, and tested on COCO val2017 to evaluate the detection performance of known, related unknown, and unrelated unknown classes.\n\nThe evaluation results of our MEPU-FS, MEPU-SS, the most competitive method OpenDet, and CAT on the proposed split are demonstrated in Tab. [6.](#page-12-1) All models are trained using the known categories on MS-COCO, and tested on the known, related unknown, and unrelated unknown classes of COCO val2017 respectively. It can be observed that OpenDet can achieve similar performance as MEPU-FS in the 'Related Unknown' classes (e.g., 27.0 vs 27.8 in R@10), but it fails to detect the unknown objects which have no semantic relations to the known ones due to its severe label bias problem. Our MEPU-FS can effectively eliminate the label bias (e.g., + 11.3 Unrelated Unknown R@10) because we recognize unknown objects based on the shared region feature frequencies between the known and unknown classes, which can largely improve our generalization ability toward unknown objects, especially the unknown objects that have no semantic relations with the known classes.\n\n#### <span id=\"page-11-0\"></span>4.3 Ablation Study\n\nThe REW and ROLNet Modules. We analyze the contribution of two main modules, REW and ROL-Net, on the COCO dataset. The results of Known-mAP, Unknown R@10, R@30, and R@100 on the S-OWODB Task 1 are reported in Tab. [7.](#page-12-2) Without both components, the method is equivalent to directly training Faster-RCNN using proposals generated by unsupervised proposal methods as unknown pseudo labels. Such unknown pseudo labels are mostly inaccurate, making the detector confused with foreground and background regions, which harms both closed-set and openset performances. REW improves the Known-mAP significantly (+2.5 using FS and +3.6 using SS) and also contributes to unknown-recall performance, especially Unknown R@10 (+2.1 using FS and +8.5 using SS), mainly because it helps recognize true unknown objects from the noisy pseudo labels. ROLNet aims to extend the incomplete pseudo unknown labels, so it mainly improves the Unknown-R@100 (+6.6 using FS and +15.2 using SS). By combining REW and ROL-Net, we achieve our full model which effectively recognizes true unknown objects and accurately extends the incomplete unknown labels. This further improves the overall detection performance.\n\nImportance of Region Proposal Generators. We also evaluate the contribution of the unsupervised proposal generators to our model. As reported in Tab. [8,](#page-12-3) five unsupervised proposal generation methods are used and compared with a baseline method that does not use any proposal generators. The five methods include deep learning-based method DETReg [\\[17\\]](#page-16-4) and learning-free methods (EdgeBoxes [\\[13\\]](#page-16-3) and Geodesic Object Proposals (GOP) [\\[15\\]](#page-16-17)), in addition to FreeSOLO and Selec-\n\n<span id=\"page-12-0\"></span>\n\n|                | Known                                                                                                | Related Unknown                                                                        | Unrelated Unknown                                                |\n|----------------|------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|------------------------------------------------------------------|\n| Semantic split | Animals(5), Person(1),<br>Vehicles(4), Outdoor(3),<br>Furniture(3), Appliances(2),<br>Accessories(2) | Animals(5), Vehicles(4),<br>Outdoor(2), Furniture(3),<br>Appliances(3), Accessories(3) | Sports(10), Food(10),<br>Electronic(6), Indoor(7),<br>Kitchen(7) |\n\nTable 5: Proposed dataset split of MS COCO for evaluating the impact of label bias problem. Numbers in brackets represent the number of classes belonging to each super-category.\n\n<span id=\"page-12-1\"></span>\n\n|                | Known      | Related Unknown |            |            | Unrelated Unknown |             |            |  |\n|----------------|------------|-----------------|------------|------------|-------------------|-------------|------------|--|\n|                | mAP        | R@10            | R@30       | R@100      | R@10              | R@30        | R@100      |  |\n| CAT            | 63.8       | 25.1            | 41.2       | 52.1       | 11.6              | 23.3        | 39.1       |  |\n| OpenDet        | 62.2       | 27.0            | 43.5       | 59.5       | 13.7              | 26.5        | 45.5       |  |\n| MEPU-FS (Ours) | 66.9(+4.7) | 27.8(+0.8)      | 45.2(+1.7) | 60.9(+1.4) | 25.0(+11.3)       | 37.3(+10.8) | 51.1(+5.6) |  |\n| MEPU-SS (Ours) | 66.7(+4.5) | 27.2(+0.2)      | 44.6(+1.1) | 60.5(+1.0) | 21.3(+7.6)        | 36.0(+9.5)  | 51.0(+5.5) |  |\n\nTable 6: Results on the proposed dataset split for handling the label bias issue.\n\n<span id=\"page-12-2\"></span>\n\n|     | ROLNet | Proposal  | Known |      | Unknown |       |  |\n|-----|--------|-----------|-------|------|---------|-------|--|\n| REW |        | Generator | mAP   | R@10 | R@30    | R@100 |  |\n| %   | %      | FS        | 71.8  | 22.1 | 31.5    | 45.2  |  |\n| !   | %      | FS        | 74.1  | 24.2 | 36.6    | 50.3  |  |\n| %   | !      | FS        | 72.5  | 21.9 | 35.5    | 51.8  |  |\n| !   | !      | FS        | 74.3  | 25.1 | 39.4    | 54.8  |  |\n| %   | %      | SS        | 70.6  | 11.0 | 17.8    | 34.8  |  |\n| !   | %      | SS        | 74.0  | 19.5 | 30.9    | 49.1  |  |\n| %   | !      | SS        | 71.4  | 15.6 | 32.4    | 50.0  |  |\n| !   | !      | SS        | 74.2  | 21.2 | 36.2    | 54.5  |  |\n\nTable 7: Our full model (shaded) vs. its variants.\n\n<span id=\"page-12-3\"></span>\n\n|                       | Known | Unknown |      |       |\n|-----------------------|-------|---------|------|-------|\n| Proposal Generator    | mAP   | R@10    | R@30 | R@100 |\n| Baseline              | 73.5  | 16.3    | 31.2 | 48.2  |\n| DETReg [17]           | 74.2  | 22.5    | 37.2 | 53.5  |\n| EdgeBoxes [13]        | 73.6  | 21.5    | 36.5 | 52.7  |\n| GOP [15]              | 73.8  | 19.3    | 35.1 | 52.0  |\n| Selective Search [14] | 74.2  | 21.2    | 36.2 | 54.5  |\n| FreeSOLO [16]         | 74.3  | 25.1    | 39.4 | 54.8  |\n\nTable 8: Results of using different unsupervised region proposal methods to generate pseudo labels. 'Baseline' is a method trained using only selftraining without region proposal generators.\n\ntive Search. All proposal generators generally produce better results than the baseline, enabling substantially improved unknown object detection results, especially R@10 (e.g., +8.8 using FreeSOLO). Note that although the traditional methods, such as Selective Search and GOP, mostly generate coarse bounding boxes with poor localization quality, they still improve the final performance since our REW module can recognize true unknown objects from the noisy labels.\n\nAnalysis of Hyperparameters. Tab. [9](#page-12-4) shows the impact of using different settings of three key hyperpa-\n\n<span id=\"page-12-4\"></span>\n\n|     |    |   | Known |      | Unknown |       |\n|-----|----|---|-------|------|---------|-------|\n| γ   | P  | l | mAP   | R@10 | R@30    | R@100 |\n| 0.5 | 30 | 1 | 73.4  | 23.0 | 36.0    | 52.3  |\n| 1.0 | 30 | 1 | 73.8  | 24.0 | 38.2    | 54.5  |\n| 2.0 | 30 | 1 | 73.8  | 24.8 | 38.5    | 54.2  |\n| 4.0 | 30 | 1 | 74.3  | 25.1 | 39.4    | 54.8  |\n| 8.0 | 30 | 1 | 74.2  | 25.2 | 38.5    | 52.8  |\n| 4.0 | 10 | 1 | 74.1  | 25.2 | 38.6    | 52.3  |\n| 4.0 | 50 | 1 | 74.0  | 22.4 | 36.8    | 53.6  |\n| 4.0 | 70 | 1 | 73.8  | 21.0 | 35.3    | 53.0  |\n| 4.0 | 30 | 0 | 74.1  | 24.2 | 35.6    | 48.3  |\n| 4.0 | 30 | 2 | 74.2  | 24.4 | 38.5    | 55.4  |\n| 4.0 | 30 | 3 | 74.0  | 24.0 | 38.7    | 55.0  |\n\nTable 9: Analysis on hyperparameter settings.\n\nrameters on S-OWODB Task-1. γ controls the scores of the soft labels in REW. When adopting a small γ, it cannot effectively reduce the negative impact from the background regions; whereas a large γ can lead to the rejection of most pseudo labels. It can be observed that γ = 4.0 produces balanced results. P is to retain the top P % proposals as pseudo labels for self-training in ROLNet, so it affects the quality of pseudo labels. We find that P = 30 achieves a good trade-off between the unknown recalls. l is the number of self-training iterations in ROLNet. There is a significant performance gain compared with no self-training (i.e., l = 0), especially R@100, since ROLNet can extend the incomplete pseudo labels. While self-training for more than one iteration leads to a drop in R@10, because the extended labels can be less accurate (e.g., with poorer localization quality).\n\nReconstruction error-based OOD detection vs. REW. Previous reconstruction-based OOD detection models typically adopt reconstruction error as OOD\n\n<span id=\"page-13-0\"></span>![](_page_13_Picture_1.jpeg)\n\nFig. 7: Qualitative results of MEPU-FS on S-OWODB Task-1. We sample the images from COCO val2017 and plot the object predictions with scores higher than 0.3. Even though the known classes include persons, animals, and vehicles, our model can still recognize unknown objects, e.g., computer, chairs, and bottles, that do not have any semantic relation to the known ones.\n\nscores and reject all samples whose scores are higher than a pre-define threshold during testing. The comparison results of our REW model (soft labeling) with the base model (original reconstruction error-based OOD detection) are shown in Tab. [10.](#page-14-0) The threshold in the OOD detector is chosen at 95% true positive rate (foreground is the positive category). Our REW surpasses the base model in both known mAP and unknown recall, since (i) some true unknown objects may be filtered out by the non-adaptive hard thresholding in the base model, leading to high false negative errors, (ii) whereas the Weibull modeling in REW provides an adaptive recognition of unknown objects from the background regions, achieving substantially enhanced unknown object recognition performance.\n\nAlternative Designs in REW and ROLNet. We also evaluate different variants of REW and ROLNet on S-OWODB Task-1, with the results reported in Tab. [11.](#page-14-1) For REW, we test the effectiveness of replacing its offline training with an online training strategy, meaning we train the auto-encoder together with the detection model using the same backbone, rather than a separate reconstruction backbone; as for ROLNet, one alternative approach is to produce the pseudo labels directly using the predictions of RPN instead of OLN. The results show that, compared to our default model (MEPU-FS), using the online training in REW leads to a performance drop (-0.3 Known mAP and -4.1 Unknown R@100) because the downstream tasks in object detection affect the semantic expressiveness of dense features (e.g., it is biased towards features of known objects), which is also discussed in [\\[48,](#page-17-5) [49\\]](#page-17-6). When selftraining without employing OLN, the Unknown R@100 dropped by 6.1. This decrease can be attributed to the fact that the classification loss of Faster-RCNN heavily suppressed the scores of unlabeled regions, making it challenging to extend the detection to unlabeled objects.\n\n## 4.4 Qualitative Results.\n\nVisualizations of MEPU-FS. Fig. [7](#page-13-0) visualizes the object predictions of our full model MEPU-FS on S-OWODB Task-1. Although the known classes only include persons, animals, and vehicles, our method can\n\n<span id=\"page-14-2\"></span>![](_page_14_Picture_1.jpeg)\n\nFig. 8: Visualizations of our REW module on M-OWODB Task-1. The blue bounding boxes denote the pseudo labels generated by FreeSOLO on COCO-Task 1. The numbers on the top left corner of the blue boxes indicate the soft labels yielded by REW for the pseudo unknown objects. The red bounding boxes denote the known object labels.\n\n<span id=\"page-14-0\"></span>\n\n|                  | Proposal | Known | Unknown |      |       |\n|------------------|----------|-------|---------|------|-------|\n| Method           | Genrator | mAP   | R@10    | R@30 | R@100 |\n| Base Model       | FS       | 72.9  | 23.2    | 33.9 | 45.6  |\n| (Hard Threshold) | SS       | 71.6  | 15.2    | 27.3 | 43.1  |\n| REW              | FS       | 74.1  | 24.2    | 36.6 | 50.3  |\n| (Soft Labeling)  | SS       | 74.0  | 19.5    | 30.9 | 49.1  |\n\nTable 10: Our REW model (soft labeling) vs. base model (hard threshold).\n\n<span id=\"page-14-1\"></span>\n\n|                   | Known | Unknown |      |       |  |  |\n|-------------------|-------|---------|------|-------|--|--|\n| Design            | mAP   | R@10    | R@30 | R@100 |  |  |\n| Online Training   | 74.0  | 23.5    | 36.4 | 50.7  |  |  |\n| Self-Training     |       |         |      |       |  |  |\n| w/ RPN            | 74.2  | 23.9    | 35.6 | 48.7  |  |  |\n| Our Default Model | 74.3  | 25.1    | 39.4 | 54.8  |  |  |\n\nTable 11: Comparison to alternative REW and ROLNet designs. Either online training in REW or RPN-based self-training in ROLNet is used.\n\nstill recognize various types of unknown objects, e.g., computers, chairs, and sofas, which do not have any semantic relation to known ones. This remarkable capability can be attributed primarily to our REW module,\n\nwhich autonomously identifies unknown objects, ultimately mitigating the label bias problem.\n\nEffectiveness of REW. We further conducted a qualitative study of our REW module to provide a more detailed analysis. Fig. [8](#page-14-2) visualizes the soft labels generated by REW on S-OWODB Task-1, indicating the likelihood score of a pseudo object being a true unknown object. For true unknown object proposals or boxes covering both foreground and background regions, REW assigns relatively higher scores (e.g., 0.7 - 1.0) or medium scores (e.g., 0.3 - 0.7). In contrast, for non-object background regions such as the sky, road surface, floor, and white wall, REW produces lower scores (e.g., 0.0 - 0.3). These largely help enhance the accuracy of unknown object recognition by our detector. Notably, REW can assign high scores for unknown objects that do not exhibit similar appearances to known objects, since it utilizes the shared regional feature frequencies to recognize unknown objects. Instead, previous methods like OWOD/OSOD rely on objectness scores based on the supervision of known objects for unknown object pseudolabeling. Consequently, they suffer from a severe label\n\n<span id=\"page-15-4\"></span>![](_page_15_Picture_1.jpeg)\n\n(c) Incomplete predictions\n\nFig. 9: Failure cases of our method on S-OWODB Task-1.\n\nbias problem towards the known classes, which reduces their generalization ability.\n\nFailure Cases. Although our model exhibits superior performance in identifying unknown objects compared to previous methods, there are inevitably cases of failure in certain situations, since we do not leverage any labels of unknown classes. Fig. [9](#page-15-4) illustrates typical failure cases of MEPU, such as misclassifying backgrounds with complex patterns as unknowns, identifying parts of objects as unknowns, or producing incomplete predictions.\n\n# 5 Conclusion\n\nThis work proposes the MEPU approach that addresses the label bias problem in OWOD by identifying unknown objects in an unsupervised manner and iteratively extending the identified pseudo unknown objects. To this end, it first performs an unsupervised recognition of unknown objects from the pseudo labels generated by unsupervised region proposal methods, and then extends new unknown objects that are not covered by the pseudo labels. These two tasks are implemented by two main components, REW and ROLNet. Experiments show that our MEPU approach achieves superior performance in detecting unknown objects on S-OWODB and M-OWODB. Besides, our method also outperforms existing models on cross-dataset experiments on the LVIS and Object365 datasets. Although the detection performance gap between known objects and unknown ones is still large, our work makes a step forward in OWOD for real-world applications.\n\n#### References\n\n- <span id=\"page-15-0\"></span>1. S. Ren, K. He, R. Girshick, and J. Sun, \"Faster r-cnn: Towards real-time object detection with region proposal networks,\" Advances in neural information processing systems, vol. 28, 2015.\n- 2. J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You only look once: Unified, real-time object detection,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779–788.\n- 3. X. Zhou, D. Wang, and P. Kr¨ahenb¨uhl, \"Objects as points,\" arXiv preprint arXiv:1904.07850, 2019.\n- 4. T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar, \"Focal loss for dense object detection,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2980–2988.\n- <span id=\"page-15-1\"></span>5. X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \"Deformable detr: Deformable transformers for end-toend object detection,\" arXiv preprint arXiv:2010.04159, 2020.\n- <span id=\"page-15-2\"></span>6. A. Dhamija, M. Gunther, J. Ventura, and T. Boult, \"The overlooked elephant of object detection: Open set,\" in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp. 1021–1030.\n- <span id=\"page-15-3\"></span>7. K. Joseph, S. Khan, F. S. Khan, and V. N. Balasubramanian, \"Towards open world object detection,\" in Proceed-\n\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 5830–5840.\n\n- <span id=\"page-16-7\"></span>8. A. Gupta, S. Narayan, K. Joseph, S. Khan, F. S. Khan, and M. Shah, \"Ow-detr: Open-world detection transformer,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 9235–9244.\n- <span id=\"page-16-15\"></span>9. O. Zohar, K.-C. Wang, and S. Yeung, \"Prob: Probabilistic objectness for open world object detection,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 11 444–11 453.\n- <span id=\"page-16-1\"></span>10. S. Ma, Y. Wang, Y. Wei, J. Fan, T. H. Li, H. Liu, and F. Lv, \"Cat: Localization and identification cascade detection transformer for open-world object detection,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 19 681–19 690.\n- <span id=\"page-16-0\"></span>11. J. Han, Y. Ren, J. Ding, X. Pan, K. Yan, and G.-S. Xia, \"Expanding low-density latent regions for open-set object detection,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 9591–9600.\n- <span id=\"page-16-2\"></span>12. N. Dong, Y. Zhang, M. Ding, and G. H. Lee, \"Open world detr: transformer based open world object detection,\" arXiv preprint arXiv:2212.02969, 2022.\n- <span id=\"page-16-3\"></span>13. C. L. Zitnick and P. Doll´ar, \"Edge boxes: Locating object proposals from edges,\" in European conference on computer vision. Springer, 2014, pp. 391–405.\n- <span id=\"page-16-20\"></span>14. J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders, \"Selective search for object recognition,\" International journal of computer vision, vol. 104, no. 2, pp. 154–171, 2013.\n- <span id=\"page-16-17\"></span>15. P. Kr¨ahenb¨uhl and V. Koltun, \"Geodesic object proposals,\" in European conference on computer vision. Springer, 2014, pp. 725–739.\n- <span id=\"page-16-21\"></span>16. X. Wang, Z. Yu, S. De Mello, J. Kautz, A. Anandkumar, C. Shen, and J. M. Alvarez, \"Freesolo: Learning to segment objects without annotations,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 14 176–14 186.\n- <span id=\"page-16-4\"></span>17. A. Bar, X. Wang, V. Kantorov, C. J. Reed, R. Herzig, G. Chechik, A. Rohrbach, T. Darrell, and A. Globerson, \"Detreg: Unsupervised pretraining with region priors for object detection,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 14 605–14 615.\n- <span id=\"page-16-5\"></span>18. T. Denouden, R. Salay, K. Czarnecki, V. Abdelzad, B. Phan, and S. Vernekar, \"Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance,\" arXiv preprint arXiv:1812.02765, 2018.\n- <span id=\"page-16-23\"></span>19. Y. Zhou, \"Rethinking reconstruction autoencoder-based out-of-distribution detection,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 7379–7387.\n- <span id=\"page-16-24\"></span>20. W. Jiang, Y. Ge, H. Cheng, M. Chen, S. Feng, and C. Wang, \"Read: Aggregating reconstruction error into out-of-distribution detection,\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 12, 2023, pp. 14 910–14 918.\n- <span id=\"page-16-6\"></span>21. G. Osada, T. Takahashi, B. Ahsan, and T. Nishide, \"Out-of-distribution detection with reconstruction error and typicality-based penalty,\" in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023, pp. 5551–5563.\n- <span id=\"page-16-8\"></span>22. D. Kim, T.-Y. Lin, A. Angelova, I. S. Kweon, and W. Kuo, \"Learning open-world object proposals without learning to classify,\" IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 5453–5460, 2022.\n- <span id=\"page-16-9\"></span>23. D. Miller, F. Dayoub, M. Milford, and N. S¨underhauf, \"Evaluating merging strategies for sampling-based uncertainty techniques in object detection,\" in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 2348–2354.\n- <span id=\"page-16-10\"></span>24. D. Miller, L. Nicholson, F. Dayoub, and N. S¨underhauf, \"Dropout sampling for robust object detection in openset conditions,\" in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 3243–3249.\n- <span id=\"page-16-11\"></span>25. X. Du, Z. Wang, M. Cai, and Y. Li, \"Vos: Learning what you don't know by virtual outlier synthesis,\" arXiv preprint arXiv:2202.01197, 2022.\n- <span id=\"page-16-12\"></span>26. K. Saito, P. Hu, T. Darrell, and K. Saenko, \"Learning to detect every thing in an open world,\" in European Conference on Computer Vision. Springer, 2022, pp. 268–284.\n- <span id=\"page-16-14\"></span>27. W. Wang, M. Feiszli, H. Wang, J. Malik, and D. Tran, \"Open-world instance segmentation: Exploiting pseudo ground truth from learned pairwise affinity,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 4422–4432.\n- <span id=\"page-16-13\"></span>28. L. Qi, J. Kuen, Y. Wang, J. Gu, H. Zhao, P. Torr, Z. Lin, and J. Jia, \"Open world entity segmentation,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n- <span id=\"page-16-16\"></span>29. Z. Wu, Y. Lu, X. Chen, Z. Wu, L. Kang, and J. Yu, \"Ucowod: Unknown-classified open world object detection,\" arXiv e-prints, pp. arXiv–2207, 2022.\n- <span id=\"page-16-18\"></span>30. B. Alexe, T. Deselaers, and V. Ferrari, \"What is an object?\" in 2010 IEEE computer society conference on computer vision and pattern recognition. IEEE, 2010, pp. 73–80.\n- 31. J. Carreira and C. Sminchisescu, \"Cpmc: Automatic object segmentation using constrained parametric mincuts,\" IEEE transactions on pattern analysis and machine intelligence, vol. 34, no. 7, pp. 1312–1328, 2011.\n- <span id=\"page-16-19\"></span>32. J. Pont-Tuset, P. Arbelaez, J. T. Barron, F. Marques, and J. Malik, \"Multiscale combinatorial grouping for image segmentation and object proposal generation,\" IEEE transactions on pattern analysis and machine intelligence, vol. 39, no. 1, pp. 128–140, 2016.\n- <span id=\"page-16-22\"></span>33. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248–255.\n- <span id=\"page-16-25\"></span>34. K. Shmelkov, C. Schmid, and K. Alahari, \"Incremental learning of object detectors without catastrophic forgetting,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 3400–3409.\n- 35. Y. Hao, Y. Fu, Y.-G. Jiang, and Q. Tian, \"An end-toend architecture for class-incremental object detection with knowledge distillation,\" in 2019 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2019, pp. 1–6.\n- 36. J. Kj, J. Rajasegaran, S. Khan, F. S. Khan, and V. N. Balasubramanian, \"Incremental object detection via meta-learning,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n- <span id=\"page-16-26\"></span>37. B. Yang, X. Deng, H. Shi, C. Li, G. Zhang, H. Xu, S. Zhao, L. Lin, and X. Liang, \"Continual object detection via prototypical task correlation guided gating mechanism,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 9255–9264.\n- <span id=\"page-16-27\"></span>38. I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio, \"An empirical investigation of catastrophic\n\nforgetting in gradient-based neural networks,\" arXiv preprint arXiv:1312.6211, 2013.\n\n- 39. R. M. French, \"Catastrophic forgetting in connectionist networks,\" Trends in cognitive sciences, vol. 3, no. 4, pp. 128–135, 1999.\n- <span id=\"page-17-0\"></span>40. M. McCloskey and N. J. Cohen, \"Catastrophic interference in connectionist networks: The sequential learning problem,\" in Psychology of learning and motivation. Elsevier, 1989, vol. 24, pp. 109–165.\n- <span id=\"page-17-1\"></span>41. T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L. Zitnick, \"Microsoft coco: Common objects in context,\" in European conference on computer vision. Springer, 2014, pp. 740–755.\n- <span id=\"page-17-2\"></span>42. K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, \"Mask r-cnn,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2961–2969.\n- <span id=\"page-17-3\"></span>43. Q. Yang, X. Wei, B. Wang, X.-S. Hua, and L. Zhang, \"Interactive self-training with mean teachers for semisupervised object detection,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 5941–5950.\n- 44. B. Chen, W. Chen, S. Yang, Y. Xuan, J. Song, D. Xie, S. Pu, M. Song, and Y. Zhuang, \"Label matching semi-supervised object detection,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 14 381–14 390.\n- 45. K. Sohn, Z. Zhang, C.-L. Li, H. Zhang, C.-Y. Lee, and T. Pfister, \"A simple semi-supervised learning framework for object detection,\" arXiv preprint arXiv:2005.04757, 2020.\n- 46. Q. Zhou, C. Yu, Z. Wang, Q. Qian, and H. Li, \"Instantteaching: An end-to-end semi-supervised object detection framework,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 4081–4090.\n- <span id=\"page-17-4\"></span>47. P. Mi, J. Lin, Y. Zhou, Y. Shen, G. Luo, X. Sun, L. Cao, R. Fu, Q. Xu, and R. Ji, \"Active teacher for semi-supervised object detection,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 14 482–14 491.\n- <span id=\"page-17-5\"></span>48. Y.-C. Liu, C.-Y. Ma, X. Dai, J. Tian, P. Vajda, Z. He, and Z. Kira, \"Open-set semi-supervised object detection,\" arXiv preprint arXiv:2208.13722, 2022.\n- <span id=\"page-17-6\"></span>49. J. Winkens, R. Bunel, A. G. Roy, R. Stanforth, V. Natarajan, J. R. Ledsam, P. MacWilliams, P. Kohli, A. Karthikesalingam, S. Kohl et al., \"Contrastive training for improved out-of-distribution detection,\" arXiv preprint arXiv:2007.05566, 2020.\n- <span id=\"page-17-7\"></span>50. F. Wei, Y. Gao, Z. Wu, H. Hu, and S. Lin, \"Aligning pretraining for detection via object-level contrastive learning,\" Advances in Neural Information Processing Systems, vol. 34, pp. 22 682–22 694, 2021.\n- <span id=\"page-17-8\"></span>51. Z. Li and D. Hoiem, \"Learning without forgetting,\" IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 12, pp. 2935–2947, 2017.\n- 52. S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \"icarl: Incremental classifier and representation learning,\" in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2017, pp. 2001–2010.\n- 53. P. Dhar, R. V. Singh, K.-C. Peng, Z. Wu, and R. Chellappa, \"Learning without memorizing,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 5138–5146.\n- <span id=\"page-17-9\"></span>54. A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny, \"Efficient lifelong learning with a-gem,\" in International Conference on Learning Representations, 2018.\n- <span id=\"page-17-10\"></span>55. A. Gupta, P. Dollar, and R. Girshick, \"Lvis: A dataset for large vocabulary instance segmentation,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 5356–5364.\n- <span id=\"page-17-11\"></span>56. S. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li, and J. Sun, \"Objects365: A large-scale, highquality dataset for object detection,\" in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 8430–8439.\n- <span id=\"page-17-12\"></span>57. Y. Wu, A. Kirillov, F. Massa, W.-Y. Lo, and R. Girshick, \"Detectron2,\" [https://github.com/facebookresearch/](https://github.com/facebookresearch/detectron2) [detectron2,](https://github.com/facebookresearch/detectron2) 2019.\n- <span id=\"page-17-13\"></span>58. K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.\n- <span id=\"page-17-14\"></span>59. T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie, \"Feature pyramid networks for object detection,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2117– 2125.\\n，请你分析其研究动机、核心方法与公式推导细节。请结合摘要与正文信息，提取论文背景、问题定义、方法核心流程与理论基础。\n",
        "agent": "论文解读专家\n",
        "status": "started"
    }
]
