[
    {
        "timestamp": "2025-06-13 16:11:57",
        "task_name": "research_task",
        "task": "阅读论文《Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss》，论文于2024-03-12发布于arXiv.org内容如下：\\n# Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss\n\nXuhua Ren MiHOYO Shanghai, China renxuhua1993@gmail.com\n\nHengcan Shi Monash University Melbourne, Australia shihengcan@gmail.com\n\nJin Li Shaanxi Normal University Xian, China\n\n# Abstract\n\n*Scene text recognition is an important and challenging task in computer vision. However, most prior works focus on recognizing pre-defined words, while there are various out-of-vocabulary (OOV) words in real-world applications. In this paper, we propose a novel open-vocabulary text recognition framework, Pseudo-OCR, to recognize OOV words. The key challenge in this task is the lack of OOV training data. To solve this problem, we first propose a pseudo label generation module that leverages character detection and image inpainting to produce substantial pseudo OOV training data from real-world images. Unlike previous synthetic data, our pseudo OOV data contains real characters and backgrounds to simulate real-world applications. Secondly, to reduce noises in pseudo data, we present a semantic checking mechanism to filter semantically meaningful data. Thirdly, we introduce a qualityaware margin loss to boost the training with pseudo data. Our loss includes a margin-based part to enhance the classification ability, and a quality-aware part to penalize lowquality samples in both real and pseudo data. Extensive experiments demonstrate that our approach outperforms the state-of-the-art on eight datasets and achieves the first rank in the ICDAR2022 challenge.*\n\n# 1. Introduction\n\nScene text recognition expects to recognize all characters in an image. It serves as a crucial component in various computer vision applications, such as image captioning [\\[26\\]](#page-9-0), image generation [\\[18,](#page-8-0) [35\\]](#page-9-1), multi-modal systems [\\[11,](#page-8-1) [56\\]](#page-10-0) and image retrieval [\\[50,](#page-9-2) [43\\]](#page-9-3).\n\nThe existing scene text recognition methods [\\[16,](#page-8-2) [41,](#page-9-4) [37\\]](#page-9-5) can be mainly divided into two types: segmentationbased and context-aware. Segmentation-based approaches [\\[16,](#page-8-2) [41\\]](#page-9-4) first segment or detect every character in the image, and then separately recognize them. Nevertheless, they ignore the significant semantic correlations among characters, which are crucial for accurate character recognition. Context-aware works [\\[37\\]](#page-9-5) leverage word-level prior knowledge to learn the relationships among characters to improve text recognition, which usually generates more accurate results than segmentation-based methods. However, they heavily rely on pre-defined words. Real-world applications usually involve diverse words and even new words such as 'WASSUP', which are hard to be fully covered by the predefined dictionary.\n\nSeveral works [\\[45\\]](#page-9-6) notice this challenge and present the open-vocabulary scene text recognition task, which expects to recognize not only in-vocabulary (IV) words but also out-of-vocabulary (OOV) ones. For this task, although prior works [\\[45\\]](#page-9-6) employ multiple decoders to improve their generalization ability, the OOV word recognition accuracy is only slightly improved. A key reason is the lack of OOV training samples. To obtain OOV training samples, a straightforward way is data synthesis. Some methods [\\[20,](#page-8-3) [43\\]](#page-9-3) synthesize pseudo text images to improve text recognition performance. However, as shown in Fig. [1,](#page-1-0) these synthetic images only contain computer-generated characters and lack lighting, distortion as well as texture, which are important for scene text recognition in real-world applications.\n\nIn this paper, we propose a novel Pseudo-OCR framework for open-vocabulary text recognition, which includes a pseudo label generation module to capture real-imagebased pseudo data and a quality-aware margin loss to enhance the model training. Our pseudo label generation module first leverages a character detector to decompose real images into character and background elements, and then generates various OOV pseudo labels from these elements. To obtain more real-like pseudo labels, we employ image inpainting technologies to optimize the appearance of our pseudo labels. In this way, our pseudo labels contain real characters, lighting, distortion and texture, and thus are more suitable for real-world applications, as shown in Fig. [1.](#page-1-0) However, there are inevitable noises in pseudo data,\n\n![](_page_1_Figure_0.jpeg)\n\n<span id=\"page-1-0\"></span>Figure 1. Our proposed method builds a pseudo label generation system using real images. For example, the model produces two pseudo labels, 'PARS' and 'PASIR' from the real 'PARIS' image. The \"PARS\" is an IV word while \"PASIR\" is an OOV word. Compared with traditional synthetic images, our pseudo labels are closer to real-world images.\n\nsuch as semantically unreasonable data. Therefore, we propose a semantic checking mechanism to filter and correct our pseudo labels. Specifically, we compare pseudo labels with a large-scale dictionary. If a pseudo label is close to a word in the dictionary, we correct the pseudo label to that word. If a pseudo label largely differs from every word in the dictionary, we filter out it. After our semantic checking, semantically meaningful pseudo labels can be captured.\n\nMeanwhile, a quality-aware margin loss is also presented to better leverage real and our pseudo labels for training. In both pseudo and real training data, low-quality labels (such as unclear characters and missing annotations) affect the model performance. To address challenges, we took inspiration from face recognition, where low-quality faces can significantly reduce performance. Inspired by MagFace [\\[30\\]](#page-9-7), we adopted margin loss that improves the classification ability by enlarging class margins. More importantly, we design a quality indicator specific to text recognition images. This indicator is based on the confidence of the character detection model to identify low-quality areas. We then penalize these low-quality samples in the margin loss.\n\nIn summary, our proposed work enhances previous research in three key areas:\n\n- 1. We present a novel approach, Pseudo-OCR, to address the open-vocabulary text recognition problem. Pseudo-OCR is able to generate real-image-based pseudo labels for better OOV word recognition. A semantic checking mechanism is also proposed to reduce noises in pseudo labels.\n- 2. We further design a quality-aware margin loss to improve the training with pseudo labels, which contains a margin loss to enhance classification, and a quality indicator to detect and penalize low-quality labels.\n- 3. Experiments show that our approach significantly outperforms previous state-of-the-art methods on eight datasets (i.e., IIIT5k, SVT, IC13, IC13, IC15, IC15, SVTP and CUTE). Moreover, our method achieves the first rank in the ICDAR2022 challenge.\n\n# 2. Related work\n\n#### 2.1. Scene Text Recognition\n\nIn the past year, the attention-based decoder [\\[6\\]](#page-8-4) has been considered the state-of-the-art pipeline for text recognition, incorporating language modeling, weak character detection supervision, and character recognition in a unified system. Comprised of four essential parts [\\[58\\]](#page-10-1): i) an autoregressively predicting attention-based decoder, ii) a context-modeling Bi-LSTM layer [\\[19\\]](#page-8-5), iii) a featureextracting convolutional encoder, and iv) a rectification network to straighten irregular text images, this system distinguishes itself from the Connectionist Temporal Classification (CTC) layer [\\[12\\]](#page-8-6) by considering dependencies in the output character space. Several research studies have utilized semantic segmentation for scene text recognition. TextScanner [\\[44\\]](#page-9-8), for example, generates pixel-wise, multichannel segmentation maps that determine the character class, position, and order. To enhance the recognition process, it also employs an RNN for context modeling. Another study conducted by Liao et al. [\\[27\\]](#page-9-9) proposed the CA-FCN model, which uses a semantic segmentation network and an attention mechanism for characters. The CA-FCN model also includes a word formation module, enabling it to simultaneously recognize the script and predict the position of each character.\n\nRecently, most context-aware scene text recognition methods use semantics learned from data to enhance recognition [\\[34\\]](#page-9-10). The majority of these approaches utilize Transformers to learn internal language models (LMs) through standard autoregressive (AR) training [\\[3\\]](#page-8-7). Unlike previous methods, PARSeq [\\[3\\]](#page-8-7) utilizes Permuted Language Modeling (PLM) [\\[8\\]](#page-8-8) instead of standard AR modeling to learn an internal LM. This distinguishes PARSeq from traditional ensemble methods that rely on external LMs for prediction refinement.\n\nHowever, current methods in scene text recognition do not address the OOV problem, which refers to the difficulties encountered when recognizing words outside of the vocabulary encountered during training. This challenge was first highlighted by Wan et al. [\\[45\\]](#page-9-6), who found that state-ofthe-art methods excel at recognizing in-vocabulary words that have been previously seen, but struggle with OOV words. To tackle this issue, the authors introduced a mutual learning strategy that optimizes two types of decoders simultaneously, resulting in improved performance. However, this approach seems to rely solely on ensemble methods to enhance model performance, without considering the issue from the standpoint of the training data.\n\n#### 2.2. Pseudo Label in Scene Text Recognition\n\nThe self-training approach leverages existing detectors to generate pseudo-labels for unlabeled data. Highconfidence pseudo-labels are then selected and used for retraining. Wang et al. [\\[48\\]](#page-9-11) employed tracking in videos to acquire difficult examples, which were then used to retrain their detector. This additional data enhanced the detection accuracy of still images. Image synthesis is another method used for generating pseudo-labels. One widely used algorithm for image synthesis is Synthtext[\\[20\\]](#page-8-3). This algorithm utilizes deep learning and segmentation techniques to generate synthetic images of text that blend naturally with existing natural scenes, aligning the text with the geometry of the background image and respecting scene boundaries. Despite its usefulness, Synthtext has some limitations. For instance, it may suffer from semantic incoherence and the presence of non-homogeneous regions. To address these limitations, Zhan et al. proposed the Verisimilar [\\[43\\]](#page-9-3) method, which leverages the semantic annotations of objects and image regions created through prior semantic segmentation. This method also uses visual saliency to determine the embedding locations within each semantic sensible region, resulting in semantic coherent synthesis. Despite these advancements, the generated images using both Synthtext and Verisimilar methods still have a noticeable gap with real images.\n\n### 2.3. Margin Based Loss\n\nThe margin-based softmax loss function is commonly used to train face recognition (FR) models. The addition of margin to the softmax loss improves the discriminative power of the learned features. There are various forms of margin functions introduced in models such as CosFace [\\[46\\]](#page-9-12), and ArcFace [\\[9\\]](#page-8-9). ArcFace is often referred to as an angular margin, while CosFace is known as an additive margin. MagFace [\\[30\\]](#page-9-7) is a unique approach that assigns different margins based on recognizability, using larger angular margins for high-norm features as they are easier to recognize. Some models [\\[24\\]](#page-8-10) consider the margin as a function of image quality, since it can greatly affect which samples provide the strongest gradient or learning signal during training. However, none of these models take into account the unique challenges posed by scene text images, which require a more tailored measure of text image quality, rather than relying solely on image gradient.\n\n# 3. Methodology\n\n## 3.1. Overview\n\nWe propose a novel Pseudo-OCR framework that incorporates pseudo-image labeling and margin loss to address the open-vocabulary scene text recognition problem. Our framework consists of three parts, as illustrated in Fig. [2.](#page-3-0) (a) A pseudo label generation model including character detector, image inpainting and semantic checking is first proposed to generate pseudo OOV training samples. (b) A text recognition network recognizes all characters from the input image. (c) A quality-aware margin loss is used to train the network, and leverages a quality indicator to penalize low-quality samples. Our proposed pipeline can be trained end-to-end, without the need for any additional manual labeling for the aforementioned modules. During inference, only the recognition network is required, and the pseudo label generation module as well as the loss can be omitted. Next, we introduce the details of each part.\n\n#### 3.2. Pseudo label generation\n\nThe first step in pseudo-labeling for scene text recognition is to locate each character. Then, we modify both the image and text to generate new paired data with valid semantics. This system involves four key components: character detector, image inpainting, image augmentation, and semantic checking. The character detector is used to identify and locate each character in the image. Image inpainting is used to fill in the chosen regions of the image. Image augmentation is used to generate a variety of new images from the original image. Finally, semantic checking is used to verify if the generated image-text pair is valid.\n\nCharacter Detector: We have used a general object detector, YOLO model, for character detection. In the detection branch of this model, character-level features can be easily obtained by detecting characters within the proposals. To extract both global and character-level features I, we applied RoIAlign [\\[14\\]](#page-8-11) to the text proposals generated by FPN. The RoIAlign layer then extracted the features F(I) from the proposals and performed character detection. The extracted features are 7 × 7 in size and are fused by element-wise summation. They are then passed through a 3 × 3 convolution layer and a 1 × 1 layer to create the final fused features for classification Lcls and bounding box regression Lbox. The training data for this module is generated synthetically through Synthtext [\\[13\\]](#page-8-12), without the use of manual-label data.\n\nImage Inpainting: We use a UNet-like architecture based on the one presented by Liu et al. [\\[28\\]](#page-9-13). which incorporates skip connections between mirrored layers, to great effect in object segmentation and image-to-image transla-\n\n![](_page_3_Figure_0.jpeg)\n\n<span id=\"page-3-0\"></span>Figure 2. The proposed Pseudo-OCR contains three parts: (a) A pseudo label generation module based on character detector, image inpainting and semantic checking to obtain pseudo labels; (b) a text recognition network with a ViT encoder and a permutation decoder to predict the word in the image; and (c) a quality-aware margin loss including a quality indicator to train the model. In the inference stage, only the text recognition network is used for predicting.\n\ntion tasks. We apply this mechanism to the up-sampling process, where previous encoding feature maps of the same size are concatenated to retain richer texture and restore the lost background information from the downsampling process. Our training data is generated from Synthtext, where random font, color, and deformation parameters are selected to create styled text which is then rendered onto background images. This generates the corresponding background, foreground text, and overall text all at once.\n\nImage Augmentation: As depicted in Fig. [2,](#page-3-0) we employ two strategies to fabricate novel text image datasets. The initial strategy involves an image inpainting module. We randomly select a character and, cognizant of its specific coordinates, erase it from the image. Subsequently, we feed the entire image into the image inpainting module, resulting in the production of a new image without the selected character. Simultaneously, we also remove the corresponding character from the ground truth, thereby obtaining the updated ground truth for the character. During this process, we eliminate characters with low confidence scores and images with confidence scores that are insufficiently low will not be subjected to the inpainting module. Our second approach involves randomly selecting two characters, harvesting their local images, and swapping their positions. Since the characters come from the same image, it generally does not cause any variations in the image style. By these two strategies, we can generate a multitude of novel datasets for the training of our text recognition models.\n\nSemantic Checking: If newly generated words are meaningless, they can have a detrimental effect on the training process of a model. To address this, we have compiled a comprehensive vocabulary list that includes common words from scene text image recognition datasets, as well as external natural language vocabularies [1](#page-3-1) . Words that are not included in the list after undergoing random character deletion or swapping are discarded. Additionally, we have implemented classical data augmentation techniques, such as removing the first and last characters, to simulate the truncation effect that often occurs in images.\n\n#### 3.3. Text recognition network\n\nWe utilize PARSeq [\\[3\\]](#page-8-7) as our text recognition network, which employs an ensemble of internal AR models with shared weights through PLM. PARSeq follows an encoderdecoder architecture, which is commonly used in sequence modeling tasks. The encoder has 12 layers, while the decoder consists of a single layer.\n\nThe ViT is a direct extension of the Transformer architecture to images. A ViT layer contains a single Multi-Head Attention (MHA) module used for self-attention. The encoder is a 12-layer ViT, excluding the classification head. Given an image x ∈ RW×H×<sup>C</sup> , where W represents the width, H represents the height, and C represents the number of channels, the image is tokenized by dividing it into t<sup>w</sup> × t<sup>h</sup> patches of equal size, flattening each patch, and linearly projecting them into dmodel-dimensional tokens using a patch embedding matrix Wp. This results in (W × H)/(t<sup>w</sup> × th) tokens. Prior to being processed by the first ViT layer, learned position embeddings of equal dimension are added to the tokens.\n\nThe decoder architecture of our model is based on the preLayerNorm Transformer decoder, but it features twice the number of attention heads. The decoder requires three inputs: position tokens, context tokens, and image tokens, along with an optional attention mask. The position tokens encode the target position to be predicted, and each token corresponds directly to a specific position in the output sequence. This parameterization is similar to the query stream of two-stream attention. The attention mask varies depending on how the model is used. During training, we generate masks using random permutations. At inference, the mask\n\n<span id=\"page-3-1\"></span><sup>1</sup><https://www.wordfrequency.info/>\n\ncan be a standard left-to-right lookahead mask for AR decoding, a cloze mask for iterative refinement, or no mask at all for non-autoregressive (NAR) decoding.\n\n#### 3.4. Quality-aware margin loss\n\nIn this section, a novel loss function is proposed to learn a more discriminative word recognition model, where the score of character detection is introduced as the penalty term in the MagFace loss. In details, the set of image (patch) features in the is indicated as X ∈ R<sup>d</sup>×<sup>N</sup> , containing N Ddimensional features, x1, x2, ..., x<sup>i</sup> , ...x<sup>N</sup> . The characterlevel classifier identifies each feature into M = 94 classes including letters and common symbols.\n\nThe MagFace loss function is designed to maximize the separation between classes in the feature space, making it an effective solution for open-vocabulary tasks. It ignores irrelevant factors such as background and text in images by focusing on maximizing the cosine similarity between the embeddings of the text images. This results in the model producing distinct and discriminative embeddings for each class, reducing the risk of misclassification and improving the overall performance of the task.\n\nWe have designed a loss function that overcomes the limitation of MagFace, which does not consider the quality of scene-text images. This can lead to decreased performance if the data includes a high number of low-quality images. Our approach incorporates a quality indicator module that uses a character detector to provide a more reliable indicator compared to MagFace's image-based magnitude as the quality evaluation indicator. Experimental results show that our method outperforms MagFace and is suitable for scene text images,\n\n<span id=\"page-4-0\"></span>\n$$\nL_m = -\\frac{1}{N}\n$$\n  \n\n$$\n\\sum_{i=1}^{N} \\left( \\log \\frac{e^{s(\\cos(\\theta_{y_i} + m(a_i)))}}{e^{s(\\cos(\\theta_{y_i} + m(a_i)))} + \\sum_{j \\neq y_i} e^{s \\cos(\\theta_j)}} \\right).\n$$\n (1)\n\nIn Eq. [\\(1\\)](#page-4-0), the term m(ai) is designed as a monotonically increasing function of the character box confidence ai . Our observations suggest that the confidence of character detection is strongly correlated with the quality of the text image. When the text in the image is clear, the detector can accurately detect the position of the characters. Conversely, when the image quality is poor, the detector often produces low confidence results or fails to detect text boxes altogether. Building on this observation, we have defined our function to account for the varying levels of confidence in character detection that correspond to the quality of the text image,\n\n$$\nm(a_i) = \\frac{u_m - l_m}{u_a - l_a} \\times (a_i - l_a) + l_m,\n$$\n (2)\n\nwhere u<sup>m</sup> and l<sup>m</sup> are the upper and lower bounds of the margin m, and u<sup>a</sup> and l<sup>a</sup> are the upper and lower bounds of the input confidence a<sup>i</sup> . The variable a<sup>i</sup> represents the average confidence of N bounding boxes in i -th image and is defined as follows,\n\n$$\na_i = \\frac{\\sum_{j=1}^{N} c_j}{N},\\tag{3}\n$$\n\nwhere c<sup>j</sup> is the confidence of the j-th bounding box and N is the total number of bounding boxes, our loss introduces an adaptive mechanism to learn a well-structured withinclass feature distribution. This is achieved by pulling easy samples towards class centers while pushing hard samples away, which prevents models from overfitting to noisy, lowquality samples, thereby improving scene text recognition performance.\n\n# 4. Experiment\n\n#### 4.1. Experimental Settings\n\nTraining datasets. Our experimental setup is consistent with previous work in the field. For synthetic training datasets, we utilize MJSynth (MJ) [\\[20\\]](#page-8-3) and SynthText (ST) to train proposed modules or compare with other methods. Additionally, we use real data for training, including COCO-Text (COCO) [\\[42\\]](#page-9-14), RCTW17 [\\[38\\]](#page-9-15), Uber-Text (Uber) [\\[55\\]](#page-10-2), ArT [\\[5\\]](#page-8-13), LSVT [\\[40\\]](#page-9-16), MLT19 [\\[32\\]](#page-9-17), and ReCTS [\\[54\\]](#page-10-3). We also leverage two large-scale real datasets based on Open Images: TextOCR [\\[39\\]](#page-9-18) and OpenVINO [\\[25\\]](#page-8-14).\n\nTesting datasets. We use the IIIT5K [\\[31\\]](#page-9-19), CUTE80 [\\[36\\]](#page-9-20), Street View Text (SVT) [\\[47\\]](#page-9-21), SVT-Perspective (SVTP) [\\[33\\]](#page-9-22), ICDAR 2013 (IC13) [\\[23\\]](#page-8-15), and ICDAR 2015 (IC15) [\\[22\\]](#page-8-16) datasets for evaluation, following the previous method setting. We use the case-sensitive annotations of Long and Yao [\\[29\\]](#page-9-23) for IIIT5k, CUTE, SVT, and SVTP. For IC13 and IC15, there are two versions of their respective test splits commonly used in the literature: 857 and 1,015 for IC13; 1,811 and 2,077 for IC15. To avoid confusion, we refer to the benchmark as the union of IIIT5k, CUTE, SVT, SVTP, IC13 (857), IC13 (1,015), IC15 (1,811) and IC15 (2,077).\n\nEvaluation metrics. We use Correctly Recognized Words (CRW) as our metric, which is the percentage of correctly predicted words in the entire dataset. A prediction is considered as correct only if all characters in the recognized text match the corresponding characters in the ground truth word. We reported mean values obtained from three replicates per model.\n\nCharacter sets. Varying during both training and inference. Specifically, we use two charsets: a 36-character set containing lowercase alphanumeric characters, and a 94 character set containing mixed-case alphanumeric characters with punctuation.\n\n| Method                        | Train data      | IIIT5k | SVT   | IC13  | IC13  | IC15  | IC15  | SVTP  | CUTE  |\n|-------------------------------|-----------------|--------|-------|-------|-------|-------|-------|-------|-------|\n| Num                           | -               | 3,000  | 647   | 857   | 1,015 | 1,811 | 2,077 | 645   | 288   |\n|                               | 36-char results |        |       |       |       |       |       |       |       |\n| PARSeq [3]                    | R               | 99.1   | 97.9  | 98.3  | 98.4  | 90.7  | 89.6  | 95.7  | 98.3  |\n| S-GTR [17]                    | B               | 97.5   | 95.8  | 97.8  | -     | -     | 87.3  | 90.6  | 94.7  |\n| CDistNet [57]                 | B               | 96.4   | 93.5  | 97.4  | -     | 86.0  | -     | 88.7  | 93.4  |\n| TextScanner [45]              | S               | 95.7   | 92.7  | -     | 94.9  | –     | 83.5  | 84.8  | 91.6  |\n| AutoSTR [53]                  | S               | 94.7   | 90.9  | –     | 94.2  | 81.8  | –     | 81.7  | –     |\n| RCEED [7]                     | B               | 94.9   | 91.8  | –     | –     | –     | 82.2  | 83.6  | 91.7  |\n| PREN2D [52]                   | S               | 95.6   | 94.0  | 96.4  | –     | 83.0  | –     | 87.6  | 91.7  |\n| STN-CSTR [4]                  | S               | 94.2   | 92.3  | 96.3  | 94.1  | 86.1  | 82.0  | 86.2  | -     |\n| ViTSTR-B [1]                  | S               | 88.4   | 87.7  | 93.2  | 92.4  | 78.5  | 72.6  | 81.8  | 81.3  |\n| CRNN [37]                     | S               | 84.3   | 78.9  | -     | 88.8  | –     | 61.5  | 64.8  | 61.3  |\n| TRBA [2]                      | S               | 92.1   | 88.9  | -     | 93.1  | –     | 74.7  | 79.5  | 78.2  |\n| VisionLAN [49]                | S               | 95.8   | 91.7  | 95.7  | –     | 83.7  | –     | 86.0  | 88.5  |\n| Ours                          | R               | 99.4   | 98.2  | 98.6  | 98.5  | 90.9  | 89.8  | 96.1  | 98.2  |\n| 94-char results for IV words  |                 |        |       |       |       |       |       |       |       |\n| OVR [45]                      | R               | 95.19  | 95.45 | 96.23 | 95.78 | 87.12 | 86.78 | 92.77 | 94.13 |\n| TRBA [2]                      | R               | 96.40  | 96.04 | 95.30 | 95.05 | 87.47 | 86.78 | 93.28 | 95.32 |\n| Parseq [3]                    | R               | 97.36  | 97.13 | 96.77 | 97.31 | 90.0  | 89.35 | 95.65 | 96.54 |\n| Ours                          | R               | 97.24  | 96.96 | 97.52 | 97.69 | 90.66 | 89.95 | 95.11 | 95.47 |\n| 94-char results for OV words  |                 |        |       |       |       |       |       |       |       |\n| OVR [45]                      | R               | 89.43  | 79.01 | 88.01 | 88.12 | 55.34 | 57.20 | 80.03 | 86.29 |\n| TRBA [2]                      | R               | 89.21  | 78.73 | 88.07 | 88.00 | 55.15 | 57.45 | 79.78 | 86.02 |\n| Parseq [3]                    | R               | 90.43  | 79.79 | 89.66 | 89.60 | 57.74 | 60.0  | 82.01 | 88.25 |\n| Ours                          | R               | 93.91  | 81.05 | 92.20 | 92.20 | 59.80 | 62.06 | 83.27 | 89.52 |\n| 94-char results for all words |                 |        |       |       |       |       |       |       |       |\n| OVR [45]                      | R               | 92.31  | 87.23 | 92.12 | 91.95 | 71.23 | 71.99 | 86.40 | 90.21 |\n| TRBA [2]                      | R               | 92.81  | 87.39 | 91.69 | 91.53 | 71.31 | 72.12 | 86.53 | 90.67 |\n| Parseq [3]                    | R               | 93.90  | 88.46 | 93.22 | 93.46 | 73.87 | 74.68 | 88.83 | 92.40 |\n| Ours                          | R               | 95.57  | 89.00 | 94.86 | 94.94 | 75.23 | 76.00 | 89.19 | 92.49 |\n\n<span id=\"page-5-3\"></span>Table 1. Scene text recognition results, containing 36- and 94-char results. Specifically, we tested our approach on three types of datasets: Synthetic (S) datasets, including MJ and ST; Benchmark (B) datasets, including SVT, IIIT5k, IC13, and IC15; and Real (R) datasets, including COCO, RCTW17, Uber, ArT, LSVT, MLT19, ReCTS, TextOCR, and OpenVINO. Our proposed method outperformed other state-of-the-art methods in both 36- and 94-char experiments, and archived significant improvements for OOV words.\n\n## 4.2. Implementation Details\n\nOur pseudo label generation module employs YOLOv5s [\\[21\\]](#page-8-22) as the character detector. The settings of the character detector follow the official config. For image inpainting, we use three image datasets for pretraining: ImageNet, Places2, and CelebA-HQ, and utilize the same settings to train the partial convolutions inpainting model. For the semantic checking module, we not only utilize the training and validation sets in text recognition datasets, but adopt the COCA [2](#page-5-0) , word frequency[3](#page-5-1) , and iWeb datasets[4](#page-5-2) as checking lists.\n\nWe use ResNet45 [\\[15\\]](#page-8-23) as the backbone in our text recognition network, with weights initialized by the COCO pretrained model. We set the input image size to 256 × 64 and employ data augmentation techniques including random rotation, image truncation, color jittering, perspective distortion, RandAugment operations (excluding Sharpness), Invert, GaussianBlur, and PoissonNoise. We conducted experiments on 4 NVIDIA V100 GPUs with a batch size of 256, and trained the network end-to-end using the Adam optimizer with a learning rate of 1e−4, following the same settings as Parseq [\\[3\\]](#page-8-7).\n\nFor our proposed loss, the learning rate is initialized at 0.1 and decreased by a factor of 10 at epochs 10, 20, and 25. The training is stopped at the 40-th epoch. The weight decay and momentum are set to 5e−4 and 0.9, respectively. We fix the upper and lower bounds of the text quality as l<sup>a</sup> = 0.5, u<sup>a</sup> = 1, l<sup>m</sup> = 0, and u<sup>m</sup> = 6. Finally, our other hyperparameters are also consistent with MagFace [\\[30\\]](#page-9-7).\n\n<span id=\"page-5-0\"></span><sup>2</sup><https://www.english-corpora.org/coca/>\n\n<span id=\"page-5-1\"></span><sup>3</sup>[http://www.writewords.org.uk/word\\\\_count.asp](http://www.writewords.org.uk/word_count.asp)\n\n<span id=\"page-5-2\"></span><sup>4</sup><https://searchworks.stanford.edu/view/13223153>\n\n|       | <b>Predictions</b> |                   |                   |                     |                    |                  |\n|-------|--------------------|-------------------|-------------------|---------------------|--------------------|------------------|\n| Input | GT                 | Our               | <b>PARSeq</b>     | <b>ABINet</b>       | <b>TRBA</b>        | <b>CRNN</b>      |\n|       | esolanade          | esolanade         | esplanade         | esplaiee            | esplane            | <b>BsAaE</b>     |\n|       | Timms              | Timms             | T mms             | Timms               | Timms              | Timm             |\n|       | CHATHAM-KENT       | CHATHAM-KENT      | CHATHAM-KENT      | <b>CHATHAM KENT</b> | <b>CHATHAMEREY</b> | <b>CERHAMNNE</b> |\n|       | <b>CLASSROOMS</b>  | <b>CLASSROOMS</b> | <b>CLASSROOMS</b> | <b>CANSOSSST</b>    | CLA ROOMS          | <b>ENO</b>       |\n|       | <b>ANTS</b>        | ANT               | <b>ANTSTS</b>     | <b>KANTER</b>       | <b>BANTSEN</b>     | $N_{-}$          |\n|       | pasada             | pasada            | pasada.           | pasado              | pasada.            | pasodo           |\n|       | <b>BUFFEL</b>      | <b>BUFFEL</b>     | <b>BUF EL</b>     | <b>BUFFEF</b>       | <b>BUF EL</b>      | <b>BUF EL</b>    |\n\n<span id=\"page-6-2\"></span>Figure 3. Qualitative results for samples taken from various test datasets related to the OOV problem. Both context-free methods, TRBA [\\[2\\]](#page-8-21) and CRNN [\\[37\\]](#page-9-5), were unable to accurately predict certain cases, possibly due to the ambiguity involved. ABINet [\\[10\\]](#page-8-24) encountered difficulties recognizing vertically-oriented and rotated text. PARSeq [\\[3\\]](#page-8-7) also mis-recognized many characters. Compared with them, our method achieves the best performance.\n\n|                                             | IV: % | OOV: % | AVE: % |  |  |  |\n|---------------------------------------------|-------|--------|--------|--|--|--|\n| Ablation study for semantic checking module |       |        |        |  |  |  |\n| w/                                          | 95.06 | 81.12  | 88.09  |  |  |  |\n| w/o                                         | 94.61 | 79.22  | 86.92  |  |  |  |\n| Ablation study for data augment strategy    |       |        |        |  |  |  |\n| Both                                        | 95.06 | 81.12  | 88.09  |  |  |  |\n| w/o Remove                                  | 95.05 | 80.27  | 87.66  |  |  |  |\n| w/o Swap                                    | 94.98 | 80.54  | 87.76  |  |  |  |\n| Compare with other pseudo label method      |       |        |        |  |  |  |\n| SynthText [13]                              | 95.04 | 79.79  | 87.41  |  |  |  |\n| UnrealText [29]                             | 95.03 | 79.70  | 87.37  |  |  |  |\n| Edit [51]                                   | 95.03 | 79.65  | 87.34  |  |  |  |\n| Ours                                        | 95.06 | 81.12  | 88.09  |  |  |  |\n\n<span id=\"page-6-0\"></span>Table 2. The effects of main components in our pseudo label generation module, and the comparison with other pseudo label generation methods. 'W/o' and 'w/' mean 'without' and 'with', respectively. All results are the average of the eight test datasets under the CRW metric.\n\n#### 4.3. Main Results\n\nIn order to evaluate the effectiveness of our proposed method, we conducted a comparative analysis with popular and recent state-of-the-art methods. To ensure a fair comparison, we not only evaluated our method against published results, but also reproduced state-of-the-art methods. We reported 36- and 94-char results in Table [1.](#page-5-3) In both 36 and 94-char evaluations, our method outperformed previous methods. In particular, in the more difficult 94-char experiment, our method showed more significant improvements.\n\nIV and OV results were also shown in Table [1.](#page-5-3) It could be seen that although previous methods achieved good performance for IV words, they did not recognize OOV words\n\n|                                      | IV: % | OOV: % | AVE: % |  |  |  |\n|--------------------------------------|-------|--------|--------|--|--|--|\n| Ablation study for margin-based loss |       |        |        |  |  |  |\n| w/                                   | 95.01 | 80.26  | 87.64  |  |  |  |\n| w/o                                  | 95.03 | 79.68  | 87.36  |  |  |  |\n| Ablation study for quality indicator |       |        |        |  |  |  |\n| w/o                                  | 95.01 | 80.26  | 87.64  |  |  |  |\n| w/ image                             | 95.03 | 80.33  | 87.68  |  |  |  |\n| w/ box                               | 95.03 | 80.38  | 87.71  |  |  |  |\n| Compare with other margin-based loss |       |        |        |  |  |  |\n| Arcface [9]                          | 95.01 | 80.26  | 87.64  |  |  |  |\n| Megaface [30]                        | 95.03 | 80.33  | 87.68  |  |  |  |\n| Ours                                 | 95.03 | 80.38  | 87.71  |  |  |  |\n\n<span id=\"page-6-1\"></span>Table 3. The effects of each key component in our quality-aware margin loss, and the comparison with other margin-based losses. 'W/o' and 'w/' mean 'without' and 'with', respectively. All results are the average of the eight test datasets under the CRW metric.\n\nwell. In contrast, our proposed Pseudo-OCR yielded remarkable improvements for OOV words. We outperformed our baseline PARSeq [\\[3\\]](#page-8-7) by up to 3.48% for OOV words. OVR [\\[45\\]](#page-9-6) was also designed for open-vocabulary scene text recognition. However, it only leveraged multiple decoders to improve the generalization ability, while our method generated pseudo training samples for OOV words and achieved improvements of up to 4.48%. These superior results demonstrated the effectiveness of our pseudo labels and quality-aware margin loss.\n\n#### 4.4. Ablation Study\n\nThe effects of pseudo label generation. Table [2](#page-6-0) shows the results of our experiments in terms of IV of CRW score, OOV of CRW score, and their average scores. We conducted experiments to evaluate the performance of our proposed pseudo label generation module, comparing the results using several different settings. We employed semantic checking (\"w/\") and Remove/Swap in data augmentation (\"Both\") during implementation, which resulted in our method achieving scores of 95.06%, 81.12%, and 88.09% for IV, OOV, and their average, respectively. All of which were higher than those achieved with other options.\n\nCompare with other pseudo label methods. Table [2](#page-6-0) provides a comparative analysis of our proposed method with other state-of-the-art approaches for the same task. Our approach surpasses models that are pre-trained with SynthtText [\\[20\\]](#page-8-3), UnrealText [\\[29\\]](#page-9-23), or Edit [\\[51\\]](#page-10-7), and subsequently trained on real data, thanks to the integration of our novel pseudo-label module, which utilizes semantic checking and Remove/Swap techniques. Our method's performance metrics are impressive. In particular, our approach leverages semantic knowledge to validate the training data, and correct errors to improve accuracy.\n\nThe effects of quality-aware margin loss. Table [3](#page-6-1) presents the outcomes of our experiments, showcasing the IV and OOV of the CRW score, and their average scores. Our primary aim was to evaluate the effectiveness of our proposed quality-aware margin-based module through various settings. During implementation, we utilized marginbased (\"w/\") and quality indicator with detector box (\"w/ box\"), which yielded remarkable results. Our method achieved a score of 95.03%, 80.38%, and 87.71% for IV, OOV, and their average, respectively. These scores outperformed other alternatives. Overall, our study highlights the potential of the quality-aware margin-based module for enhancing performance in the CRW score evaluation.\n\nCompare with other margin-based loss. Table [3](#page-6-1) presents a comprehensive comparison of our loss with other margin-based losses. We have outperformed previous models that have utilized cos-like loss, including Arcface [\\[9\\]](#page-8-9) and Megaface [\\[30\\]](#page-9-7), by incorporating our novel margin-based and quality-aware components. Our method has achieved remarkable performance. Our approach has the added advantage of using the detector confidence score as a quality indicator, which may further enhance the loss convergence. Overall, our study provides valuable insights into the potential of margin-based and quality-aware techniques for enhancing performance in loss functions.\n\nQualitative analysis. Fig. [3](#page-6-2) shows the qualitative results on all test datasets, with the input images displayed in their original orientations and aspect ratios. Incorrect characters are highlighted in red, while missing characters are indicated by a red underscore for predictions that are roughly aligned with the ground truth. Both context-free methods, TRBA [\\[2\\]](#page-8-21) and CRNN [\\[37\\]](#page-9-5), did not accurately predict certain cases, possibly due to the ambiguity involved. ABI-Net [\\[10\\]](#page-8-24) encountered difficulties in recognizing verticallyoriented images. Considering the presence of obstructions\n\n|            | AVG: % | IV: % | OOV: % | Ensemble |\n|------------|--------|-------|--------|----------|\n| Method     | CRW    | CRW   | CRW    |          |\n| Ours       | 70.98  | 82.81 | 59.15  |          |\n| OCRFLYV2   | 70.31  | 81.02 | 59.61  | ✓        |\n| HuiGuanV2  | 70.28  | 81.73 | 58.83  | ✓        |\n| oov3decode | 70.22  | 81.58 | 58.86  | ✓        |\n| ViT-based  | 70.00  | 81.36 | 58.64  | ✓        |\n\n<span id=\"page-7-0\"></span>Table 4. Comparison of the proposed method with other stateof-the-art methods on the on-site test set of the ICDAR Out-of-Vocabulary Challenge. Our method achieves the first rank.\n\non the image, such as occlusion, we believe that the recognition results of all models in the last row of the image are suboptimal and that this cannot be avoided. The results demonstrate that our model can accurately identify OOV images and maintain good performance on IV images.\n\n#### 4.5. Comparison in ICADR 2022 Challenge\n\nOver 200 teams participated in the ICDAR Out of Vocabulary challenge, utilizing official training and validation data from various datasets including ICDAR13, ICDAR15, MLT19, COCO-Text, TextOCR, HierText, and OpenImagesText. This challenge focuses on the cropped word recognition task, where participants are required to predict the recognition of all cropped words in the test set. The test set contains 313,751 cropped words, out of which 271,664 are IV words and 42,087 are OOV ones. According to Table [4,](#page-7-0) our method outperformed other methods in the challenge. Notably, our method achieved this without utilizing the ensemble strategy.\n\n# 5. Conclusion\n\nIn this paper, we have presented a Pseudo-OCR framework for open-vocabulary scene text recognition. Our approach entails two major contributions. Firstly, we propose a novel pseudo label generation module that integrates character detection and image inpainting techniques to create a large volume of training data. Different from previous synthetic labels, our pseudo labels are closer to real-world images. Meanwhile, we design a text semantic checking in our pseudo label generation module to filter semantically meaningful pseudo data. Moreover, we introduce a margin loss that optimizes geodesic distance margins to mitigate the effect of low-quality samples. We also present a text quality indicator to dynamically adjust the margin of each class based on image quality. Extensive experimental results demonstrate that our method outperforms the state-ofthe-art on various test datasets, enhancing both IV and OOV performance. Moreover, our approach secured the first position in the ICDAR2022 challenge.\n\n# References\n\n- <span id=\"page-8-20\"></span>[1] Rowel Atienza. Vision transformer for fast and efficient scene text recognition. In *Document Analysis and Recognition–ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part I 16*, pages 319–334. Springer, 2021.\n- <span id=\"page-8-21\"></span>[2] Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwalsuk Lee. What is wrong with scene text recognition model comparisons? dataset and model analysis. In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 4715–4723, 2019.\n- <span id=\"page-8-7\"></span>[3] Darwin Bautista and Rowel Atienza. Scene text recognition with permuted autoregressive sequence models. In *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVIII*, pages 178–196. Springer, 2022.\n- <span id=\"page-8-19\"></span>[4] Hongxiang Cai, Jun Sun, and Yichao Xiong. Revisiting classification perspective on scene text recognition. *arXiv preprint arXiv:2102.10884*, 2021.\n- <span id=\"page-8-13\"></span>[5] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In *2019 International Conference on Document Analysis and Recognition (ICDAR)*, pages 1571–1576. IEEE, 2019.\n- <span id=\"page-8-4\"></span>[6] Fuze Cong, Wenping Hu, Qiang Huo, and Li Guo. A comparative study of attention-based encoder-decoder approaches to natural scene text recognition. In *2019 International Conference on Document Analysis and Recognition (ICDAR)*, pages 916–921. IEEE, 2019.\n- <span id=\"page-8-18\"></span>[7] Mengmeng Cui, Wei Wang, Jinjin Zhang, and Liang Wang. Representation and correlation enhanced encoder-decoder framework for scene text recognition. In *Document Analysis and Recognition–ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part IV 16*, pages 156–170. Springer, 2021.\n- <span id=\"page-8-8\"></span>[8] Yiming Cui, Ziqing Yang, and Ting Liu. Pert: pretraining bert with permuted language model. *arXiv preprint arXiv:2203.06906*, 2022.\n- <span id=\"page-8-9\"></span>[9] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 4690–4699, 2019.\n- <span id=\"page-8-24\"></span>[10] Shancheng Fang, Zhendong Mao, Hongtao Xie, Yuxin Wang, Chenggang Yan, and Yongdong Zhang. Abinet++: Autonomous, bidirectional and iterative language modeling for scene text spotting. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2022.\n- <span id=\"page-8-1\"></span>[11] Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, and Xilin Chen. Multi-modal graph neural network for joint reasoning on vision and scene text. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 12746–12756, 2020.\n- <span id=\"page-8-6\"></span>[12] Alex Graves, Santiago Fernandez, Faustino Gomez, and ´ Jurgen Schmidhuber. Connectionist temporal classification: ¨\n\nlabelling unsegmented sequence data with recurrent neural networks. In *Proceedings of the 23rd international conference on Machine learning*, pages 369–376, 2006.\n\n- <span id=\"page-8-12\"></span>[13] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Synthetic data for text localisation in natural images. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 2315–2324, 2016.\n- <span id=\"page-8-11\"></span>[14] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir- ´ shick. Mask r-cnn. In *Proceedings of the IEEE international conference on computer vision*, pages 2961–2969, 2017.\n- <span id=\"page-8-23\"></span>[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 770–778, 2016.\n- <span id=\"page-8-2\"></span>[16] Wenhao He, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu. Multi-oriented and multi-lingual scene text detection with direct regression. *IEEE Transactions on Image Processing*, 27(11):5406–5419, 2018.\n- <span id=\"page-8-17\"></span>[17] Yue He, Chen Chen, Jing Zhang, Juhua Liu, Fengxiang He, Chaoyue Wang, and Bo Du. Visual semantics allow for textual reasoning better in scene text recognition. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36, pages 888–896, 2022.\n- <span id=\"page-8-0\"></span>[18] Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Semantic object accuracy for generative text-to-image synthesis. *IEEE transactions on pattern analysis and machine intelligence*, 44(3):1552–1565, 2020.\n- <span id=\"page-8-5\"></span>[19] Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for sequence tagging. *arXiv preprint arXiv:1508.01991*, 2015.\n- <span id=\"page-8-3\"></span>[20] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Synthetic data and artificial neural networks for natural scene text recognition. *arXiv preprint arXiv:1406.2227*, 2014.\n- <span id=\"page-8-22\"></span>[21] Glenn Jocher. ultralytics/yolov5: v3.1 - Bug Fixes and Performance Improvements. [https://github.com/](https://github.com/ultralytics/yolov5) [ultralytics/yolov5](https://github.com/ultralytics/yolov5), Oct. 2020.\n- <span id=\"page-8-16\"></span>[22] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. In *2015 13th international conference on document analysis and recognition (ICDAR)*, pages 1156–1160. IEEE, 2015.\n- <span id=\"page-8-15\"></span>[23] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. Icdar 2013 robust reading competition. In *2013 12th international conference on document analysis and recognition*, pages 1484–1493. IEEE, 2013.\n- <span id=\"page-8-10\"></span>[24] Minchul Kim, Anil K Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 18750–18759, 2022.\n- <span id=\"page-8-14\"></span>[25] Ilya Krylov, Sergei Nosov, and Vladislav Sovrasov. Open images v5 text annotation and yet another mask text spotter.\n\nIn *Asian Conference on Machine Learning*, pages 379–389. PMLR, 2021.\n\n- <span id=\"page-9-0\"></span>[26] Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. Visual semantic reasoning for image-text matching. In *Proceedings of the IEEE/CVF International conference on computer vision*, pages 4654–4662, 2019.\n- <span id=\"page-9-9\"></span>[27] Minghui Liao, Jian Zhang, Zhaoyi Wan, Fengming Xie, Jiajun Liang, Pengyuan Lyu, Cong Yao, and Xiang Bai. Scene text recognition from two-dimensional perspective. In *Proceedings of the AAAI conference on artificial intelligence*, volume 33, pages 8714–8721, 2019.\n- <span id=\"page-9-13\"></span>[28] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In *Proceedings of the European conference on computer vision (ECCV)*, pages 85–100, 2018.\n- <span id=\"page-9-23\"></span>[29] Shangbang Long and Cong Yao. Unrealtext: Synthesizing realistic scene text images from the unreal world. *arXiv preprint arXiv:2003.10608*, 2020.\n- <span id=\"page-9-7\"></span>[30] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou. Magface: A universal representation for face recognition and quality assessment. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 14225–14234, 2021.\n- <span id=\"page-9-19\"></span>[31] Anand Mishra, Karteek Alahari, and CV Jawahar. Scene text recognition using higher order language priors. In *BMVC-British machine vision conference*. BMVA, 2012.\n- <span id=\"page-9-17\"></span>[32] Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowdhury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Umapada Pal, Jean-Christophe Burie, Cheng-lin Liu, et al. Icdar2019 robust reading challenge on multi-lingual scene text detection and recognition—rrc-mlt-2019. In *2019 International conference on document analysis and recognition (IC-DAR)*, pages 1582–1587. IEEE, 2019.\n- <span id=\"page-9-22\"></span>[33] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, and Chew Lim Tan. Recognizing text with perspective distortion in natural scenes. In *Proceedings of the IEEE International Conference on Computer Vision*, pages 569–576, 2013.\n- <span id=\"page-9-10\"></span>[34] Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, and Weiping Wang. Seed: Semantics enhanced encoder-decoder framework for scene text recognition. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 13528–13537, 2020.\n- <span id=\"page-9-1\"></span>[35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In *International Conference on Machine Learning*, pages 8821–8831. PMLR, 2021.\n- <span id=\"page-9-20\"></span>[36] Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng Chan, and Chew Lim Tan. A robust arbitrary text detection system for natural scene images. *Expert Systems with Applications*, 41(18):8027–8048, 2014.\n- <span id=\"page-9-5\"></span>[37] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. *IEEE transactions on pattern analysis and machine intelligence*, 39(11):2298–2304, 2016.\n- <span id=\"page-9-15\"></span>[38] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang Bai. Icdar2017 competition on reading chinese text in the wild (rctw-17). In *2017 14th iapr international conference on document analysis and recognition (ICDAR)*, volume 1, pages 1429–1434. IEEE, 2017.\n- <span id=\"page-9-18\"></span>[39] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 8802–8812, 2021.\n- <span id=\"page-9-16\"></span>[40] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al. Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt. In *2019 International Conference on Document Analysis and Recognition (ICDAR)*, pages 1557–1562. IEEE, 2019.\n- <span id=\"page-9-4\"></span>[41] Youbao Tang and Xiangqian Wu. Scene text detection and segmentation based on cascaded convolution neural networks. *IEEE transactions on Image Processing*, 26(3):1509– 1520, 2017.\n- <span id=\"page-9-14\"></span>[42] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. *arXiv preprint arXiv:1601.07140*, 2016.\n- <span id=\"page-9-3\"></span>[43] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing text and image for image retrieval-an empirical odyssey. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 6439–6448, 2019.\n- <span id=\"page-9-8\"></span>[44] Zhaoyi Wan, Minghang He, Haoran Chen, Xiang Bai, and Cong Yao. Textscanner: Reading characters in order for robust scene text recognition. In *Proceedings of the AAAI conference on artificial intelligence*, volume 34, pages 12120– 12127, 2020.\n- <span id=\"page-9-6\"></span>[45] Zhaoyi Wan, Jielei Zhang, Liang Zhang, Jiebo Luo, and Cong Yao. On vocabulary reliance in scene text recognition. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 11425–11434, 2020.\n- <span id=\"page-9-12\"></span>[46] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 5265–5274, 2018.\n- <span id=\"page-9-21\"></span>[47] Kai Wang, Boris Babenko, and Serge Belongie. End-to-end scene text recognition. In *2011 International conference on computer vision*, pages 1457–1464. IEEE, 2011.\n- <span id=\"page-9-11\"></span>[48] Xiaogang Wang, Meng Wang, and Wei Li. Scene-specific pedestrian detection for static video surveillance. *IEEE transactions on pattern analysis and machine intelligence*, 36(2):361–374, 2013.\n- <span id=\"page-9-24\"></span>[49] Yuxin Wang, Hongtao Xie, Shancheng Fang, Jing Wang, Shenggao Zhu, and Yongdong Zhang. From two to one: A new scene text recognizer with visual language modeling network. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 14194–14203, 2021.\n- <span id=\"page-9-2\"></span>[50] Zihao Wang, Xihui Liu, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang Wang, and Jing Shao. Camp: Cross-modal\n\nadaptive message passing for text-image retrieval. In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 5764–5773, 2019.\n\n- <span id=\"page-10-7\"></span>[51] Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jingtuo Liu, Errui Ding, and Xiang Bai. Editing text in the wild. In *Proceedings of the 27th ACM international conference on multimedia*, pages 1500–1508, 2019.\n- <span id=\"page-10-6\"></span>[52] Ruijie Yan, Liangrui Peng, Shanyu Xiao, and Gang Yao. Primitive representation learning for scene text recognition. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 284–293, 2021.\n- <span id=\"page-10-5\"></span>[53] Hui Zhang, Quanming Yao, Mingkun Yang, Yongchao Xu, and Xiang Bai. Autostr: efficient backbone search for scene text recognition. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIV 16*, pages 751–767. Springer, 2020.\n- <span id=\"page-10-3\"></span>[54] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun Yang, et al. Icdar 2019 robust reading challenge on reading chinese text on signboard. In *2019 international conference on document analysis and recognition (ICDAR)*, pages 1577–1581. IEEE, 2019.\n- <span id=\"page-10-2\"></span>[55] Ying Zhang, Lionel Gueguen, Ilya Zharkov, Peter Zhang, Keith Seifert, and Ben Kadlec. Uber-text: A large-scale dataset for optical character recognition from street-level imagery. In *SUNw: Scene Understanding Workshop-CVPR*, volume 2017, page 5, 2017.\n- <span id=\"page-10-0\"></span>[56] Ziyuan Zhao, Huiying Zhu, Zehao Xue, Zhao Liu, Jing Tian, Matthew Chin Heng Chua, and Maofu Liu. An image-text consistency driven multimodal sentiment analysis approach for social media. *Information Processing & Management*, 56(6):102097, 2019.\n- <span id=\"page-10-4\"></span>[57] Tianlun Zheng, Zhineng Chen, Shancheng Fang, Hongtao Xie, and Yu-Gang Jiang. Cdistnet: Perceiving multi-domain character distance for robust text recognition. *arXiv preprint arXiv:2111.11011*, 2021.\n- <span id=\"page-10-1\"></span>[58] Yingying Zhu, Cong Yao, and Xiang Bai. Scene text detection and recognition: Recent advances and future trends. *Frontiers of Computer Science*, 10:19–36, 2016.\\n，请你分析其研究动机、核心方法与公式推导细节。请结合摘要与正文信息，提取论文背景、问题定义、方法核心流程与理论基础。\n",
        "agent": "论文解读专家\n",
        "status": "started"
    },
    {
        "timestamp": "2025-06-13 16:12:47",
        "task_name": "research_task",
        "task": "阅读论文《Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss》，论文于2024-03-12发布于arXiv.org内容如下：\\n# Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss\n\nXuhua Ren MiHOYO Shanghai, China renxuhua1993@gmail.com\n\nHengcan Shi Monash University Melbourne, Australia shihengcan@gmail.com\n\nJin Li Shaanxi Normal University Xian, China\n\n# Abstract\n\n*Scene text recognition is an important and challenging task in computer vision. However, most prior works focus on recognizing pre-defined words, while there are various out-of-vocabulary (OOV) words in real-world applications. In this paper, we propose a novel open-vocabulary text recognition framework, Pseudo-OCR, to recognize OOV words. The key challenge in this task is the lack of OOV training data. To solve this problem, we first propose a pseudo label generation module that leverages character detection and image inpainting to produce substantial pseudo OOV training data from real-world images. Unlike previous synthetic data, our pseudo OOV data contains real characters and backgrounds to simulate real-world applications. Secondly, to reduce noises in pseudo data, we present a semantic checking mechanism to filter semantically meaningful data. Thirdly, we introduce a qualityaware margin loss to boost the training with pseudo data. Our loss includes a margin-based part to enhance the classification ability, and a quality-aware part to penalize lowquality samples in both real and pseudo data. Extensive experiments demonstrate that our approach outperforms the state-of-the-art on eight datasets and achieves the first rank in the ICDAR2022 challenge.*\n\n# 1. Introduction\n\nScene text recognition expects to recognize all characters in an image. It serves as a crucial component in various computer vision applications, such as image captioning [\\[26\\]](#page-9-0), image generation [\\[18,](#page-8-0) [35\\]](#page-9-1), multi-modal systems [\\[11,](#page-8-1) [56\\]](#page-10-0) and image retrieval [\\[50,](#page-9-2) [43\\]](#page-9-3).\n\nThe existing scene text recognition methods [\\[16,](#page-8-2) [41,](#page-9-4) [37\\]](#page-9-5) can be mainly divided into two types: segmentationbased and context-aware. Segmentation-based approaches [\\[16,](#page-8-2) [41\\]](#page-9-4) first segment or detect every character in the image, and then separately recognize them. Nevertheless, they ignore the significant semantic correlations among characters, which are crucial for accurate character recognition. Context-aware works [\\[37\\]](#page-9-5) leverage word-level prior knowledge to learn the relationships among characters to improve text recognition, which usually generates more accurate results than segmentation-based methods. However, they heavily rely on pre-defined words. Real-world applications usually involve diverse words and even new words such as 'WASSUP', which are hard to be fully covered by the predefined dictionary.\n\nSeveral works [\\[45\\]](#page-9-6) notice this challenge and present the open-vocabulary scene text recognition task, which expects to recognize not only in-vocabulary (IV) words but also out-of-vocabulary (OOV) ones. For this task, although prior works [\\[45\\]](#page-9-6) employ multiple decoders to improve their generalization ability, the OOV word recognition accuracy is only slightly improved. A key reason is the lack of OOV training samples. To obtain OOV training samples, a straightforward way is data synthesis. Some methods [\\[20,](#page-8-3) [43\\]](#page-9-3) synthesize pseudo text images to improve text recognition performance. However, as shown in Fig. [1,](#page-1-0) these synthetic images only contain computer-generated characters and lack lighting, distortion as well as texture, which are important for scene text recognition in real-world applications.\n\nIn this paper, we propose a novel Pseudo-OCR framework for open-vocabulary text recognition, which includes a pseudo label generation module to capture real-imagebased pseudo data and a quality-aware margin loss to enhance the model training. Our pseudo label generation module first leverages a character detector to decompose real images into character and background elements, and then generates various OOV pseudo labels from these elements. To obtain more real-like pseudo labels, we employ image inpainting technologies to optimize the appearance of our pseudo labels. In this way, our pseudo labels contain real characters, lighting, distortion and texture, and thus are more suitable for real-world applications, as shown in Fig. [1.](#page-1-0) However, there are inevitable noises in pseudo data,\n\n![](_page_1_Figure_0.jpeg)\n\n<span id=\"page-1-0\"></span>Figure 1. Our proposed method builds a pseudo label generation system using real images. For example, the model produces two pseudo labels, 'PARS' and 'PASIR' from the real 'PARIS' image. The \"PARS\" is an IV word while \"PASIR\" is an OOV word. Compared with traditional synthetic images, our pseudo labels are closer to real-world images.\n\nsuch as semantically unreasonable data. Therefore, we propose a semantic checking mechanism to filter and correct our pseudo labels. Specifically, we compare pseudo labels with a large-scale dictionary. If a pseudo label is close to a word in the dictionary, we correct the pseudo label to that word. If a pseudo label largely differs from every word in the dictionary, we filter out it. After our semantic checking, semantically meaningful pseudo labels can be captured.\n\nMeanwhile, a quality-aware margin loss is also presented to better leverage real and our pseudo labels for training. In both pseudo and real training data, low-quality labels (such as unclear characters and missing annotations) affect the model performance. To address challenges, we took inspiration from face recognition, where low-quality faces can significantly reduce performance. Inspired by MagFace [\\[30\\]](#page-9-7), we adopted margin loss that improves the classification ability by enlarging class margins. More importantly, we design a quality indicator specific to text recognition images. This indicator is based on the confidence of the character detection model to identify low-quality areas. We then penalize these low-quality samples in the margin loss.\n\nIn summary, our proposed work enhances previous research in three key areas:\n\n- 1. We present a novel approach, Pseudo-OCR, to address the open-vocabulary text recognition problem. Pseudo-OCR is able to generate real-image-based pseudo labels for better OOV word recognition. A semantic checking mechanism is also proposed to reduce noises in pseudo labels.\n- 2. We further design a quality-aware margin loss to improve the training with pseudo labels, which contains a margin loss to enhance classification, and a quality indicator to detect and penalize low-quality labels.\n- 3. Experiments show that our approach significantly outperforms previous state-of-the-art methods on eight datasets (i.e., IIIT5k, SVT, IC13, IC13, IC15, IC15, SVTP and CUTE). Moreover, our method achieves the first rank in the ICDAR2022 challenge.\n\n# 2. Related work\n\n#### 2.1. Scene Text Recognition\n\nIn the past year, the attention-based decoder [\\[6\\]](#page-8-4) has been considered the state-of-the-art pipeline for text recognition, incorporating language modeling, weak character detection supervision, and character recognition in a unified system. Comprised of four essential parts [\\[58\\]](#page-10-1): i) an autoregressively predicting attention-based decoder, ii) a context-modeling Bi-LSTM layer [\\[19\\]](#page-8-5), iii) a featureextracting convolutional encoder, and iv) a rectification network to straighten irregular text images, this system distinguishes itself from the Connectionist Temporal Classification (CTC) layer [\\[12\\]](#page-8-6) by considering dependencies in the output character space. Several research studies have utilized semantic segmentation for scene text recognition. TextScanner [\\[44\\]](#page-9-8), for example, generates pixel-wise, multichannel segmentation maps that determine the character class, position, and order. To enhance the recognition process, it also employs an RNN for context modeling. Another study conducted by Liao et al. [\\[27\\]](#page-9-9) proposed the CA-FCN model, which uses a semantic segmentation network and an attention mechanism for characters. The CA-FCN model also includes a word formation module, enabling it to simultaneously recognize the script and predict the position of each character.\n\nRecently, most context-aware scene text recognition methods use semantics learned from data to enhance recognition [\\[34\\]](#page-9-10). The majority of these approaches utilize Transformers to learn internal language models (LMs) through standard autoregressive (AR) training [\\[3\\]](#page-8-7). Unlike previous methods, PARSeq [\\[3\\]](#page-8-7) utilizes Permuted Language Modeling (PLM) [\\[8\\]](#page-8-8) instead of standard AR modeling to learn an internal LM. This distinguishes PARSeq from traditional ensemble methods that rely on external LMs for prediction refinement.\n\nHowever, current methods in scene text recognition do not address the OOV problem, which refers to the difficulties encountered when recognizing words outside of the vocabulary encountered during training. This challenge was first highlighted by Wan et al. [\\[45\\]](#page-9-6), who found that state-ofthe-art methods excel at recognizing in-vocabulary words that have been previously seen, but struggle with OOV words. To tackle this issue, the authors introduced a mutual learning strategy that optimizes two types of decoders simultaneously, resulting in improved performance. However, this approach seems to rely solely on ensemble methods to enhance model performance, without considering the issue from the standpoint of the training data.\n\n#### 2.2. Pseudo Label in Scene Text Recognition\n\nThe self-training approach leverages existing detectors to generate pseudo-labels for unlabeled data. Highconfidence pseudo-labels are then selected and used for retraining. Wang et al. [\\[48\\]](#page-9-11) employed tracking in videos to acquire difficult examples, which were then used to retrain their detector. This additional data enhanced the detection accuracy of still images. Image synthesis is another method used for generating pseudo-labels. One widely used algorithm for image synthesis is Synthtext[\\[20\\]](#page-8-3). This algorithm utilizes deep learning and segmentation techniques to generate synthetic images of text that blend naturally with existing natural scenes, aligning the text with the geometry of the background image and respecting scene boundaries. Despite its usefulness, Synthtext has some limitations. For instance, it may suffer from semantic incoherence and the presence of non-homogeneous regions. To address these limitations, Zhan et al. proposed the Verisimilar [\\[43\\]](#page-9-3) method, which leverages the semantic annotations of objects and image regions created through prior semantic segmentation. This method also uses visual saliency to determine the embedding locations within each semantic sensible region, resulting in semantic coherent synthesis. Despite these advancements, the generated images using both Synthtext and Verisimilar methods still have a noticeable gap with real images.\n\n### 2.3. Margin Based Loss\n\nThe margin-based softmax loss function is commonly used to train face recognition (FR) models. The addition of margin to the softmax loss improves the discriminative power of the learned features. There are various forms of margin functions introduced in models such as CosFace [\\[46\\]](#page-9-12), and ArcFace [\\[9\\]](#page-8-9). ArcFace is often referred to as an angular margin, while CosFace is known as an additive margin. MagFace [\\[30\\]](#page-9-7) is a unique approach that assigns different margins based on recognizability, using larger angular margins for high-norm features as they are easier to recognize. Some models [\\[24\\]](#page-8-10) consider the margin as a function of image quality, since it can greatly affect which samples provide the strongest gradient or learning signal during training. However, none of these models take into account the unique challenges posed by scene text images, which require a more tailored measure of text image quality, rather than relying solely on image gradient.\n\n# 3. Methodology\n\n## 3.1. Overview\n\nWe propose a novel Pseudo-OCR framework that incorporates pseudo-image labeling and margin loss to address the open-vocabulary scene text recognition problem. Our framework consists of three parts, as illustrated in Fig. [2.](#page-3-0) (a) A pseudo label generation model including character detector, image inpainting and semantic checking is first proposed to generate pseudo OOV training samples. (b) A text recognition network recognizes all characters from the input image. (c) A quality-aware margin loss is used to train the network, and leverages a quality indicator to penalize low-quality samples. Our proposed pipeline can be trained end-to-end, without the need for any additional manual labeling for the aforementioned modules. During inference, only the recognition network is required, and the pseudo label generation module as well as the loss can be omitted. Next, we introduce the details of each part.\n\n#### 3.2. Pseudo label generation\n\nThe first step in pseudo-labeling for scene text recognition is to locate each character. Then, we modify both the image and text to generate new paired data with valid semantics. This system involves four key components: character detector, image inpainting, image augmentation, and semantic checking. The character detector is used to identify and locate each character in the image. Image inpainting is used to fill in the chosen regions of the image. Image augmentation is used to generate a variety of new images from the original image. Finally, semantic checking is used to verify if the generated image-text pair is valid.\n\nCharacter Detector: We have used a general object detector, YOLO model, for character detection. In the detection branch of this model, character-level features can be easily obtained by detecting characters within the proposals. To extract both global and character-level features I, we applied RoIAlign [\\[14\\]](#page-8-11) to the text proposals generated by FPN. The RoIAlign layer then extracted the features F(I) from the proposals and performed character detection. The extracted features are 7 × 7 in size and are fused by element-wise summation. They are then passed through a 3 × 3 convolution layer and a 1 × 1 layer to create the final fused features for classification Lcls and bounding box regression Lbox. The training data for this module is generated synthetically through Synthtext [\\[13\\]](#page-8-12), without the use of manual-label data.\n\nImage Inpainting: We use a UNet-like architecture based on the one presented by Liu et al. [\\[28\\]](#page-9-13). which incorporates skip connections between mirrored layers, to great effect in object segmentation and image-to-image transla-\n\n![](_page_3_Figure_0.jpeg)\n\n<span id=\"page-3-0\"></span>Figure 2. The proposed Pseudo-OCR contains three parts: (a) A pseudo label generation module based on character detector, image inpainting and semantic checking to obtain pseudo labels; (b) a text recognition network with a ViT encoder and a permutation decoder to predict the word in the image; and (c) a quality-aware margin loss including a quality indicator to train the model. In the inference stage, only the text recognition network is used for predicting.\n\ntion tasks. We apply this mechanism to the up-sampling process, where previous encoding feature maps of the same size are concatenated to retain richer texture and restore the lost background information from the downsampling process. Our training data is generated from Synthtext, where random font, color, and deformation parameters are selected to create styled text which is then rendered onto background images. This generates the corresponding background, foreground text, and overall text all at once.\n\nImage Augmentation: As depicted in Fig. [2,](#page-3-0) we employ two strategies to fabricate novel text image datasets. The initial strategy involves an image inpainting module. We randomly select a character and, cognizant of its specific coordinates, erase it from the image. Subsequently, we feed the entire image into the image inpainting module, resulting in the production of a new image without the selected character. Simultaneously, we also remove the corresponding character from the ground truth, thereby obtaining the updated ground truth for the character. During this process, we eliminate characters with low confidence scores and images with confidence scores that are insufficiently low will not be subjected to the inpainting module. Our second approach involves randomly selecting two characters, harvesting their local images, and swapping their positions. Since the characters come from the same image, it generally does not cause any variations in the image style. By these two strategies, we can generate a multitude of novel datasets for the training of our text recognition models.\n\nSemantic Checking: If newly generated words are meaningless, they can have a detrimental effect on the training process of a model. To address this, we have compiled a comprehensive vocabulary list that includes common words from scene text image recognition datasets, as well as external natural language vocabularies [1](#page-3-1) . Words that are not included in the list after undergoing random character deletion or swapping are discarded. Additionally, we have implemented classical data augmentation techniques, such as removing the first and last characters, to simulate the truncation effect that often occurs in images.\n\n#### 3.3. Text recognition network\n\nWe utilize PARSeq [\\[3\\]](#page-8-7) as our text recognition network, which employs an ensemble of internal AR models with shared weights through PLM. PARSeq follows an encoderdecoder architecture, which is commonly used in sequence modeling tasks. The encoder has 12 layers, while the decoder consists of a single layer.\n\nThe ViT is a direct extension of the Transformer architecture to images. A ViT layer contains a single Multi-Head Attention (MHA) module used for self-attention. The encoder is a 12-layer ViT, excluding the classification head. Given an image x ∈ RW×H×<sup>C</sup> , where W represents the width, H represents the height, and C represents the number of channels, the image is tokenized by dividing it into t<sup>w</sup> × t<sup>h</sup> patches of equal size, flattening each patch, and linearly projecting them into dmodel-dimensional tokens using a patch embedding matrix Wp. This results in (W × H)/(t<sup>w</sup> × th) tokens. Prior to being processed by the first ViT layer, learned position embeddings of equal dimension are added to the tokens.\n\nThe decoder architecture of our model is based on the preLayerNorm Transformer decoder, but it features twice the number of attention heads. The decoder requires three inputs: position tokens, context tokens, and image tokens, along with an optional attention mask. The position tokens encode the target position to be predicted, and each token corresponds directly to a specific position in the output sequence. This parameterization is similar to the query stream of two-stream attention. The attention mask varies depending on how the model is used. During training, we generate masks using random permutations. At inference, the mask\n\n<span id=\"page-3-1\"></span><sup>1</sup><https://www.wordfrequency.info/>\n\ncan be a standard left-to-right lookahead mask for AR decoding, a cloze mask for iterative refinement, or no mask at all for non-autoregressive (NAR) decoding.\n\n#### 3.4. Quality-aware margin loss\n\nIn this section, a novel loss function is proposed to learn a more discriminative word recognition model, where the score of character detection is introduced as the penalty term in the MagFace loss. In details, the set of image (patch) features in the is indicated as X ∈ R<sup>d</sup>×<sup>N</sup> , containing N Ddimensional features, x1, x2, ..., x<sup>i</sup> , ...x<sup>N</sup> . The characterlevel classifier identifies each feature into M = 94 classes including letters and common symbols.\n\nThe MagFace loss function is designed to maximize the separation between classes in the feature space, making it an effective solution for open-vocabulary tasks. It ignores irrelevant factors such as background and text in images by focusing on maximizing the cosine similarity between the embeddings of the text images. This results in the model producing distinct and discriminative embeddings for each class, reducing the risk of misclassification and improving the overall performance of the task.\n\nWe have designed a loss function that overcomes the limitation of MagFace, which does not consider the quality of scene-text images. This can lead to decreased performance if the data includes a high number of low-quality images. Our approach incorporates a quality indicator module that uses a character detector to provide a more reliable indicator compared to MagFace's image-based magnitude as the quality evaluation indicator. Experimental results show that our method outperforms MagFace and is suitable for scene text images,\n\n<span id=\"page-4-0\"></span>\n$$\nL_m = -\\frac{1}{N}\n$$\n  \n\n$$\n\\sum_{i=1}^{N} \\left( \\log \\frac{e^{s(\\cos(\\theta_{y_i} + m(a_i)))}}{e^{s(\\cos(\\theta_{y_i} + m(a_i)))} + \\sum_{j \\neq y_i} e^{s \\cos(\\theta_j)}} \\right).\n$$\n (1)\n\nIn Eq. [\\(1\\)](#page-4-0), the term m(ai) is designed as a monotonically increasing function of the character box confidence ai . Our observations suggest that the confidence of character detection is strongly correlated with the quality of the text image. When the text in the image is clear, the detector can accurately detect the position of the characters. Conversely, when the image quality is poor, the detector often produces low confidence results or fails to detect text boxes altogether. Building on this observation, we have defined our function to account for the varying levels of confidence in character detection that correspond to the quality of the text image,\n\n$$\nm(a_i) = \\frac{u_m - l_m}{u_a - l_a} \\times (a_i - l_a) + l_m,\n$$\n (2)\n\nwhere u<sup>m</sup> and l<sup>m</sup> are the upper and lower bounds of the margin m, and u<sup>a</sup> and l<sup>a</sup> are the upper and lower bounds of the input confidence a<sup>i</sup> . The variable a<sup>i</sup> represents the average confidence of N bounding boxes in i -th image and is defined as follows,\n\n$$\na_i = \\frac{\\sum_{j=1}^{N} c_j}{N},\\tag{3}\n$$\n\nwhere c<sup>j</sup> is the confidence of the j-th bounding box and N is the total number of bounding boxes, our loss introduces an adaptive mechanism to learn a well-structured withinclass feature distribution. This is achieved by pulling easy samples towards class centers while pushing hard samples away, which prevents models from overfitting to noisy, lowquality samples, thereby improving scene text recognition performance.\n\n# 4. Experiment\n\n#### 4.1. Experimental Settings\n\nTraining datasets. Our experimental setup is consistent with previous work in the field. For synthetic training datasets, we utilize MJSynth (MJ) [\\[20\\]](#page-8-3) and SynthText (ST) to train proposed modules or compare with other methods. Additionally, we use real data for training, including COCO-Text (COCO) [\\[42\\]](#page-9-14), RCTW17 [\\[38\\]](#page-9-15), Uber-Text (Uber) [\\[55\\]](#page-10-2), ArT [\\[5\\]](#page-8-13), LSVT [\\[40\\]](#page-9-16), MLT19 [\\[32\\]](#page-9-17), and ReCTS [\\[54\\]](#page-10-3). We also leverage two large-scale real datasets based on Open Images: TextOCR [\\[39\\]](#page-9-18) and OpenVINO [\\[25\\]](#page-8-14).\n\nTesting datasets. We use the IIIT5K [\\[31\\]](#page-9-19), CUTE80 [\\[36\\]](#page-9-20), Street View Text (SVT) [\\[47\\]](#page-9-21), SVT-Perspective (SVTP) [\\[33\\]](#page-9-22), ICDAR 2013 (IC13) [\\[23\\]](#page-8-15), and ICDAR 2015 (IC15) [\\[22\\]](#page-8-16) datasets for evaluation, following the previous method setting. We use the case-sensitive annotations of Long and Yao [\\[29\\]](#page-9-23) for IIIT5k, CUTE, SVT, and SVTP. For IC13 and IC15, there are two versions of their respective test splits commonly used in the literature: 857 and 1,015 for IC13; 1,811 and 2,077 for IC15. To avoid confusion, we refer to the benchmark as the union of IIIT5k, CUTE, SVT, SVTP, IC13 (857), IC13 (1,015), IC15 (1,811) and IC15 (2,077).\n\nEvaluation metrics. We use Correctly Recognized Words (CRW) as our metric, which is the percentage of correctly predicted words in the entire dataset. A prediction is considered as correct only if all characters in the recognized text match the corresponding characters in the ground truth word. We reported mean values obtained from three replicates per model.\n\nCharacter sets. Varying during both training and inference. Specifically, we use two charsets: a 36-character set containing lowercase alphanumeric characters, and a 94 character set containing mixed-case alphanumeric characters with punctuation.\n\n| Method                        | Train data      | IIIT5k | SVT   | IC13  | IC13  | IC15  | IC15  | SVTP  | CUTE  |\n|-------------------------------|-----------------|--------|-------|-------|-------|-------|-------|-------|-------|\n| Num                           | -               | 3,000  | 647   | 857   | 1,015 | 1,811 | 2,077 | 645   | 288   |\n|                               | 36-char results |        |       |       |       |       |       |       |       |\n| PARSeq [3]                    | R               | 99.1   | 97.9  | 98.3  | 98.4  | 90.7  | 89.6  | 95.7  | 98.3  |\n| S-GTR [17]                    | B               | 97.5   | 95.8  | 97.8  | -     | -     | 87.3  | 90.6  | 94.7  |\n| CDistNet [57]                 | B               | 96.4   | 93.5  | 97.4  | -     | 86.0  | -     | 88.7  | 93.4  |\n| TextScanner [45]              | S               | 95.7   | 92.7  | -     | 94.9  | –     | 83.5  | 84.8  | 91.6  |\n| AutoSTR [53]                  | S               | 94.7   | 90.9  | –     | 94.2  | 81.8  | –     | 81.7  | –     |\n| RCEED [7]                     | B               | 94.9   | 91.8  | –     | –     | –     | 82.2  | 83.6  | 91.7  |\n| PREN2D [52]                   | S               | 95.6   | 94.0  | 96.4  | –     | 83.0  | –     | 87.6  | 91.7  |\n| STN-CSTR [4]                  | S               | 94.2   | 92.3  | 96.3  | 94.1  | 86.1  | 82.0  | 86.2  | -     |\n| ViTSTR-B [1]                  | S               | 88.4   | 87.7  | 93.2  | 92.4  | 78.5  | 72.6  | 81.8  | 81.3  |\n| CRNN [37]                     | S               | 84.3   | 78.9  | -     | 88.8  | –     | 61.5  | 64.8  | 61.3  |\n| TRBA [2]                      | S               | 92.1   | 88.9  | -     | 93.1  | –     | 74.7  | 79.5  | 78.2  |\n| VisionLAN [49]                | S               | 95.8   | 91.7  | 95.7  | –     | 83.7  | –     | 86.0  | 88.5  |\n| Ours                          | R               | 99.4   | 98.2  | 98.6  | 98.5  | 90.9  | 89.8  | 96.1  | 98.2  |\n| 94-char results for IV words  |                 |        |       |       |       |       |       |       |       |\n| OVR [45]                      | R               | 95.19  | 95.45 | 96.23 | 95.78 | 87.12 | 86.78 | 92.77 | 94.13 |\n| TRBA [2]                      | R               | 96.40  | 96.04 | 95.30 | 95.05 | 87.47 | 86.78 | 93.28 | 95.32 |\n| Parseq [3]                    | R               | 97.36  | 97.13 | 96.77 | 97.31 | 90.0  | 89.35 | 95.65 | 96.54 |\n| Ours                          | R               | 97.24  | 96.96 | 97.52 | 97.69 | 90.66 | 89.95 | 95.11 | 95.47 |\n| 94-char results for OV words  |                 |        |       |       |       |       |       |       |       |\n| OVR [45]                      | R               | 89.43  | 79.01 | 88.01 | 88.12 | 55.34 | 57.20 | 80.03 | 86.29 |\n| TRBA [2]                      | R               | 89.21  | 78.73 | 88.07 | 88.00 | 55.15 | 57.45 | 79.78 | 86.02 |\n| Parseq [3]                    | R               | 90.43  | 79.79 | 89.66 | 89.60 | 57.74 | 60.0  | 82.01 | 88.25 |\n| Ours                          | R               | 93.91  | 81.05 | 92.20 | 92.20 | 59.80 | 62.06 | 83.27 | 89.52 |\n| 94-char results for all words |                 |        |       |       |       |       |       |       |       |\n| OVR [45]                      | R               | 92.31  | 87.23 | 92.12 | 91.95 | 71.23 | 71.99 | 86.40 | 90.21 |\n| TRBA [2]                      | R               | 92.81  | 87.39 | 91.69 | 91.53 | 71.31 | 72.12 | 86.53 | 90.67 |\n| Parseq [3]                    | R               | 93.90  | 88.46 | 93.22 | 93.46 | 73.87 | 74.68 | 88.83 | 92.40 |\n| Ours                          | R               | 95.57  | 89.00 | 94.86 | 94.94 | 75.23 | 76.00 | 89.19 | 92.49 |\n\n<span id=\"page-5-3\"></span>Table 1. Scene text recognition results, containing 36- and 94-char results. Specifically, we tested our approach on three types of datasets: Synthetic (S) datasets, including MJ and ST; Benchmark (B) datasets, including SVT, IIIT5k, IC13, and IC15; and Real (R) datasets, including COCO, RCTW17, Uber, ArT, LSVT, MLT19, ReCTS, TextOCR, and OpenVINO. Our proposed method outperformed other state-of-the-art methods in both 36- and 94-char experiments, and archived significant improvements for OOV words.\n\n## 4.2. Implementation Details\n\nOur pseudo label generation module employs YOLOv5s [\\[21\\]](#page-8-22) as the character detector. The settings of the character detector follow the official config. For image inpainting, we use three image datasets for pretraining: ImageNet, Places2, and CelebA-HQ, and utilize the same settings to train the partial convolutions inpainting model. For the semantic checking module, we not only utilize the training and validation sets in text recognition datasets, but adopt the COCA [2](#page-5-0) , word frequency[3](#page-5-1) , and iWeb datasets[4](#page-5-2) as checking lists.\n\nWe use ResNet45 [\\[15\\]](#page-8-23) as the backbone in our text recognition network, with weights initialized by the COCO pretrained model. We set the input image size to 256 × 64 and employ data augmentation techniques including random rotation, image truncation, color jittering, perspective distortion, RandAugment operations (excluding Sharpness), Invert, GaussianBlur, and PoissonNoise. We conducted experiments on 4 NVIDIA V100 GPUs with a batch size of 256, and trained the network end-to-end using the Adam optimizer with a learning rate of 1e−4, following the same settings as Parseq [\\[3\\]](#page-8-7).\n\nFor our proposed loss, the learning rate is initialized at 0.1 and decreased by a factor of 10 at epochs 10, 20, and 25. The training is stopped at the 40-th epoch. The weight decay and momentum are set to 5e−4 and 0.9, respectively. We fix the upper and lower bounds of the text quality as l<sup>a</sup> = 0.5, u<sup>a</sup> = 1, l<sup>m</sup> = 0, and u<sup>m</sup> = 6. Finally, our other hyperparameters are also consistent with MagFace [\\[30\\]](#page-9-7).\n\n<span id=\"page-5-0\"></span><sup>2</sup><https://www.english-corpora.org/coca/>\n\n<span id=\"page-5-1\"></span><sup>3</sup>[http://www.writewords.org.uk/word\\\\_count.asp](http://www.writewords.org.uk/word_count.asp)\n\n<span id=\"page-5-2\"></span><sup>4</sup><https://searchworks.stanford.edu/view/13223153>\n\n|       | <b>Predictions</b> |                   |                   |                     |                    |                  |\n|-------|--------------------|-------------------|-------------------|---------------------|--------------------|------------------|\n| Input | GT                 | Our               | <b>PARSeq</b>     | <b>ABINet</b>       | <b>TRBA</b>        | <b>CRNN</b>      |\n|       | esolanade          | esolanade         | esplanade         | esplaiee            | esplane            | <b>BsAaE</b>     |\n|       | Timms              | Timms             | T mms             | Timms               | Timms              | Timm             |\n|       | CHATHAM-KENT       | CHATHAM-KENT      | CHATHAM-KENT      | <b>CHATHAM KENT</b> | <b>CHATHAMEREY</b> | <b>CERHAMNNE</b> |\n|       | <b>CLASSROOMS</b>  | <b>CLASSROOMS</b> | <b>CLASSROOMS</b> | <b>CANSOSSST</b>    | CLA ROOMS          | <b>ENO</b>       |\n|       | <b>ANTS</b>        | ANT               | <b>ANTSTS</b>     | <b>KANTER</b>       | <b>BANTSEN</b>     | $N_{-}$          |\n|       | pasada             | pasada            | pasada.           | pasado              | pasada.            | pasodo           |\n|       | <b>BUFFEL</b>      | <b>BUFFEL</b>     | <b>BUF EL</b>     | <b>BUFFEF</b>       | <b>BUF EL</b>      | <b>BUF EL</b>    |\n\n<span id=\"page-6-2\"></span>Figure 3. Qualitative results for samples taken from various test datasets related to the OOV problem. Both context-free methods, TRBA [\\[2\\]](#page-8-21) and CRNN [\\[37\\]](#page-9-5), were unable to accurately predict certain cases, possibly due to the ambiguity involved. ABINet [\\[10\\]](#page-8-24) encountered difficulties recognizing vertically-oriented and rotated text. PARSeq [\\[3\\]](#page-8-7) also mis-recognized many characters. Compared with them, our method achieves the best performance.\n\n|                                             | IV: % | OOV: % | AVE: % |  |  |  |\n|---------------------------------------------|-------|--------|--------|--|--|--|\n| Ablation study for semantic checking module |       |        |        |  |  |  |\n| w/                                          | 95.06 | 81.12  | 88.09  |  |  |  |\n| w/o                                         | 94.61 | 79.22  | 86.92  |  |  |  |\n| Ablation study for data augment strategy    |       |        |        |  |  |  |\n| Both                                        | 95.06 | 81.12  | 88.09  |  |  |  |\n| w/o Remove                                  | 95.05 | 80.27  | 87.66  |  |  |  |\n| w/o Swap                                    | 94.98 | 80.54  | 87.76  |  |  |  |\n| Compare with other pseudo label method      |       |        |        |  |  |  |\n| SynthText [13]                              | 95.04 | 79.79  | 87.41  |  |  |  |\n| UnrealText [29]                             | 95.03 | 79.70  | 87.37  |  |  |  |\n| Edit [51]                                   | 95.03 | 79.65  | 87.34  |  |  |  |\n| Ours                                        | 95.06 | 81.12  | 88.09  |  |  |  |\n\n<span id=\"page-6-0\"></span>Table 2. The effects of main components in our pseudo label generation module, and the comparison with other pseudo label generation methods. 'W/o' and 'w/' mean 'without' and 'with', respectively. All results are the average of the eight test datasets under the CRW metric.\n\n#### 4.3. Main Results\n\nIn order to evaluate the effectiveness of our proposed method, we conducted a comparative analysis with popular and recent state-of-the-art methods. To ensure a fair comparison, we not only evaluated our method against published results, but also reproduced state-of-the-art methods. We reported 36- and 94-char results in Table [1.](#page-5-3) In both 36 and 94-char evaluations, our method outperformed previous methods. In particular, in the more difficult 94-char experiment, our method showed more significant improvements.\n\nIV and OV results were also shown in Table [1.](#page-5-3) It could be seen that although previous methods achieved good performance for IV words, they did not recognize OOV words\n\n|                                      | IV: % | OOV: % | AVE: % |  |  |  |\n|--------------------------------------|-------|--------|--------|--|--|--|\n| Ablation study for margin-based loss |       |        |        |  |  |  |\n| w/                                   | 95.01 | 80.26  | 87.64  |  |  |  |\n| w/o                                  | 95.03 | 79.68  | 87.36  |  |  |  |\n| Ablation study for quality indicator |       |        |        |  |  |  |\n| w/o                                  | 95.01 | 80.26  | 87.64  |  |  |  |\n| w/ image                             | 95.03 | 80.33  | 87.68  |  |  |  |\n| w/ box                               | 95.03 | 80.38  | 87.71  |  |  |  |\n| Compare with other margin-based loss |       |        |        |  |  |  |\n| Arcface [9]                          | 95.01 | 80.26  | 87.64  |  |  |  |\n| Megaface [30]                        | 95.03 | 80.33  | 87.68  |  |  |  |\n| Ours                                 | 95.03 | 80.38  | 87.71  |  |  |  |\n\n<span id=\"page-6-1\"></span>Table 3. The effects of each key component in our quality-aware margin loss, and the comparison with other margin-based losses. 'W/o' and 'w/' mean 'without' and 'with', respectively. All results are the average of the eight test datasets under the CRW metric.\n\nwell. In contrast, our proposed Pseudo-OCR yielded remarkable improvements for OOV words. We outperformed our baseline PARSeq [\\[3\\]](#page-8-7) by up to 3.48% for OOV words. OVR [\\[45\\]](#page-9-6) was also designed for open-vocabulary scene text recognition. However, it only leveraged multiple decoders to improve the generalization ability, while our method generated pseudo training samples for OOV words and achieved improvements of up to 4.48%. These superior results demonstrated the effectiveness of our pseudo labels and quality-aware margin loss.\n\n#### 4.4. Ablation Study\n\nThe effects of pseudo label generation. Table [2](#page-6-0) shows the results of our experiments in terms of IV of CRW score, OOV of CRW score, and their average scores. We conducted experiments to evaluate the performance of our proposed pseudo label generation module, comparing the results using several different settings. We employed semantic checking (\"w/\") and Remove/Swap in data augmentation (\"Both\") during implementation, which resulted in our method achieving scores of 95.06%, 81.12%, and 88.09% for IV, OOV, and their average, respectively. All of which were higher than those achieved with other options.\n\nCompare with other pseudo label methods. Table [2](#page-6-0) provides a comparative analysis of our proposed method with other state-of-the-art approaches for the same task. Our approach surpasses models that are pre-trained with SynthtText [\\[20\\]](#page-8-3), UnrealText [\\[29\\]](#page-9-23), or Edit [\\[51\\]](#page-10-7), and subsequently trained on real data, thanks to the integration of our novel pseudo-label module, which utilizes semantic checking and Remove/Swap techniques. Our method's performance metrics are impressive. In particular, our approach leverages semantic knowledge to validate the training data, and correct errors to improve accuracy.\n\nThe effects of quality-aware margin loss. Table [3](#page-6-1) presents the outcomes of our experiments, showcasing the IV and OOV of the CRW score, and their average scores. Our primary aim was to evaluate the effectiveness of our proposed quality-aware margin-based module through various settings. During implementation, we utilized marginbased (\"w/\") and quality indicator with detector box (\"w/ box\"), which yielded remarkable results. Our method achieved a score of 95.03%, 80.38%, and 87.71% for IV, OOV, and their average, respectively. These scores outperformed other alternatives. Overall, our study highlights the potential of the quality-aware margin-based module for enhancing performance in the CRW score evaluation.\n\nCompare with other margin-based loss. Table [3](#page-6-1) presents a comprehensive comparison of our loss with other margin-based losses. We have outperformed previous models that have utilized cos-like loss, including Arcface [\\[9\\]](#page-8-9) and Megaface [\\[30\\]](#page-9-7), by incorporating our novel margin-based and quality-aware components. Our method has achieved remarkable performance. Our approach has the added advantage of using the detector confidence score as a quality indicator, which may further enhance the loss convergence. Overall, our study provides valuable insights into the potential of margin-based and quality-aware techniques for enhancing performance in loss functions.\n\nQualitative analysis. Fig. [3](#page-6-2) shows the qualitative results on all test datasets, with the input images displayed in their original orientations and aspect ratios. Incorrect characters are highlighted in red, while missing characters are indicated by a red underscore for predictions that are roughly aligned with the ground truth. Both context-free methods, TRBA [\\[2\\]](#page-8-21) and CRNN [\\[37\\]](#page-9-5), did not accurately predict certain cases, possibly due to the ambiguity involved. ABI-Net [\\[10\\]](#page-8-24) encountered difficulties in recognizing verticallyoriented images. Considering the presence of obstructions\n\n|            | AVG: % | IV: % | OOV: % | Ensemble |\n|------------|--------|-------|--------|----------|\n| Method     | CRW    | CRW   | CRW    |          |\n| Ours       | 70.98  | 82.81 | 59.15  |          |\n| OCRFLYV2   | 70.31  | 81.02 | 59.61  | ✓        |\n| HuiGuanV2  | 70.28  | 81.73 | 58.83  | ✓        |\n| oov3decode | 70.22  | 81.58 | 58.86  | ✓        |\n| ViT-based  | 70.00  | 81.36 | 58.64  | ✓        |\n\n<span id=\"page-7-0\"></span>Table 4. Comparison of the proposed method with other stateof-the-art methods on the on-site test set of the ICDAR Out-of-Vocabulary Challenge. Our method achieves the first rank.\n\non the image, such as occlusion, we believe that the recognition results of all models in the last row of the image are suboptimal and that this cannot be avoided. The results demonstrate that our model can accurately identify OOV images and maintain good performance on IV images.\n\n#### 4.5. Comparison in ICADR 2022 Challenge\n\nOver 200 teams participated in the ICDAR Out of Vocabulary challenge, utilizing official training and validation data from various datasets including ICDAR13, ICDAR15, MLT19, COCO-Text, TextOCR, HierText, and OpenImagesText. This challenge focuses on the cropped word recognition task, where participants are required to predict the recognition of all cropped words in the test set. The test set contains 313,751 cropped words, out of which 271,664 are IV words and 42,087 are OOV ones. According to Table [4,](#page-7-0) our method outperformed other methods in the challenge. Notably, our method achieved this without utilizing the ensemble strategy.\n\n# 5. Conclusion\n\nIn this paper, we have presented a Pseudo-OCR framework for open-vocabulary scene text recognition. Our approach entails two major contributions. Firstly, we propose a novel pseudo label generation module that integrates character detection and image inpainting techniques to create a large volume of training data. Different from previous synthetic labels, our pseudo labels are closer to real-world images. Meanwhile, we design a text semantic checking in our pseudo label generation module to filter semantically meaningful pseudo data. Moreover, we introduce a margin loss that optimizes geodesic distance margins to mitigate the effect of low-quality samples. We also present a text quality indicator to dynamically adjust the margin of each class based on image quality. Extensive experimental results demonstrate that our method outperforms the state-ofthe-art on various test datasets, enhancing both IV and OOV performance. Moreover, our approach secured the first position in the ICDAR2022 challenge.\n\n# References\n\n- <span id=\"page-8-20\"></span>[1] Rowel Atienza. Vision transformer for fast and efficient scene text recognition. In *Document Analysis and Recognition–ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part I 16*, pages 319–334. Springer, 2021.\n- <span id=\"page-8-21\"></span>[2] Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwalsuk Lee. What is wrong with scene text recognition model comparisons? dataset and model analysis. In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 4715–4723, 2019.\n- <span id=\"page-8-7\"></span>[3] Darwin Bautista and Rowel Atienza. Scene text recognition with permuted autoregressive sequence models. In *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVIII*, pages 178–196. Springer, 2022.\n- <span id=\"page-8-19\"></span>[4] Hongxiang Cai, Jun Sun, and Yichao Xiong. Revisiting classification perspective on scene text recognition. *arXiv preprint arXiv:2102.10884*, 2021.\n- <span id=\"page-8-13\"></span>[5] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In *2019 International Conference on Document Analysis and Recognition (ICDAR)*, pages 1571–1576. IEEE, 2019.\n- <span id=\"page-8-4\"></span>[6] Fuze Cong, Wenping Hu, Qiang Huo, and Li Guo. A comparative study of attention-based encoder-decoder approaches to natural scene text recognition. In *2019 International Conference on Document Analysis and Recognition (ICDAR)*, pages 916–921. IEEE, 2019.\n- <span id=\"page-8-18\"></span>[7] Mengmeng Cui, Wei Wang, Jinjin Zhang, and Liang Wang. Representation and correlation enhanced encoder-decoder framework for scene text recognition. In *Document Analysis and Recognition–ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part IV 16*, pages 156–170. Springer, 2021.\n- <span id=\"page-8-8\"></span>[8] Yiming Cui, Ziqing Yang, and Ting Liu. Pert: pretraining bert with permuted language model. *arXiv preprint arXiv:2203.06906*, 2022.\n- <span id=\"page-8-9\"></span>[9] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 4690–4699, 2019.\n- <span id=\"page-8-24\"></span>[10] Shancheng Fang, Zhendong Mao, Hongtao Xie, Yuxin Wang, Chenggang Yan, and Yongdong Zhang. Abinet++: Autonomous, bidirectional and iterative language modeling for scene text spotting. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2022.\n- <span id=\"page-8-1\"></span>[11] Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, and Xilin Chen. Multi-modal graph neural network for joint reasoning on vision and scene text. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 12746–12756, 2020.\n- <span id=\"page-8-6\"></span>[12] Alex Graves, Santiago Fernandez, Faustino Gomez, and ´ Jurgen Schmidhuber. Connectionist temporal classification: ¨\n\nlabelling unsegmented sequence data with recurrent neural networks. In *Proceedings of the 23rd international conference on Machine learning*, pages 369–376, 2006.\n\n- <span id=\"page-8-12\"></span>[13] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Synthetic data for text localisation in natural images. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 2315–2324, 2016.\n- <span id=\"page-8-11\"></span>[14] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir- ´ shick. Mask r-cnn. In *Proceedings of the IEEE international conference on computer vision*, pages 2961–2969, 2017.\n- <span id=\"page-8-23\"></span>[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 770–778, 2016.\n- <span id=\"page-8-2\"></span>[16] Wenhao He, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu. Multi-oriented and multi-lingual scene text detection with direct regression. *IEEE Transactions on Image Processing*, 27(11):5406–5419, 2018.\n- <span id=\"page-8-17\"></span>[17] Yue He, Chen Chen, Jing Zhang, Juhua Liu, Fengxiang He, Chaoyue Wang, and Bo Du. Visual semantics allow for textual reasoning better in scene text recognition. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36, pages 888–896, 2022.\n- <span id=\"page-8-0\"></span>[18] Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Semantic object accuracy for generative text-to-image synthesis. *IEEE transactions on pattern analysis and machine intelligence*, 44(3):1552–1565, 2020.\n- <span id=\"page-8-5\"></span>[19] Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for sequence tagging. *arXiv preprint arXiv:1508.01991*, 2015.\n- <span id=\"page-8-3\"></span>[20] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Synthetic data and artificial neural networks for natural scene text recognition. *arXiv preprint arXiv:1406.2227*, 2014.\n- <span id=\"page-8-22\"></span>[21] Glenn Jocher. ultralytics/yolov5: v3.1 - Bug Fixes and Performance Improvements. [https://github.com/](https://github.com/ultralytics/yolov5) [ultralytics/yolov5](https://github.com/ultralytics/yolov5), Oct. 2020.\n- <span id=\"page-8-16\"></span>[22] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. In *2015 13th international conference on document analysis and recognition (ICDAR)*, pages 1156–1160. IEEE, 2015.\n- <span id=\"page-8-15\"></span>[23] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. Icdar 2013 robust reading competition. In *2013 12th international conference on document analysis and recognition*, pages 1484–1493. IEEE, 2013.\n- <span id=\"page-8-10\"></span>[24] Minchul Kim, Anil K Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 18750–18759, 2022.\n- <span id=\"page-8-14\"></span>[25] Ilya Krylov, Sergei Nosov, and Vladislav Sovrasov. Open images v5 text annotation and yet another mask text spotter.\n\nIn *Asian Conference on Machine Learning*, pages 379–389. PMLR, 2021.\n\n- <span id=\"page-9-0\"></span>[26] Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. Visual semantic reasoning for image-text matching. In *Proceedings of the IEEE/CVF International conference on computer vision*, pages 4654–4662, 2019.\n- <span id=\"page-9-9\"></span>[27] Minghui Liao, Jian Zhang, Zhaoyi Wan, Fengming Xie, Jiajun Liang, Pengyuan Lyu, Cong Yao, and Xiang Bai. Scene text recognition from two-dimensional perspective. In *Proceedings of the AAAI conference on artificial intelligence*, volume 33, pages 8714–8721, 2019.\n- <span id=\"page-9-13\"></span>[28] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In *Proceedings of the European conference on computer vision (ECCV)*, pages 85–100, 2018.\n- <span id=\"page-9-23\"></span>[29] Shangbang Long and Cong Yao. Unrealtext: Synthesizing realistic scene text images from the unreal world. *arXiv preprint arXiv:2003.10608*, 2020.\n- <span id=\"page-9-7\"></span>[30] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou. Magface: A universal representation for face recognition and quality assessment. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 14225–14234, 2021.\n- <span id=\"page-9-19\"></span>[31] Anand Mishra, Karteek Alahari, and CV Jawahar. Scene text recognition using higher order language priors. In *BMVC-British machine vision conference*. BMVA, 2012.\n- <span id=\"page-9-17\"></span>[32] Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowdhury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Umapada Pal, Jean-Christophe Burie, Cheng-lin Liu, et al. Icdar2019 robust reading challenge on multi-lingual scene text detection and recognition—rrc-mlt-2019. In *2019 International conference on document analysis and recognition (IC-DAR)*, pages 1582–1587. IEEE, 2019.\n- <span id=\"page-9-22\"></span>[33] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, and Chew Lim Tan. Recognizing text with perspective distortion in natural scenes. In *Proceedings of the IEEE International Conference on Computer Vision*, pages 569–576, 2013.\n- <span id=\"page-9-10\"></span>[34] Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, and Weiping Wang. Seed: Semantics enhanced encoder-decoder framework for scene text recognition. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 13528–13537, 2020.\n- <span id=\"page-9-1\"></span>[35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In *International Conference on Machine Learning*, pages 8821–8831. PMLR, 2021.\n- <span id=\"page-9-20\"></span>[36] Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng Chan, and Chew Lim Tan. A robust arbitrary text detection system for natural scene images. *Expert Systems with Applications*, 41(18):8027–8048, 2014.\n- <span id=\"page-9-5\"></span>[37] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. *IEEE transactions on pattern analysis and machine intelligence*, 39(11):2298–2304, 2016.\n- <span id=\"page-9-15\"></span>[38] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang Bai. Icdar2017 competition on reading chinese text in the wild (rctw-17). In *2017 14th iapr international conference on document analysis and recognition (ICDAR)*, volume 1, pages 1429–1434. IEEE, 2017.\n- <span id=\"page-9-18\"></span>[39] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 8802–8812, 2021.\n- <span id=\"page-9-16\"></span>[40] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al. Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt. In *2019 International Conference on Document Analysis and Recognition (ICDAR)*, pages 1557–1562. IEEE, 2019.\n- <span id=\"page-9-4\"></span>[41] Youbao Tang and Xiangqian Wu. Scene text detection and segmentation based on cascaded convolution neural networks. *IEEE transactions on Image Processing*, 26(3):1509– 1520, 2017.\n- <span id=\"page-9-14\"></span>[42] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. *arXiv preprint arXiv:1601.07140*, 2016.\n- <span id=\"page-9-3\"></span>[43] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing text and image for image retrieval-an empirical odyssey. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 6439–6448, 2019.\n- <span id=\"page-9-8\"></span>[44] Zhaoyi Wan, Minghang He, Haoran Chen, Xiang Bai, and Cong Yao. Textscanner: Reading characters in order for robust scene text recognition. In *Proceedings of the AAAI conference on artificial intelligence*, volume 34, pages 12120– 12127, 2020.\n- <span id=\"page-9-6\"></span>[45] Zhaoyi Wan, Jielei Zhang, Liang Zhang, Jiebo Luo, and Cong Yao. On vocabulary reliance in scene text recognition. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 11425–11434, 2020.\n- <span id=\"page-9-12\"></span>[46] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 5265–5274, 2018.\n- <span id=\"page-9-21\"></span>[47] Kai Wang, Boris Babenko, and Serge Belongie. End-to-end scene text recognition. In *2011 International conference on computer vision*, pages 1457–1464. IEEE, 2011.\n- <span id=\"page-9-11\"></span>[48] Xiaogang Wang, Meng Wang, and Wei Li. Scene-specific pedestrian detection for static video surveillance. *IEEE transactions on pattern analysis and machine intelligence*, 36(2):361–374, 2013.\n- <span id=\"page-9-24\"></span>[49] Yuxin Wang, Hongtao Xie, Shancheng Fang, Jing Wang, Shenggao Zhu, and Yongdong Zhang. From two to one: A new scene text recognizer with visual language modeling network. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 14194–14203, 2021.\n- <span id=\"page-9-2\"></span>[50] Zihao Wang, Xihui Liu, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang Wang, and Jing Shao. Camp: Cross-modal\n\nadaptive message passing for text-image retrieval. In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 5764–5773, 2019.\n\n- <span id=\"page-10-7\"></span>[51] Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jingtuo Liu, Errui Ding, and Xiang Bai. Editing text in the wild. In *Proceedings of the 27th ACM international conference on multimedia*, pages 1500–1508, 2019.\n- <span id=\"page-10-6\"></span>[52] Ruijie Yan, Liangrui Peng, Shanyu Xiao, and Gang Yao. Primitive representation learning for scene text recognition. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 284–293, 2021.\n- <span id=\"page-10-5\"></span>[53] Hui Zhang, Quanming Yao, Mingkun Yang, Yongchao Xu, and Xiang Bai. Autostr: efficient backbone search for scene text recognition. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIV 16*, pages 751–767. Springer, 2020.\n- <span id=\"page-10-3\"></span>[54] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun Yang, et al. Icdar 2019 robust reading challenge on reading chinese text on signboard. In *2019 international conference on document analysis and recognition (ICDAR)*, pages 1577–1581. IEEE, 2019.\n- <span id=\"page-10-2\"></span>[55] Ying Zhang, Lionel Gueguen, Ilya Zharkov, Peter Zhang, Keith Seifert, and Ben Kadlec. Uber-text: A large-scale dataset for optical character recognition from street-level imagery. In *SUNw: Scene Understanding Workshop-CVPR*, volume 2017, page 5, 2017.\n- <span id=\"page-10-0\"></span>[56] Ziyuan Zhao, Huiying Zhu, Zehao Xue, Zhao Liu, Jing Tian, Matthew Chin Heng Chua, and Maofu Liu. An image-text consistency driven multimodal sentiment analysis approach for social media. *Information Processing & Management*, 56(6):102097, 2019.\n- <span id=\"page-10-4\"></span>[57] Tianlun Zheng, Zhineng Chen, Shancheng Fang, Hongtao Xie, and Yu-Gang Jiang. Cdistnet: Perceiving multi-domain character distance for robust text recognition. *arXiv preprint arXiv:2111.11011*, 2021.\n- <span id=\"page-10-1\"></span>[58] Yingying Zhu, Cong Yao, and Xiang Bai. Scene text detection and recognition: Recent advances and future trends. *Frontiers of Computer Science*, 10:19–36, 2016.\\n，请你分析其研究动机、核心方法与公式推导细节。请结合摘要与正文信息，提取论文背景、问题定义、方法核心流程与理论基础。\n",
        "agent": "论文解读专家\n",
        "status": "completed",
        "output": "**论文标题**: Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss  \n**作者**: Xuhua Ren (MiHOYO Shanghai), Hengcan Shi (Monash University), Jin Li (Shaanxi Normal University)  \n**发表平台**: arXiv.org  \n**年份**: 2024  \n\n### 研究动机  \n现有场景文本识别方法主要针对预定义词汇（IV），但真实场景中存在大量词汇外（OOV）词汇。传统方法因缺乏OOV训练数据导致泛化能力不足，而合成数据因缺乏真实背景和光照纹理难以模拟真实场景。论文提出通过生成真实图像衍生的伪标签和设计质量感知边界损失，解决OOV识别难题。  \n\n### 方法原理  \n1. **伪标签生成模块**  \n   - **字符检测**：使用YOLOv5检测图像中的字符，通过RoIAlign提取字符级特征。  \n   - **图像修复与增强**：采用UNet结构修复被擦除字符区域，并通过字符交换或删除策略生成多样化OOV样本。  \n   - **语义校验**：基于大规模词典（如COCA、WordFrequency）过滤无意义伪标签，保留语义合理的样本。  \n\n2. **文本识别网络**  \n   基于PARSeq框架，采用ViT编码器和排列解码器结构：  \n   - **编码器**：12层ViT，将图像分块为token并加入位置编码。  \n   - **解码器**：单层Transformer，支持自回归或非自回归解码，通过随机排列掩码增强语言建模能力。  \n\n3. **质量感知边界损失**  \n   - **边界损失**（公式1）：引入动态边距 \\( m(a_i) \\)，基于字符检测置信度 \\( a_i \\) 调整类间间距，提升分类判别性。  \n     $$\n     L_m = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( \\log \\frac{e^{s(\\cos(\\theta_{y_i} + m(a_i)))}}{e^{s(\\cos(\\theta_{y_i} + m(a_i)))} + \\sum_{j \\neq y_i} e^{s \\cos(\\theta_j)}} \\right)\n     $$\n   - **质量指示器**（公式2-3）：利用检测框平均置信度 \\( a_i \\) 动态计算边距 \\( m(a_i) \\)，抑制低质量样本影响。  \n     $$\n     m(a_i) = \\frac{u_m - l_m}{u_a - l_a} \\times (a_i - l_a) + l_m, \\quad a_i = \\frac{\\sum_{j=1}^{N} c_j}{N}\n     $$  \n\n### 关键创新点  \n1. **真实背景伪标签生成**：结合字符检测与图像修复，生成包含真实光照和扭曲的OOV训练数据，优于传统合成方法。  \n2. **语义校验机制**：通过词典匹配过滤无效伪标签，提升数据质量。  \n3. **动态边界损失**：首次将字符检测置信度作为文本质量指标，自适应调整损失权重，缓解低质量样本干扰。  \n\n### 实验效果  \n在8个数据集（IIIT5k、SVT等）上超越SOTA，OOV识别准确率提升3.48%-4.48%，并在ICDAR2022挑战赛排名第一。"
    },
    {
        "timestamp": "2025-06-13 16:12:47",
        "task_name": "review_task",
        "task": "审稿视角评估论文《Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss》，指出其亮点与不足，覆盖方法合理性、实验设计、贡献大小与适用范围等方面。\n",
        "agent": "严谨的学术审稿人\n",
        "status": "started"
    },
    {
        "timestamp": "2025-06-13 16:13:28",
        "task_name": "review_task",
        "task": "审稿视角评估论文《Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and Margin Loss》，指出其亮点与不足，覆盖方法合理性、实验设计、贡献大小与适用范围等方面。\n",
        "agent": "严谨的学术审稿人\n",
        "status": "completed",
        "output": "**结构化评审意见**  \n\n### **优点分析**  \n1. **问题重要性**：  \n   - 针对开放词汇场景文本识别（OV-STR）这一实际应用中的关键挑战，准确指出现有方法在OOV词汇处理上的局限性，研究动机明确且具有现实意义。  \n\n2. **方法创新性**：  \n   - **伪标签生成策略**：结合字符检测（YOLOv5）与图像修复（UNet），生成的OOV样本保留真实场景的纹理和光照特性，显著优于传统合成数据方法。  \n   - **动态边界损失设计**：首次引入字符检测置信度作为质量指标，自适应调整分类边界，有效缓解低质量样本对模型训练的干扰，具有理论创新性。  \n   - **语义校验机制**：利用大规模词典过滤无意义伪标签，提升数据可靠性，方法简单但实用性强。  \n\n3. **实验完整性**：  \n   - 在8个主流数据集（如IIIT5k、SVT）和ICDAR2022挑战赛上验证方法有效性，OOV识别准确率提升显著（3.48%-4.48%），实验覆盖全面，结果可信。  \n   - 提供详实的消融实验（如伪标签生成模块、边界损失的单独作用），验证各组件贡献。  \n\n### **存在的问题**  \n1. **伪标签生成局限性**：  \n   - 依赖字符检测器（YOLOv5）的精度，若检测漏检或误检会直接导致伪标签错误，但未分析检测失败对最终性能的影响程度。  \n   - 图像修复可能引入伪影，论文未对比修复前后对识别性能的具体影响。  \n\n2. **边界损失理论支持不足**：  \n   - 动态边距公式（公式1-3）的设计偏向启发式，缺乏严格的数学推导或理论证明（如Margin为何与检测置信度线性相关）。  \n\n3. **实验对比不充分**：  \n   - 未与近期基于大语言模型（如LLM）的OV-STR方法（如TAPIR）对比，可能低估现有SOTA的潜力。  \n   - 跨领域泛化实验缺失（如艺术字体、手写文本等场景）。  \n\n4. **计算成本**：  \n   - 伪标签生成需额外训练检测器和修复模型，未讨论实际部署时的效率问题。  \n\n### **改进建议**  \n1. **增强鲁棒性分析**：  \n   - 补充字符检测失败案例对模型影响的量化分析（如检测AP与最终识别准确率的相关性）。  \n   - 对比修复前后的识别性能差异，验证修复必要性。  \n\n2. **理论深化**：  \n   - 从度量学习角度（如角度间隔损失理论）解释动态边距的合理性，或通过实验验证线性关系的优越性。  \n\n3. **实验补充**：  \n   - 增加与大模型基线的对比（如BLIP-2、LLaVA等），或探讨结合大语言模型的可能性。  \n   - 测试在非标准文本（弯曲、遮挡、低光照）上的表现，体现实际适用性。  \n\n4. **效率优化**：  \n   - 分析伪标签生成的耗时占比，或提出轻量级替代方案（如基于扩散模型的快速生成）。  \n\n### **最终评估意见**  \n**推荐有条件接收（Major Revision）**。论文在开放词汇文本识别领域提出了具有创新性的方法，实验充分证明了其有效性，但在理论严谨性、对比全面性和鲁棒性分析上需进一步改进。修订后有望成为该领域的重要参考。  \n\n---  \n**具体修订要求**：  \n1. 补充理论分析或实验验证动态边距设计的合理性。  \n2. 增加与大模型基线的对比实验或讨论。  \n3. 详细说明伪标签生成的失败案例及应对策略。"
    },
    {
        "timestamp": "2025-06-13 16:13:28",
        "task_name": "reporting_task",
        "task": "整合前述研究内容与评审内容，撰写面向公众的论文解读报告（中文），类似机器之心或PaperWeekly风格，要求逻辑清晰、条理明确、结构如下： # 论文解析 1. 论文信息（论文英文标题、作者、机构、发表论文会议/期刊名，年份） 2. 研究背景与动机 3. 相关工作介绍（重点强调本工作和以往工作的区别，以及过往工作的脉络梳理） 3. 方法简介（包括公式详细解析） 4. 实验设计与主要结果\n# 评审意见 1. 不足1 2. 不足2\n# 总体评价与启示\n",
        "agent": "科技写作与内容整理专家\n",
        "status": "started"
    }
]
