[
    {
        "timestamp": "2025-06-13 16:14:11",
        "task_name": "research_task",
        "task": "阅读论文《POWN: Prototypical Open-World Node Classification》，论文于2024-06-14发布于CoLLAs内容如下：\\n# POWN: PROTOTYPICAL OPEN-WORLD NODE CLASSIFICATION\n\nMarcel Hoffmann University of Ulm Germany marcel.hoffmann@uni-ulm.de\n\nLukas Galke MPI for Psycholinguistics Nijmegen, Netherlands lukas.galke@mpi.nl Ansgar Scherp University of Ulm Germany ansgar.scherp@uni-ulm.de\n\n### ABSTRACT\n\nWe consider the problem of *true* open-world semi-supervised node classification, in which nodes in a graph either belong to known or new classes, with the latter not present during training. Existing methods detect and reject new classes but fail to distinguish between different new classes. We adapt existing methods and show they do not solve the problem sufficiently. We introduce a novel end-toend approach for classification into known classes and new classes based on class prototypes, which we call Prototypical Open-World Learning for Node Classification (POWN). Our method combines graph semi-supervised learning, self-supervised learning, and pseudo-labeling to learn prototype representations of new classes in a zero-shot way. In contrast to existing solutions from the vision domain, POWN does not require data augmentation techniques for node classification. Experiments on benchmark datasets demonstrate the effectiveness of POWN, where it outperforms baselines by up to 20% accuracy on the small and up to 30% on the large datasets. Source code is available at <https://github.com/Bobowner/POWN>.\n\n### 1 INTRODUCTION\n\nNode classification is the task of assigning labels to nodes of a graph based on information such as the node's features and the structure of the neighborhood. A typical application is assigning topics to papers in a citation graph. A common assumption in supervised node classification is that all classes in the test set were also part of the training set [\\(Kipf & Welling,](#page-11-0) [2017;](#page-11-0) [Velickovic et al.,](#page-12-0) [2018;](#page-12-0) [Hu et al.,](#page-10-0) [2021\\)](#page-10-0), known as closed-world assumption. Considering a real-world node classification setting, this assumption does not necessarily hold true. For instance, in a co-purchase graph in e-commerce, vendors add products from new categories at any time. In citation graphs, new research topics may arise that do not fit into the predefined categories, or in social networks, new communities may form discussing new topics. In these scenarios, the unlabeled graph can be collected nearly for free, but acquiring labels is expensive as it involves human annotators.\n\nIn an open-world setting, methods need to deal with the appearance of new classes. This entails problems of out-ofdistribution detection and generalization. There are two kinds of strategies to handle the new classes in this setting: The model can either reject and deny to classify new classes [\\(Wu et al.,](#page-12-1) [2021;](#page-12-1) [Hoffmann et al.,](#page-10-1) [2023;](#page-10-1) [Galke et al.,](#page-10-2) [2023\\)](#page-10-2), i. e., become a *robust open-world* model, or the model integrates the new classes and classifies new instances in a zero-shot way [\\(Sun & Li,](#page-12-2) [2023;](#page-12-2) [Cao et al.,](#page-9-0) [2022\\)](#page-9-0), i. e., become a *true open-world* model. Following the latter, we formulate the task of *true open-world semi-supervised node classification*. Given a graph G = (V, E), with nodes V and edges E, a set of labeled nodes V<sup>l</sup> ⊂ V with known classes, and a set of unlabeled nodes V<sup>u</sup> ⊂ V , such that V = V<sup>l</sup> ∪ V<sup>u</sup> and V<sup>l</sup> ∩ V<sup>u</sup> = ∅. The task is to classify the unlabeled nodes V<sup>u</sup> into all classes, including new ones, as V<sup>u</sup> may contain nodes of known and new classes. It requires the model to be a strong classifier on the known classes, while being able to discover and classify new classes, too.\n\nA similar problem has been studied by [Sun & Li](#page-12-2) [\\(2023\\)](#page-12-2) and [Cao et al.](#page-9-0) [\\(2022\\)](#page-9-0) on images in computer vision. However, these methods heavily rely on image augmentation strategies to produce positive and negative samples for selfsupervised contrastive learning. This renders them not directly applicable to node classification due to the non-i. i. d. nature of each sample, i. e., modifying edges of a node to change its class influences also the neighboring nodes.\n\nExisting works on open-world node classification [\\(Wu et al.,](#page-12-1) [2021;](#page-12-1) [Hoffmann et al.,](#page-10-1) [2023;](#page-10-1) [Galke et al.,](#page-10-2) [2023\\)](#page-10-2) are limited as they only detect the presence of new classes and reject them. Thus, they cannot distinguish between multiple new classes, i. e., do not learn to classify them.\n\nTo address the true open-world semi-supervised node classification problem, we propose a novel end-to-end prototypical open-world learning method for node classification (POWN). It is not only able to distinguish between known and new classes but can also classify nodes in the new classes in a zero-shot way without prior knowledge of the classes, i. e., semantic class descriptions such as a class name [\\(Ju et al.,](#page-10-3) [2023\\)](#page-10-3). The proposed POWN extends the work by [Sun](#page-12-2) [& Li](#page-12-2) [\\(2023\\)](#page-12-2), a method designed for vision, by supporting the graph structure and avoiding the strong dependence on data augmentation. POWN combines graph semi-supervised learning with self-supervised learning and pseudo-labels from label propagation [\\(Zhu & Ghahramani,](#page-13-0) [2002\\)](#page-13-0) to learn prototype representations for known and new classes. It models each class by a prototype in the embedding space and assigns labels to nodes based on the distance to the closest prototype. The node and prototype representations are learned by three loss functions: a supervised loss to learn representations of the labeled nodes, an unsupervised loss based on Deep Graph Infomax (DGI) [\\(Velickovic](#page-12-3) [et al.,](#page-12-3) [2019\\)](#page-12-3) to learn representations of the unlabeled nodes, and a loss based on pseudo-labels to assign the unlabeled nodes to a prototype. The pseudo-labels are assigned by label propagation where the edges are weighted by the distance to the closest prototype in the embedding space. This combination of label propagation on the edges and the distance of the embedding to the prototype bridges the gap between the graph topology and the embedding space for the pseudo-label assignment.\n\nOur experiments confirm that POWN effectively tackles the true open-world node classification setting. It is able to maintain the accuracy on the labeled classes, while successfully outperforming the baselines by up to 20% in accuracy on all classes, i. e., known and new classes together. In summary, our contributions are\n\n- We formalize the task of true open-world semi-supervised node classification. For this task, the model has to be a strong classifier on the known classes, while being able to distinguish known and new classes, and learn meaningful representations without access to any labels.\n- We propose the first true open-world semi-supervised node classification method for graphs called POWN. The method is trained end-to-end.\n- Experiments on six benchmark datasets show the performance of our method. We outperform all baselines based on GCN [\\(Kipf & Welling,](#page-11-0) [2017\\)](#page-11-0), DGI [\\(Velickovic et al.,](#page-12-3) [2019\\)](#page-12-3), spectral clustering [\\(Ng et al.,](#page-11-1) [2001\\)](#page-11-1), and OpenWGL [\\(Wu et al.,](#page-12-1) [2021\\)](#page-12-1), especially on large graphs.\n- Ablation studies provide detailed insights into the embeddings and show the robustness to hyperparameter selection of the new method.\n\n### 2 RELATED WORK\n\nWe introduce graph neural networks and self-supervised graph learning, which focuses on learning good representations solely based on the node features and edges. We discuss open-world learning and the limitations of current literature. Finally, we describe graph few-shot learning, which uses some labeled data points for learning new classes.\n\nGraph Neural Networks and Graph Self-Supervised Learning Graph neural networks (GNNs) aggregate node representations by message-passing over the edges of the graph. Among the most prominent GNNs are GCN [\\(Kipf &](#page-11-0) [Welling,](#page-11-0) [2017\\)](#page-11-0), GAT [\\(Velickovic et al.,](#page-12-0) [2018\\)](#page-12-0), and GraphSAGE [\\(Hamilton et al.,](#page-10-4) [2017\\)](#page-10-4). GNNs became the standard model in (semi-)supervised node classification, where only a few labels are provided as training instances for each class.\n\nIn contrast to (semi-)supervised node classification, the goal of graph self-supervised learning is to acquire meaningful representations of the nodes without relying on any predefined labels [\\(Liu et al.,](#page-11-2) [2021\\)](#page-11-2). During training, graph selfsupervised learning models predict supervision signals computed only from the graph itself. Depending on how these supervision signals are generated, these graph self-supervised learning methods can be categorized into generationbased, auxiliary property-based, contrast-based, and hybrid methods [\\(Liu et al.,](#page-11-2) [2021\\)](#page-11-2). Generation-based methods train the model to reconstruct some part of the graph, e. g., node features [\\(You et al.,](#page-12-4) [2020\\)](#page-12-4) or graph structure [\\(Kipf](#page-11-3) [& Welling,](#page-11-3) [2016\\)](#page-11-3). Auxiliary-based methods define selected properties of the graph and train a model to predict them, e. g., node degree or cluster indices [\\(You et al.,](#page-12-4) [2020\\)](#page-12-4). Contrast-based methods maximize the mutual information between two corrupted versions of the same node [\\(Velickovic et al.,](#page-12-3) [2019;](#page-12-3) [Mavromatis & Karypis,](#page-11-4) [2021;](#page-11-4) [Liu et al.,](#page-11-5) [2023\\)](#page-11-5). A popular representative is Deep Graph Infomax (DGI) [\\(Velickovic et al.,](#page-12-3) [2019\\)](#page-12-3). It trains a model to predict whether a node or its corrupted version belongs to an aggregated vector representation of the graph, which is computed by averaging the node embeddings. Expanding on DGI, [Mavromatis & Karypis](#page-11-4) [\\(2021\\)](#page-11-4) introduced an additional cluster-level similarity to the global aggregated vector of DGI and optimized both. [Liu et al.](#page-11-5) [\\(2023\\)](#page-11-5) used multiple autoregressively generated subgraphs as positive samples for the contrastive loss in graph classification. Hybrid methods combine multiple objectives. Two representatives are GPT-GNN [\\(Hu et al.,](#page-10-5) [2020b\\)](#page-10-5), which is simultaneously trained for feature-based and structure-based graph generation, and GMI [\\(Peng et al.,](#page-12-5) [2020\\)](#page-12-5), which maximizes the mutual information among node neighbors while minimizing the generation error on edge reconstruction.\n\nOpen-World Learning The (semi-)supervised learning setting is based on the closed-world assumption, i. e., each class observed during test time has been present during training [\\(Chen & Liu,](#page-10-6) [2016;](#page-10-6) [Kipf & Welling,](#page-11-0) [2017;](#page-11-0) [Wu et al.,](#page-12-1) [2021\\)](#page-12-1). Settings that drop the closed-world assumption are denoted as *open-world* settings. In the open-world setting, there are two strategies to handle the new classes in the test data. The model either rejects and refuses to classify new classes to become a *robust open-world* model, or the model integrates new classes and classifies new instances in a zero-shot way to become a *true open-world* model. So far, research has focused mainly on the robust open-world setting [\\(Esmaeilpour et al.,](#page-10-7) [2022;](#page-10-7) [Parmar et al.,](#page-11-6) [2023;](#page-11-6) [Wu et al.,](#page-12-1) [2021;](#page-12-1) [Hoffmann et al.,](#page-10-1) [2023;](#page-10-1) [Galke et al.,](#page-10-8) [2021\\)](#page-10-8).\n\nThere are two exceptions coming from the computer vision domain. One is ORCA [\\(Cao et al.,](#page-9-0) [2022\\)](#page-9-0), which exploits the different learning speeds of known versus new classes by an adaptive margin mechanism and introduces a pseudo-label mechanism based on pairwise similarity, i. e., assigns the same pseudo-label to close samples. The other is OpenCon [\\(Sun & Li,](#page-12-2) [2023\\)](#page-12-2), which learns representations based on three contrastive loss functions. The supervised loss L<sup>S</sup> is computed on the labeled dataset, pushing labeled samples close together in the embedding space. The unsupervised loss L<sup>U</sup> is computed on the unlabeled dataset, pushing similar examples together. The third one is a supervised loss L<sup>P</sup> based on the pseudo-labels determined by the closest class prototype. The total loss is computed by a weighted sum of the three loss functions. Both methods require data augmentation to generate positive and negative samples in contrastive learning. Data augmentations as they are done for images, e. g., rotation, translation, and clipping [\\(Chen et al.,](#page-9-1) [2020\\)](#page-9-1), cannot be applied to graphs. Although there are data augmentation strategies for graphs [\\(Rong](#page-12-6) [et al.,](#page-12-6) [2020\\)](#page-12-6), they are not suitable for creating explicit positive and negative samples for node classification.\n\nIn contrast to images, the work on graphs is limited to the robust open-world setting [\\(Galke et al.,](#page-10-2) [2023;](#page-10-2) [Hoffmann](#page-10-1) [et al.,](#page-10-1) [2023;](#page-10-1) [Wu et al.,](#page-12-1) [2021\\)](#page-12-1). OpenWGL [\\(Wu et al.,](#page-12-1) [2021\\)](#page-12-1) trained a variational graph auto-encoder to learn embeddings, optimized to increase the uncertainty of new classes for the model and reject to classify nodes for which the prediction is highly uncertain. [Galke et al.](#page-10-2) [\\(2023\\)](#page-10-2) applied Deep Open Classification [\\(Shu et al.,](#page-12-7) [2017\\)](#page-12-7) from the text domain to graphs by weighting the loss function to account for the class imbalance in citation graphs. [Hoffmann](#page-10-1) [et al.](#page-10-1) [\\(2023\\)](#page-10-1) proposed a meta-method for aggregating confidence scores combined with a weakly-supervised threshold detection method to reject new classes. [Xu et al.](#page-12-8) [\\(2023\\)](#page-12-8) focused on active learning and thus require a human oracle to label a set of nodes under a given budget. This way, they not only rejected but also learned new classes, similar to our work. However, [Xu et al.](#page-12-8) [\\(2023\\)](#page-12-8) shift the focus towards determining a set of good nodes given to the annotator and relaxes the open-world scenario, where in general no labels are provided for new classes. Overall, existing open-world learning approaches focus on identifying and excluding new classes, rather than classifying objects into them, or they rely on human annotators to handle new classes.\n\nGraph Few-Shot Learning In graph few-shot learning, the model has to learn classes based on a few labeled nodes per class [\\(Ding et al.,](#page-10-9) [2020;](#page-10-9) [Zhou et al.,](#page-13-1) [2019\\)](#page-13-1), e. g., three or five. Most graph few-shot learning models use metalearning where the objective is to learn a few-shot learner from a large number of labeled classes. A prominent representative is Graph Prototypical Networks [\\(Ding et al.,](#page-10-9) [2020\\)](#page-10-9). It learns to derive prototypes from a few labeled samples based on a node encoder and a node valuator, which assigns a weight to each node. The prototype is the weighted mean of the samples from its class. Graph Prototypical Networks has been extended to Geometer [\\(Lu et al.,](#page-11-7) [2022\\)](#page-11-7) for the class incremental graph few-shot learning setting by adding geometry-motivated loss functions to the prototypes, i. e., supporting a uniform distribution of the prototypes and maximizing the distance between prototypes. Other methods, e. g., Meta-GNN [\\(Zhou et al.,](#page-13-1) [2019\\)](#page-13-1), avoid using prototypes by applying meta-learning directly to the classifier. Graph few-shot learning differs from our setting since we assume to encounter new classes without any labeled nodes in the training set. In few-shot learning, a few labeled nodes are available, while in zero-shot learning the models have no access to training data of specific classes. Thus, in zero-shot learning, prior knowledge is often required such as a semantic description of the class [\\(Ju et al.,](#page-10-3) [2023\\)](#page-10-3), e. g., the class name \"biology\". In our approach, we do not have such a requirement.\n\n### 3 PROBLEM FORMALIZATION\n\nBased on the discussions in the related work, we formulate the true open-world node classification task. Given a graph G = (V, E) and a set of classes Y , where V is the set of nodes and E is the set of edges. The nodes are separated into a set of labeled nodes V<sup>l</sup> ⊆ V and a set of unlabeled nodes V<sup>u</sup> ⊆ V . The nodes in V<sup>l</sup> are from known classes Y<sup>k</sup> ⊆ Y , while the nodes in V<sup>u</sup> need to be classified into known classes Y<sup>k</sup> as well as new classes Yn. For the latter, no instance has been observed in the training data V<sup>l</sup> , i. e., Y<sup>n</sup> ̸⊂ Yk. The task is to determine a model f that maintains performance on the known classes Yk, while also correctly classifying the new classes Y<sup>n</sup> in a zero-shot manner. To excel in this task, a model needs to unify three characteristics: It needs to be a strong classifier of the known classes while distinguishing known and new classes, and is learning meaningful representations without access to any labels.\n\n<span id=\"page-3-0\"></span>![](_page_3_Figure_1.jpeg)\n\nFigure 1: Overview of the losses in POWN. The input graph has four classes, where two are known (red and blue) and two are new (orange and green). The nodes without a question mark are nodes in V<sup>l</sup> and the nodes with a question mark are from Vu. Stars represent the prototypes in embedding space, and w<sup>i</sup> is the label propagation weight.\n\nAssumptions We assume to know the number of existing classes. The number of potential new classes is either known from the domain or can be estimated by existing algorithms [\\(Han et al.,](#page-10-10) [2019\\)](#page-10-10). Experiments with an estimated number of classes are provided in Appendix [B.1.](#page-14-0) We assume a transductive setting where the full graph structure and node features are available for training. Such a transductive setting is relevant for many real-world scenarios, where the graph can often be collected easily but human annotations are expensive. Furthermore, we assume homophilic graphs, i. e., connected nodes likely belong to the same class. If a domain is homophilic, this property holds for known and new classes. This is the foundation for the label propagation of pseudo-labels. Many real-world graphs are homophilic, e. g., citation graphs, social networks, or co-purchase graphs [\\(Kipf & Welling,](#page-11-0) [2017\\)](#page-11-0).\n\n### 4 PROTOTYPICAL OPEN-WORLD NODE CLASSIFICATION (POWN)\n\nOur proposed method, POWN, combines graph semi-supervised with self-supervised learning and pseudo-labels from label propagation to learn prototype representations for known and new classes. An illustration of POWN is presented in Figure [1.](#page-3-0) Similar to the approach of [Sun & Li](#page-12-2) [\\(2023\\)](#page-12-2), we use three different loss functions for different parts of the data. The supervised loss L<sup>S</sup> is computed on the labeled part of the data V<sup>l</sup> , the unsupervised loss L<sup>U</sup> is computed on the unlabeled part of the data Vu, and a pseudo-label loss L<sup>P</sup> is computed on a subset of the unlabeled data Vn, where a new class can be determined with high confidence. The supervised and pseudo-label losses could be combined into one loss, but having them separate allows for applying different weights during training depending on the dataset.\n\n#### 4.1 PROTOTYPES AND SOFT PROTOTYPE MEMBERSHIP\n\nConsider a set of prototypes P. Each prototype p<sup>y</sup> ∈ P represents a (potential) class. We compute the embedding of each node v by z = f(x), where f is some encoder, e. g., a graph neural network. We treat each prototype as an additional vector of learnable parameters of the neural network. The probability that a node v belongs to prototype p<sup>i</sup> , i. e., class y<sup>i</sup> , is computed by:\n\n<span id=\"page-3-1\"></span>\n$$\n\\mathbf{p}_i(p_i|v) = \\frac{\\exp(-d(z, p_i)/\\tau)}{\\sum_{p_i \\in P} \\exp(-d(z, p_i)/\\tau)},\n$$\n\\n(1)\n\nwhere d is some distance metric, P the set of prototypes, and τ a temperature parameter.\n\n#### 4.2 PROTOTYPE-BASED REPRESENTATION LEARNING\n\nWe compute different loss functions on different parts of the data. The general aim is to minimize the distance between a prototype vector and the embeddings of the nodes belonging to the prototype's class by using the negative log-likelihood loss function:\n\n<span id=\"page-3-2\"></span>\n$$\n\\mathcal{L}_{\\text{nl}}(V', P') = \\frac{1}{|P'| |V'|} \\sum_{p_i \\in P'} \\sum_{v \\in V'_i} -\\log(\\mathbf{p}_i(p_i|v)), \\text{ where } V'_i = \\{v \\in V' \\mid \\text{label}(v) = i \\}\n$$\n (2)\n\npi(p<sup>i</sup> |v<sup>j</sup> ) is the probability computed in Equation [1,](#page-3-1) with P ′ ⊆ P a subset of the prototypes and V ′ ⊆ V a subset of the nodes. The set V ′ i is a subset of V ′ consisting of the vertices with the (pseudo-)label of class i, i. e., label(v) = i means that the node v belongs to class i. Both, V ′ and P ′ are parameters that are defined later by the respective loss function.\n\nTo learn compact representations for the labeled data, we use the labels of the nodes in V<sup>l</sup> to minimize the distance between each node in V<sup>l</sup> and its corresponding prototype. This is achieved by optimizing the supervised loss L<sup>S</sup> = Lnll(V<sup>l</sup> , Pk), where P<sup>k</sup> is the set of prototypes corresponding to a known class.\n\nModels for the image domain often combine data augmentation with contrastive learning to obtain good representations of unlabeled data [\\(Chen et al.,](#page-9-1) [2020;](#page-9-1) [Li et al.,](#page-11-8) [2021\\)](#page-11-8). Creating positive samples by data augmentation for node classification is difficult [\\(Liu et al.,](#page-11-2) [2021\\)](#page-11-2) because of the non-i. i. d. property of the samples. Therefore, we use the Deep Graph Infomax (DGI) loss, which only requires creating negative samples. For applying DGI, two additional functions are needed, a corruption function that modifies node features and a summary function to compute an embedding of the whole graph by aggregating all node embeddings. For both, we follow the results of DGI and shuffle node features as a corruption function and take the mean of the node embeddings as a summary function. Given the corruption and summary functions, the DGI loss for the unlabeled data V<sup>u</sup> results in:\n\n$$\n\\mathcal{L}_U = \\frac{1}{|V_u|} \\sum_{v_i \\in V_u} - (\\log (D(z_i, s)) + \\log (1 - D(\\tilde{z}_i, s))), \\tag{3}\n$$\n\nwhere s is the summary vector of the graph, z˜<sup>i</sup> is the corrupted version of z<sup>i</sup> , and D is a linear binary classifier that is trained to distinguish between samples that belong to the summary s and samples that do not.\n\nSubsequently, we split the unlabeled nodes V<sup>u</sup> in a set V<sup>n</sup> ⊂ V<sup>u</sup> and V<sup>n</sup> = V<sup>u</sup> \\ V<sup>n</sup> based on their distance to knownclass prototypes. The subset V<sup>n</sup> contains all unlabeled nodes which likely belong to new classes and V<sup>n</sup> contains all unlabeled nodes which likely belong to known class. We define the subset V<sup>n</sup> of unlabeled nodes v<sup>i</sup> ∈ V<sup>u</sup> by\n\n$$\nV_n = \\{v_i \\in V_u | \\max_{y \\in Y_k} \\{ \\langle p_y, z_i \\rangle \\} \\} < \\gamma \\},\\tag{4}\n$$\n\nwhere v<sup>i</sup> belongs with high confidence to a new class, i. e., its embedding z<sup>i</sup> is highly different from a prototype of one of the known classes. The threshold γ is determined on the labeled data V<sup>l</sup> such that q percent, e. g., q = 90%, of the labeled data is above the threshold. This procedure implements a simplified OOD detection based on the method of [Macedo et al.](#page-11-9) ˆ [\\(2022\\)](#page-11-9) to obtain high likelihoods that nodes in V<sup>n</sup> belong to a new class. For each node v<sup>i</sup> ∈ Vn, we determine the closest prototype based on cosine distance and assign the pseudo-label yˆ of the prototype to the node, i. e., yˆ = arg maxy∈Y<sup>n</sup> (⟨ py, zi⟩). We calculate the loss with respect to the pseudo labels as a variant of Equation [2](#page-3-2) by L<sup>P</sup> = Lnll(Vn, Pp), where P<sup>p</sup> is the set of prototypes to which at least one node of V<sup>n</sup> has been assigned to by the pseudo-label method. We assume that there are new classes in Vn, use V<sup>n</sup> to assign pseudo labels and calculate the loss L<sup>P</sup> , while nodes in V c n are only affected by the unsupervised loss.\n\n#### 4.3 ASSIGNMENT AND PROPAGATION OF PSEUDO LABELS\n\nTo update the prototypes for new classes, we assign pseudo-labels to the unlabeled nodes. Our pseudo-label method is based on the homophily assumption of the label-propagation algorithm [\\(Zhu & Ghahramani,](#page-13-0) [2002\\)](#page-13-0). To obtain the final labels, we perform three steps: First, we assign to the nodes in V<sup>n</sup> the pseudo label of their closest prototype as described in the previous section. The pseudo-labeled nodes serve as seeds for the second step, our weighted labelpropagation algorithm, where each edge (v<sup>i</sup> , v<sup>j</sup> ) is weighted by the distance of v<sup>i</sup> to its closest prototype. This inhibits the label flow from uncertain nodes. Thus, the weighting combines the graph topology with the embedding space. Given two nodes v<sup>i</sup> , v<sup>j</sup> and the edge (v<sup>i</sup> , v<sup>j</sup> ), the weight is\n\n<span id=\"page-4-0\"></span>\n$$\nw_j = 1/(\\|z_j - p_c\\|)\\,,\\tag{5}\n$$\n\nwhere z<sup>j</sup> is the embedding of v<sup>j</sup> and p<sup>c</sup> is the prototype that is closest to v<sup>i</sup> in the embedding space. These weights enhance the propagation of more confident labels (close to a prototype) compared to less confident ones. After l hops of label propagation, we apply a softmax to the output and obtain a probability distribution for each node.\n\nIn the third step, we compute the Shannon entropy of the label distribution of each pseudo-labeled node and remove the 10% with the highest entropy. This ensures that only highly confident [\\(Joshi et al.,](#page-10-11) [2009\\)](#page-10-11) pseudo-labels are used for the loss update, and nodes that either have a too heterogeneous neighborhood for the label propagation to perform well or have not been affected by the label propagation (because l was too low) are removed.\n\nAt the beginning of the training, the edge weights are noisy and small since the mean distance of the embeddings to a prototype is high. This inhibits the effect of label propagation and avoids propagating wrong labels of randomly assigned prototypes. As the training continues, the weights increase and facilitate the flow of the label propagation, leading to more confident labels and prototype assignments. See Appendix [B.2](#page-15-0) for an empirical analysis.\n\n#### 4.4 COMBINED LOSS FUNCTION\n\nThe final loss function is the combination of the proposed losses plus a regularization, defined by\n\n$$\n\\mathcal{L} = \\lambda \\mathcal{L}_S + \\mu \\mathcal{L}_U + \\nu \\mathcal{L}_P + \\kappa \\mathcal{R},\\tag{6}\n$$\n\nwhere λ, µ, and ν are the weights of the loss functions. R is a regularization term consisting of the sum of the entropy regularization of [Cao et al.](#page-9-0) [\\(2022\\)](#page-9-0) and the maximal negative pairwise distance between the prototypes of [Lu](#page-11-7) [et al.](#page-11-7) [\\(2022\\)](#page-11-7) with κ as the weight of the regularization term. The entropy regularization is defined as the Kullback-Leibler divergence of the output class distribution and uniform distribution, preventing the output on the new classes from being too flat, i. e., to collapse to one or only a few classes, and distributing the embeddings equally around the prototypes. The pairwise prototype distance ensures that the prototypes are biased towards uniform distribution in the embedding space. A formalization is given in Appendix [A.](#page-14-1)\n\n#### 4.5 COMPLEXITY ANALYSIS\n\nThe computational complexity depends on the backbone model f. Since we use a GCN, the complexity for f is given by O(|E|h <sup>L</sup>−1d) [\\(Kipf & Welling,](#page-11-0) [2017\\)](#page-11-0), where |E| is the number of edges of the graph, h the hidden dimension of the GCN, L the number of layers, and d the input feature dimension. For the supervised loss, we have to compute the distances between each sample and each prototype, which can be done in O(|V ||P|h). For the unsupervised loss, we have to evaluate f two times, resulting in the run-time complexity of GCN. For the pseudo-labels, we do a constant number of iterations of label propagation, which is in O(|E||Y |), then we do the same computation as for the supervised loss. We compute the pairwise distance of the prototypes for the regularization term, which scales with O(|Y | 2 ) = O(|V |) since it holds that |Y | <sup>2</sup> ≪ |V | for most applications. As these complexities are additive, the complexity of POWN stays linear w. r. t. the input dimensions |V |, |E|, and d.\n\n#### 4.6 SUMMARY\n\nWe formulated the problem of true open-world semi-supervised node classification and proposed POWN, an openworld learning method to classify seen as well as unseen classes. We outlined how to combine information in the embedding space with graph topology to assign pseudo-labels for potentially new classes. Next, we introduce the experimental apparatus to evaluate the performance of our method.\n\n### 5 EXPERIMENTAL APPARATUS\n\n#### 5.1 DATASETS\n\n<span id=\"page-5-0\"></span>To show that POWN can distinguish between the new classes, we ensure that the datasets have at least two new classes in the validation set and two new classes in the test set. We use the citation graphs Cora, CiteSeer [\\(Sen et al.,](#page-12-9) [2008\\)](#page-12-9), and OGB-arXiv [\\(Hu et al.,](#page-10-12) [2020a\\)](#page-10-12) as well as the co-purchase graphs Amazon-Photo (Photo) [\\(Shchur et al.,](#page-12-10) [2018\\)](#page-12-10), Amazon-Computers (Computers) [\\(Shchur et al.,](#page-12-10) [2018\\)](#page-12-10), and the social network Reddit2 [\\(Zeng et al.,](#page-12-11) [2020\\)](#page-12-11). Descriptive statistics of the datasets are shown in Table [1,](#page-5-0) i. e., the number of nodes |V |, the number of edges |E|, the dimension of the features d, the number of classes |Y |, and the homophily H. The homophily H, the tendency of connected nodes to share the same class, is measured by the class-insensitive homophily measure [\\(Lim et al.,](#page-11-10) [2021\\)](#page-11-10).\n\n|           | V        | E            | d      | Y  | H     |\n|-----------|----------|--------------|--------|----|-------|\n| Cora      | 2, 708   | 10, 556      | 1, 433 | 7  | 0.766 |\n| CiteSeer  | 3, 327   | 9, 104       | 3, 703 | 6  | 0.627 |\n| OGB-arXiv | 169, 343 | 1, 166, 243  | 128    | 40 | 0.421 |\n| Photo     | 7, 650   | 238, 162     | 745    | 8  | 0.772 |\n| Computers | 13, 752  | 491, 722     | 767    | 10 | 0.700 |\n| Reddit2   | 232, 965 | 23, 213, 838 | 602    | 41 | 0.691 |\n\nTable 1: Number of nodes |V |, edges |E|, features d, classes |Y |, and homophily measure H of the datasets.\n\n### <span id=\"page-6-1\"></span>5.2 PROCEDURE\n\nNode Split and Class Split As in the common node classification setting, we need to divide the nodes into train, validation, and test nodes, which are fixed across all experiments. We use the Planetoid split [\\(Yang et al.,](#page-12-12) [2016\\)](#page-12-12) for Cora and CiteSeer and we employ the split of [Shchur et al.](#page-12-10) [\\(2018\\)](#page-12-10) for Photo and Computers. For OGB-arXiv, we use the default split from [Hu et al.](#page-10-12) [\\(2020a\\)](#page-10-12) and for Reddit2 the split of [Zeng et al.](#page-12-11) [\\(2020\\)](#page-12-11). Details on the node split can be found in Appendix [C.1.](#page-16-0)\n\nAdditionally, we need to split the classes into train, validation, and test classes to evaluate true open-world learning. We split the classes into labeled and new classes, Y = Y<sup>l</sup> . ∪ Yu. From the labeled classes, we split a third subset Y<sup>v</sup> ⊂ Y<sup>l</sup> for validation. The number of new classes is defined by the new class ratio r. We partition the classes into three sets. We split r% of the classes, which are used as new classes for testing and another r% of the classes, which are used for validation. This results in the labeled node set V<sup>l</sup> , which consists of (1 − 2r)|Y | classes, for training, a test set Vu, which is equal to the test set of the dataset, containing all, i. e., labeled and new, classes, and a validation set containing r|Y | new classes and labeled classes. Following the splits over nodes and classes, we obtain unlabeled nodes in the train set that stem from validation classes and test classes. Test nodes come either from classes known during training or new classes.\n\nFolds and Repeats Since the selection of classes may have a high impact on the overall performance, we use a cross-validation method to minimize potential threats to the validity of the results. Given the ratio r of new classes, we split the classes into 1/r folds. We choose the new class ratio as r = 0.2. The experiment is repeated for each fold, where each fold once represents the new test classes and another fold represents the new validation classes. Note that the set of training, validation, and test nodes is fixed all the time, only the labeled versus unlabeled classes are varied across the folds.\n\nWe repeat the whole procedure over all folds 5 times for the smaller datasets, i. e., Cora, CiteSeer, Photo, and Computers, resulting in 25 overall runs. We repeat the experiment two times for the larger graphs OGB-arXiv and Reddit2, resulting in 10 overall runs each. Details on the dataset folds can be found in Appendix [C.2.](#page-16-1)\n\nBaselines We use POWN with a GCN [\\(Kipf & Welling,](#page-11-0) [2017\\)](#page-11-0) backbone for all experiments. Since we are the first to explore true open-world semi-supervised node classification, there is no direct method we can compare to. Therefore, we use and adapt existing methods to apply to our setting. As a representative of common semi-supervised node classification methods, we select a GCN with an output layer equal to the number of labeled and new classes. For unsupervised methods, we use DGI [\\(Velickovic et al.,](#page-12-3) [2019\\)](#page-12-3) embeddings, which we cluster by k-means and spectral clustering of the graph structure. For both methods, we provide the number of classes as the number of clusters. Furthermore, we extend the existing robust open-world learning method OpenWGL [\\(Wu et al.,](#page-12-1) [2021\\)](#page-12-1) to the true open-world semi-supervised node classification setting. It classifies the samples into the labeled classes and rejects all instances that do not belong to a labeled class. We apply k-means clustering on the embeddings of all rejected instances to obtain a separation of new classes. Details on the training procedure can be found in Appendix [D.](#page-17-0)\n\nHyperparameter Optimization To ensure comparability, all compared methods use the same GCN backbone. We use the GCN hyperparameter values of [Lell & Scherp](#page-11-11) [\\(2023\\)](#page-11-11) for Cora and CiteSeer. For Photo and Computer, we use the parameter values of [Shchur et al.](#page-12-10) [\\(2018\\)](#page-12-10), and for OGB-arXiv, we use the hyperparameters of [Hoffmann et al.](#page-10-1) [\\(2023\\)](#page-10-1). On Reddit2, we use the hyperparameter values of [Zeng et al.](#page-12-11) [\\(2020\\)](#page-12-11). For the OpenWGL baseline, we have tuned the hidden size with possible values {16, 32, 64} and a fixed learning rate of 0.001. For the DGI baseline, we employ standard feature shuffle as a corruption function and standard mean aggregation as graph-level readout. For POWN, we tune the scalars λ, µ, ν of the loss function, and κ by Bayesian optimization. We select the hyperparameters with the highest average validation accuracy on all classes computed over all folds and repeats. Further details on the procedure and final hyperparameter values can be found in Appendix [D.](#page-17-0)\n\n#### <span id=\"page-6-0\"></span>5.3 MEASURES\n\nWe use the evaluation measures of [Sun & Li](#page-12-2) [\\(2023\\)](#page-12-2); [Cao et al.](#page-9-0) [\\(2022\\)](#page-9-0). For the known classes, we compute the known-class accuracy as the ratio of correct predictions to all predictions. For the new classes and all classes, the methods cannot determine which of the new classes belong to which label. For this reason, we first solve an optimal assignment problem between the predicted and true labels using the Hungarian algorithm [\\(Kuhn,](#page-11-12) [2010\\)](#page-11-12). Based on this assignment, we compute the new class accuracy and the all class accuracy, respectively.\n\n# 6 RESULTS\n\n<span id=\"page-7-0\"></span>\n\n|                                    |           |           | Small datasets |           | Large datasets |           |  |\n|------------------------------------|-----------|-----------|----------------|-----------|----------------|-----------|--|\n| Method                             | Cora      | CiteSeer  | Photo          | Computers | OGB-arXiv      | Reddit2   |  |\n| All classes w/ Hungarian Algorithm |           |           |                |           |                |           |  |\n| GCN                                | 54.480.74 | 50.990.92 | 52.211.77      | 61.402.75 | 37.981.45      | 46.052.19 |  |\n| DGI + k-means                      | 38.852.22 | 38.672.57 | 25.870.94      | 22.270.80 | 28.161.45      | 17.100.96 |  |\n| Spectral clustering                | 29.610.97 | 32.260.67 | 27.110.35      | 29.990.54 | 24.240.71      | OOM       |  |\n| OpenWGL + k-means                  | 64.102.25 | 50.382.48 | 62.450.94      | 51.181.61 | 31.621.72      | 15.931.08 |  |\n| POWN (own)                         | 61.281.09 | 56.151.56 | 71.271.44      | 71.331.84 | 55.513.26      | 76.992.19 |  |\n| Known classes (Fully Supervised)   |           |           |                |           |                |           |  |\n| GCN                                | 95.190.28 | 85.931.43 | 89.001.67      | 91.993.65 | 46.374.04      | 55.162.34 |  |\n| OpenWGL + k-means                  | 61.743.69 | 38.180.99 | 60.061.35      | 36.934.06 | 27.042.58      | 7.481.50  |  |\n| POWN (own)                         | 87.951.99 | 75.452.31 | 91.050.97      | 82.452.91 | 71.242.60      | 92.522.34 |  |\n| New classes w/ Hungarian Algorithm |           |           |                |           |                |           |  |\n| GCN                                | 62.161.82 | 57.751.60 | 65.241.53      | 69.371.46 | 42.824.10      | 55.012.11 |  |\n| DGI + k-means                      | 41.972.21 | 41.172.21 | 32.973.81      | 40.405.91 | 49.064.10      | 19.621.42 |  |\n| Spectral clustering                | 39.221.47 | 37.700.51 | 36.680.86      | 56.512.13 | 37.813.33      | OOM       |  |\n| OpenWGL + k-means                  | 68.883.22 | 58.882.22 | 66.042.48      | 55.994.36 | 28.161.29      | 32.381.36 |  |\n| POWN (own)                         | 63.291.59 | 57.292.22 | 70.762.74      | 65.612.95 | 56.114.35      | 64.422.11 |  |\n\nTable 2: Mean accuracy with standard error across up to 5 class folds and 10 runs for each of the three cases: All classes (Top), Known classes (Center), New classes (Bottom). The best score per measure is marked in bold. Unsupervised methods DGI+k-means and Spectral clustering are not applicable for the supervised setting with known classes.\n\nTable [2](#page-7-0) shows the results of the true open-world node classification setting. We report all-class accuracy, knownclass accuracy, and new-class accuracy along with their standard error over all class folds and runs. We observe that POWN outperforms all baselines in terms of all-class accuracy on all datasets except Cora, where OpenWGL is the best-performing model. In general, OpenWGL is the second-best model in terms of all-class accuracy. In terms of known class accuracy, POWN loses up to 8% on the small datasets compared to the supervised GCN, but improves the known-class accuracy on the large datasets. We do not report known-class accuracy for the unsupervised methods since they cannot learn the mapping between the cluster ids and labels of the dataset, i. e., their known class accuracy is simply random. The best new-class accuracy depends on the dataset and is either achieved by GCN, OpenWGL, or POWN. However, POWN outperforms all methods on the large datasets by a wide margin of at least 18% in the accuracy on all classes. Spectral clustering had an out-of-memory on Reddit2 on a server with 2 TB of RAM.\n\n## 7 DISCUSSION\n\nTrue Open-World Semi-Supervised Node Classification Our results show that POWN effectively approaches the true open-world semi-supervised learning problem, i. e., learns representation for nodes of known and new classes at the same time. It improves the accuracy over all classes by up to 20% on the small and up to 30% on the large datasets over the best baseline. We observe that the margin between POWN and the baselines increases on large datasets, which is the exact opposite trend of the performance of OpenWGL.\n\nThe positive effect of the dataset size on POWN's performance is attributed to two reasons. First, POWN's contrastive losses benefit from the larger amount of data. Second, in contrast to models relying mainly on labels, like GCN and OpenWGL, POWN learns from all parts of the data, even when there are no labels available.\n\nPOWN loses some accuracy on the labeled classes compared to the fully supervised GCN on the small datasets. However, it is always better in terms of all-class accuracy since it also learns to separate known from new classes and uses unlabeled nodes more explicitly in its loss functions. Nevertheless, GCN performs comparably well in capturing the structure of the new classes, especially on Computers. The homophily of the datasets and the message passing of GCN helps the model to group embeddings of the same class together, even without access to the labels. Since our evaluation procedure takes the best mapping of the output labels to the classes using the Hungarian algorithm, GCN is a strong baseline in all measures.\n\nThe unsupervised baselines consistently have the lowest scores over all datasets and measures, showing that utilizing label information is crucial for learning meaningful representations. In particular, the known-class accuracy becomes meaningless without supervised information. Note that the all-class accuracy is not just the average of the knownclass accuracy and the new-class accuracy. As defined in Section [5.3,](#page-6-0) the all-class accuracy and new-classes accuracy are calculated based on the assignments obtained from the Hungarian algorithm to receive a mapping between the labels and clusters. For this reason, the known-class accuracy can also be lower than the all-class accuracy where the Hungarian algorithm improves the accuracy by mapping the output labels to the true labels.\n\nWe have used homophilic datasets from different domains and of different sizes in our experiments. We expect that our results do generalize to other homophilic datasets.\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_3.jpeg)\n\nFigure 2: Test accuracy on all classes for variations of the hyperparameters λ, µ, ν, and κ (figures from left to right). The orange dot marks the hyperparameters with the highest validation accuracy found by Bayesian search.\n\nHyperparameter Sensitivity Analysis We conducted a hyperparameter sensitivity analysis to examine how POWN behaves for deviations from our hyperparameter values. We use the small dataset Photo and the large dataset OGBarXiv. We change POWN's loss weights λ, µ, and ν, and the entropy weight κ, while we keep all other values found by our hyperparameter optimization. The results are presented in Figure [2.](#page-8-0) We observe that the model performance is quite robust for variations of the hyperparameters, except for the supervised loss weight λ that has to be sufficiently high. For OGB-arXiv, running Bayesian optimization for more iterations may have further improved the hyperparameters. Additional analysis of the temperature parameters can be found in Appendix [B.4.](#page-16-2)\n\n<span id=\"page-8-1\"></span>Ablation Study We run an ablation study to verify that all parts of the loss contribute to the performance. We repeat the main experiment but remove one of the loss functions for each setting. The results for Photo and OGB-arXiv are presented in Table [3,](#page-8-1) where each line leaves out one of the loss functions.\n\n|                                     |                                                  | Photo                                    |                                                  |                                                  | OGB-arXiv                                |                                                  |\n|-------------------------------------|--------------------------------------------------|------------------------------------------|--------------------------------------------------|--------------------------------------------------|------------------------------------------|--------------------------------------------------|\n|                                     | All                                              | Known                                    | New                                              | All                                              | Known                                    | New                                              |\n| w/o LS<br>w/o LU<br>w/o LP<br>w/o R | 33.950.71<br>68.511.68<br>69.501.99<br>69.381.49 | —<br>92.760.66<br>90.431.78<br>89.970.93 | 72.540.40<br>70.181.89<br>70.942.90<br>65.622.71 | 28.162.18<br>52.912.49<br>55.311.56<br>54.841.38 | —<br>69.093.39<br>72.712.01<br>66.701.89 | 49.065.65<br>50.802.73<br>55.463.77<br>53.272.89 |\n| POWN                                | 71.271.44                                        | 91.050.97                                | 70.762.74                                        | 55.513.26                                        | 71.242.60                                | 56.114.35                                        |\n\nTable 3: Ablation study: Accuracy on all, the known, and the new classes on the loss functions of POWN with the respective standard error.\n\nThe combination of all loss functions gives the best result with respect to all-class accuracy. Leaving out a loss can improve the performance on some subset of the classes, e. g., removing the supervised loss improves the performance on the new classes. However, the performance on the known classes collapses, since there is no mapping of label indices to model output indices anymore. Furthermore, the regularization term R contributes most to the new classes by preventing the embeddings to collapse around one or only a few prototypes. Assuming the regularization term distributes the node representations among the prototypes, the unsupervised loss ensures that similar node representations are close to the same prototype. The pseudo-label loss contributes to the overall performance by denoising the edges, i. e., assigning high weights to homophilic and low weights to heterophilic edges. It mainly improves the performance on all classes, where the edge denoising effect and the homophily of the graph lead to a better separation of known from new classes. It also improves the accuracy of the known classes, although it does not affect these prototypes. This is due to the shared GNN encoder.\n\nt-SNE Embeddings For a qualitative assessment of the embeddings, we compare the t-SNE-reduced embeddings [\\(van der Maaten & Hinton,](#page-12-13) [2008\\)](#page-12-13) of GCN and POWN on the unlabeled part V<sup>u</sup> of the Photo dataset. The known classes are all shown in gray, while the new classes have different colors. The embeddings are presented in Figure [3.](#page-9-2) We see that POWN produces denser clusters on the known classes and new classes.\n\n<span id=\"page-9-2\"></span>![](_page_9_Figure_2.jpeg)\n\nFigure 3: Left plot: t-SNE embeddings of GCN, right: POWN on the Photo dataset. Known classes are colored in gray, each new class in a different color.\n\nEstimating the # of Classes For the main setting, we had assumed that the number of classes |Y | is known. To investigate the effect of this assumption, we ran the experiments with an unsupervised estimator for the number of classes [Han et al.](#page-10-10) [\\(2019\\)](#page-10-10), see details in Appendix [B.1.](#page-14-0) The results show that the difference in performance on for all-classes is below 1%.\n\n### 8 LIMITATIONS\n\nOur method relies on the homophily assumption. Although many real-world graphs are homophilic, like the datasets we use in the experiments, the model will perform worse on heterophilic datasets. POWN is inherently transductive due to the learning of prototypes from unlabeled data. However, for every application, the test data has to be available at some point. Thus, one can re-train POWN as soon as the data is available. While POWN achieves remarkable performance on all classes combined, it falls behind the accuracy of a fully supervised GCN on the known classes on the small datasets. The performance of the classification task highly depends on the specific class fold, since not all classes are equally difficult to distinguish from each other, leading to a high expected standard error w.r.t. the chosen classes for training, validation, and test. Therefore, we applied a cross-validation approach and averaged our results over three class folds on Cora and CiteSeer, four on Photo, and five on Computers, OGB-arXiv, and Reddit2. Additionally, we averaged the results over multiple repeats, i. e., random seeds, per class fold.\n\n### 9 CONCLUSION\n\nWe address the problem of true open-world semi-supervised node classification. We adapted existing methods from the literature to be suitable for the problem and show that they could not sufficiently solve it. Therefore, we propose POWN, an end-to-end prototype-based open-world learning model that can effectively tackle the true open-world learning problem. Our experiments show that POWN outperforms all baselines, especially on large graphs, and is robust to the hyperparameter selection. Future work may investigate the effect of providing few, e. g., one to five, examples for unlabeled classes to measure the difference to a labeled setting. Furthermore, POWN can be extended to temporal graphs to see how the performance evolves if multiple iterations of POWN are applied.\n\n### REFERENCES\n\n- <span id=\"page-9-3\"></span>David Arthur and Sergei Vassilvitskii. k-means++: The Advantages of Careful Seeding. In Nikhil Bansal, Kirk Pruhs, and Clifford Stein (eds.), *Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, New Orleans, Louisiana, USA, January 7-9, 2007*, pp. 1027–1035. SIAM, 2007. URL [http://dl.](http://dl.acm.org/citation.cfm?id=1283383.1283494) [acm.org/citation.cfm?id=1283383.1283494](http://dl.acm.org/citation.cfm?id=1283383.1283494).\n- <span id=\"page-9-0\"></span>Kaidi Cao, Maria Brbic, and Jure Leskovec. Open-World Semi-Supervised Learning. In *The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022*. OpenReview.net, 2022. URL <https://openreview.net/forum?id=O-r8LOR-CCA>.\n- <span id=\"page-9-1\"></span>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In *Proceedings of the 37th International Conference on Machine Learning,*\n\n*ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine Learning Research*, pp. 1597– 1607. PMLR, 2020. URL <http://proceedings.mlr.press/v119/chen20j.html>.\n\n- <span id=\"page-10-6\"></span>Zhiyuan Chen and Bing Liu. *Lifelong Machine Learning*. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2016. doi: 10.2200/S00737ED1V01Y201610AIM033. URL [https:](https://doi.org/10.2200/S00737ED1V01Y201610AIM033) [//doi.org/10.2200/S00737ED1V01Y201610AIM033](https://doi.org/10.2200/S00737ED1V01Y201610AIM033).\n- <span id=\"page-10-9\"></span>Kaize Ding, Jianling Wang, Jundong Li, Kai Shu, Chenghao Liu, and Huan Liu. Graph Prototypical Networks for Few-shot Learning on Attributed Networks. In *CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020*, pp. 295–304. ACM, 2020. doi: 10.1145/ 3340531.3411922. URL <https://doi.org/10.1145/3340531.3411922>.\n- <span id=\"page-10-7\"></span>Sepideh Esmaeilpour, Lei Shu, and Bing Liu. Open Set Recognition Via Augmentation-Based Similarity Learning. In Sarath Chandar, Razvan Pascanu, and Doina Precup (eds.), *Proceedings of The 1st Conference on Lifelong Learning Agents*, volume 199 of *Proceedings of Machine Learning Research*, pp. 875–885. PMLR, 22–24 Aug 2022. URL <https://proceedings.mlr.press/v199/esmaeilpour22a.html>.\n- <span id=\"page-10-8\"></span>Lukas Galke, Benedikt Franke, Tobias Zielke, and Ansgar Scherp. Lifelong Learning of Graph Neural Networks for Open-World Node Classification. In *International Joint Conference on Neural Networks, IJCNN 2021, Shenzhen, China, July 18-22, 2021*, pp. 1–8. IEEE, 2021. doi: 10.1109/IJCNN52387.2021.9533412. URL [https://doi.](https://doi.org/10.1109/IJCNN52387.2021.9533412) [org/10.1109/IJCNN52387.2021.9533412](https://doi.org/10.1109/IJCNN52387.2021.9533412).\n- <span id=\"page-10-2\"></span>Lukas Galke, Iacopo Vagliano, Benedikt Franke, Tobias Zielke, Marcel Hoffmann, and Ansgar Scherp. Lifelong learning on evolving graphs under the constraints of imbalanced classes and new classes. *Neural Networks*, 164: 156–176, 2023. doi: 10.1016/J.NEUNET.2023.04.022. URL [https://doi.org/10.1016/j.neunet.](https://doi.org/10.1016/j.neunet.2023.04.022) [2023.04.022](https://doi.org/10.1016/j.neunet.2023.04.022).\n- <span id=\"page-10-4\"></span>William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs. In *Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA*, pp. 1024–1034, 2017. URL [https://proceedings.](https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html) [neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html).\n- <span id=\"page-10-10\"></span>Kai Han, Andrea Vedaldi, and Andrew Zisserman. Learning to Discover Novel Visual Categories via Deep Transfer Clustering. In *2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019*, pp. 8400–8408. IEEE, 2019. doi: 10.1109/ICCV.2019.00849. URL [https:](https://doi.org/10.1109/ICCV.2019.00849) [//doi.org/10.1109/ICCV.2019.00849](https://doi.org/10.1109/ICCV.2019.00849).\n- <span id=\"page-10-1\"></span>Marcel Hoffmann, Lukas Galke, and Ansgar Scherp. Open-World Lifelong Graph Learning. In *International Joint Conference on Neural Networks, IJCNN 2023, Gold Coast, Australia, June 18-23, 2023*, pp. 1–9. IEEE, 2023. doi: 10.1109/IJCNN54540.2023.10191071. URL [https://doi.org/10.1109/IJCNN54540.](https://doi.org/10.1109/IJCNN54540.2023.10191071) [2023.10191071](https://doi.org/10.1109/IJCNN54540.2023.10191071).\n- <span id=\"page-10-12\"></span>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In *NeurIPS*, 2020a.\n- <span id=\"page-10-0\"></span>Yang Hu, Haoxuan You, Zhecan Wang, Zhicheng Wang, Erjin Zhou, and Yue Gao. Graph-MLP: Node Classification without Message Passing in Graph. *CoRR*, abs/2106.04051, 2021. URL [https://arxiv.org/abs/2106.](https://arxiv.org/abs/2106.04051) [04051](https://arxiv.org/abs/2106.04051).\n- <span id=\"page-10-5\"></span>Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. GPT-GNN: Generative Pre-Training of Graph Neural Networks. In *KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020*, pp. 1857–1867. ACM, 2020b. doi: 10.1145/3394486. 3403237. URL <https://doi.org/10.1145/3394486.3403237>.\n- <span id=\"page-10-11\"></span>Ajay J. Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-Class Active Learning for Image Classification. In *2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA*, pp. 2372–2379. IEEE Computer Society, 2009. doi: 10.1109/CVPR.2009.5206627. URL <https://doi.org/10.1109/CVPR.2009.5206627>.\n- <span id=\"page-10-3\"></span>Wei Ju, Yifang Qin, Siyu Yi, Zhengyang Mao, Kangjie Zheng, Luchen Liu, Xiao Luo, and Ming Zhang. Zero-shot Node Classification with Graph Contrastive Embedding Network. *Transactions on Machine Learning Research*, 2023. ISSN 2835-8856. URL <https://openreview.net/forum?id=8wGXnjRLSy>.\n\n<span id=\"page-11-14\"></span>Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In *ICLR*, 2015.\n\n- <span id=\"page-11-3\"></span>Thomas N. Kipf and Max Welling. Variational Graph Auto-Encoders. *CoRR*, abs/1611.07308, 2016. URL [http:](http://arxiv.org/abs/1611.07308) [//arxiv.org/abs/1611.07308](http://arxiv.org/abs/1611.07308).\n- <span id=\"page-11-0\"></span>Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Networks. In *5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net, 2017. URL <https://openreview.net/forum?id=SJU4ayYgl>.\n- <span id=\"page-11-12\"></span>Harold W. Kuhn. The Hungarian Method for the Assignment Problem. In *50 Years of Integer Programming 1958-2008 - From the Early Years to the State-of-the-Art*, pp. 29–47. Springer, 2010. URL [https://doi.org/10.1007/](https://doi.org/10.1007/978-3-540-68279-0_2) [978-3-540-68279-0\\\\_2](https://doi.org/10.1007/978-3-540-68279-0_2).\n- <span id=\"page-11-11\"></span>Nicolas Lell and Ansgar Scherp. The Split Matters: Flat Minima Methods for Improving the Performance of GNNs. In *Machine Learning and Knowledge Extraction - 7th IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2023, Benevento, Italy, August 29 - September 1, 2023, Proceedings*, volume 14065 of *Lecture Notes in Computer Science*, pp. 200–226. Springer, 2023. doi: 10.1007/978-3-031-40837-3\\ 13. URL [https://doi.org/10.1007/978-3-031-40837-3\\\\_13](https://doi.org/10.1007/978-3-031-40837-3_13).\n- <span id=\"page-11-8\"></span>Junnan Li, Pan Zhou, Caiming Xiong, and Steven C. H. Hoi. Prototypical Contrastive Learning of Unsupervised Representations. In *9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021*. OpenReview.net, 2021. URL <https://openreview.net/forum?id=KmykpuSrjcq>.\n- <span id=\"page-11-10\"></span>Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser-Nam Lim. Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods. In *Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual*, pp. 20887–20902, 2021. URL [https://proceedings.](https://proceedings.neurips.cc/paper/2021/hash/ae816a80e4c1c56caa2eb4e1819cbb2f-Abstract.html) [neurips.cc/paper/2021/hash/ae816a80e4c1c56caa2eb4e1819cbb2f-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/ae816a80e4c1c56caa2eb4e1819cbb2f-Abstract.html).\n- <span id=\"page-11-2\"></span>Yixin Liu, Shirui Pan, Ming Jin, Chuan Zhou, Feng Xia, and Philip S. Yu. Graph Self-Supervised Learning: A Survey. *CoRR*, abs/2103.00111, 2021. URL <https://arxiv.org/abs/2103.00111>.\n- <span id=\"page-11-5\"></span>Ziwen Liu, Chenguang Wang, Congying Han, and Tiande Guo. Learning Graph Representation by Aggregating Subgraphs via Mutual Information Maximization. *Neurocomputing*, 548:126392, 2023. URL [https://doi.](https://doi.org/10.1016/j.neucom.2023.126392) [org/10.1016/j.neucom.2023.126392](https://doi.org/10.1016/j.neucom.2023.126392).\n- <span id=\"page-11-7\"></span>Bin Lu, Xiaoying Gan, Lina Yang, Weinan Zhang, Luoyi Fu, and Xinbing Wang. Geometer: Graph Few-Shot Class-Incremental Learning via Prototype Representation. In *KDD '22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022*, pp. 1152–1161. ACM, 2022. doi: 10.1145/3534678.3539280. URL <https://doi.org/10.1145/3534678.3539280>.\n- <span id=\"page-11-9\"></span>David Macedo, Tsang Ing Ren, Cleber Zanchettin, Adriano L. I. Oliveira, and Teresa Bernarda Ludermir. Entropic ˆ Out-of-Distribution Detection: Seamless Detection of Unknown Examples. *IEEE Trans. Neural Networks Learn. Syst.*, 33(6):2350–2364, 2022. doi: 10.1109/TNNLS.2021.3112897. URL [https://doi.org/10.1109/](https://doi.org/10.1109/TNNLS.2021.3112897) [TNNLS.2021.3112897](https://doi.org/10.1109/TNNLS.2021.3112897).\n- <span id=\"page-11-13\"></span>J. MacQueen. Some Methods for Classification and Analysis of Multivariate Observations. 1967. URL [https:](https://api.semanticscholar.org/CorpusID:6278891) [//api.semanticscholar.org/CorpusID:6278891](https://api.semanticscholar.org/CorpusID:6278891).\n- <span id=\"page-11-4\"></span>Costas Mavromatis and George Karypis. Graph InfoClust: Maximizing Coarse-Grain Mutual Information in Graphs. In *Advances in Knowledge Discovery and Data Mining - 25th Pacific-Asia Conference, PAKDD 2021, Virtual Event, May 11-14, 2021, Proceedings, Part I*, volume 12712 of *Lecture Notes in Computer Science*, pp. 541–553. Springer, 2021. doi: 10.1007/978-3-030-75762-5\\ 43. URL [https://doi.org/10.1007/978-3-030-75762-5\\\\_](https://doi.org/10.1007/978-3-030-75762-5_43) [43](https://doi.org/10.1007/978-3-030-75762-5_43).\n- <span id=\"page-11-1\"></span>Andrew Ng, Michael Jordan, and Yair Weiss. On Spectral Clustering: Analysis and an Algorithm. In T. Dietterich, S. Becker, and Z. Ghahramani (eds.), *Advances in Neural Information Processing Systems*, volume 14. MIT Press, 2001. URL [https://proceedings.neurips.cc/paper\\\\_files/paper/2001/file/](https://proceedings.neurips.cc/paper_files/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf) [801272ee79cfde7fa5960571fee36b9b-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf).\n- <span id=\"page-11-6\"></span>Jitendra Parmar, Satyendra Singh Chouhan, Vaskar Raychoudhury, and Santosh Singh Rathore. Open-World Machine Learning: Applications, Challenges, and Opportunities. *ACM Comput. Surv.*, 55(10):205:1–205:37, 2023. doi: 10.1145/3561381. URL <https://doi.org/10.1145/3561381>.\n- <span id=\"page-12-5\"></span>Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou Huang. Graph Representation Learning via Graphical Mutual Information Maximization. In *WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020*, pp. 259–270. ACM / IW3C2, 2020. doi: 10.1145/3366423.3380112. URL <https://doi.org/10.1145/3366423.3380112>.\n- <span id=\"page-12-6\"></span>Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. DropEdge: Towards Deep Graph Convolutional Networks on Node Classification. In *8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020. URL [https://openreview.net/forum?id=](https://openreview.net/forum?id=Hkx1qkrKPr) [Hkx1qkrKPr](https://openreview.net/forum?id=Hkx1qkrKPr).\n- <span id=\"page-12-14\"></span>Peter J. Rousseeuw. Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis. *Journal of Computational and Applied Mathematics*, 20:53–65, 1987. ISSN 0377-0427. doi: https://doi.org/ 10.1016/0377-0427(87)90125-7. URL [https://www.sciencedirect.com/science/article/pii/](https://www.sciencedirect.com/science/article/pii/0377042787901257) [0377042787901257](https://www.sciencedirect.com/science/article/pii/0377042787901257).\n- <span id=\"page-12-9\"></span>Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad. Collective Classification in Network Data. *AI Mag.*, 29(3):93–106, 2008.\n- <span id=\"page-12-10\"></span>Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls of Graph Neural ¨ Network Evaluation. *CoRR*, abs/1811.05868, 2018. URL <http://arxiv.org/abs/1811.05868>.\n- <span id=\"page-12-7\"></span>Lei Shu, Hu Xu, and Bing Liu. DOC: Deep Open Classification of Text Documents. In *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017*, pp. 2911–2916. Association for Computational Linguistics, 2017. doi: 10.18653/v1/d17-1314. URL <https://doi.org/10.18653/v1/d17-1314>.\n- <span id=\"page-12-2\"></span>Yiyou Sun and Yixuan Li. OpenCon: Open-World Contrastive Learning. *Trans. Mach. Learn. Res.*, 2023, 2023. URL <https://openreview.net/forum?id=2wWJxtpFer>.\n- <span id=\"page-12-13\"></span>Laurens van der Maaten and Geoffrey Hinton. Visualizing Data using t-SNE. *Journal of Machine Learning Research*, 9(86):2579–2605, 2008. URL <http://jmlr.org/papers/v9/vandermaaten08a.html>.\n- <span id=\"page-12-0\"></span>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph ` Attention Networks. In *6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*. OpenReview.net, 2018. URL [https:](https://openreview.net/forum?id=rJXMpikCZ) [//openreview.net/forum?id=rJXMpikCZ](https://openreview.net/forum?id=rJXMpikCZ).\n- <span id=\"page-12-3\"></span>Petar Velickovic, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R. Devon Hjelm. Deep Graph ` Infomax. In *7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net, 2019. URL <https://openreview.net/forum?id=rklz9iAcKQ>.\n- <span id=\"page-12-1\"></span>Man Wu, Shirui Pan, and Xingquan Zhu. OpenWGL: Open-World Graph Learning for Unseen Class Node Classification. *Knowl. Inf. Syst.*, 63(9):2405–2430, 2021. doi: 10.1007/s10115-021-01594-0. URL [https:](https://doi.org/10.1007/s10115-021-01594-0) [//doi.org/10.1007/s10115-021-01594-0](https://doi.org/10.1007/s10115-021-01594-0).\n- <span id=\"page-12-8\"></span>Hui Xu, Liyao Xiang, Junjie Ou, yuting weng, Xinbing Wang, and Chenghu Zhou. Open-World Graph Active Learning for Node Classification. *ACM Trans. Knowl. Discov. Data*, jul 2023. ISSN 1556-4681. doi: 10.1145/3607144. URL <https://doi.org/10.1145/3607144>. Just Accepted.\n- <span id=\"page-12-12\"></span>Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting Semi-Supervised Learning with Graph Embeddings. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), *Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016*, volume 48 of *JMLR Workshop and Conference Proceedings*, pp. 40–48. JMLR.org, 2016. URL [http://proceedings.mlr.press/v48/](http://proceedings.mlr.press/v48/yanga16.html) [yanga16.html](http://proceedings.mlr.press/v48/yanga16.html).\n- <span id=\"page-12-4\"></span>Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When Does Self-Supervision Help Graph Convolutional Networks? In *Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine Learning Research*, pp. 10871–10880. PMLR, 2020. URL <http://proceedings.mlr.press/v119/you20a.html>.\n- <span id=\"page-12-11\"></span>Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. GraphSAINT: Graph Sampling Based Inductive Learning Method. In *8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020. URL [https://openreview.net/](https://openreview.net/forum?id=BJe8pkHFwS) [forum?id=BJe8pkHFwS](https://openreview.net/forum?id=BJe8pkHFwS).\n- <span id=\"page-13-1\"></span>Fan Zhou, Chengtai Cao, Kunpeng Zhang, Goce Trajcevski, Ting Zhong, and Ji Geng. Meta-GNN: On Few-shot Node Classification in Graph Meta-Learning. In *Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019*, pp. 2357–2360. ACM, 2019. doi: 10.1145/3357384.3358106. URL <https://doi.org/10.1145/3357384.3358106>.\n- <span id=\"page-13-0\"></span>Xiaojin Zhu and Zoubin Ghahramani. Learning from Labeled and Unlabeled Data with Label Propagation. *Pro-Quest Number: INFORMATION TO ALL USERS*, 2002. URL [https://api.semanticscholar.org/](https://api.semanticscholar.org/CorpusID:15008961) [CorpusID:15008961](https://api.semanticscholar.org/CorpusID:15008961).\n\n### <span id=\"page-14-1\"></span>A REGULARIZATION TERM\n\nThe regularization term R consists of two parts based on [Cao et al.](#page-9-0) [\\(2022\\)](#page-9-0) and [Lu et al.](#page-11-7) [\\(2022\\)](#page-11-7). It is defined by\n\n$$\n\\mathcal{R} = KL\\left(\\frac{1}{|V|} \\sum_{j \\in |V|} \\sum_{i \\in P} \\mathbf{p_i}(p_i|v_j), \\mathbf{U}\\right) + \\sum_{i \\in P} \\min_{j \\in P, i \\neq j} \\exp{-d(p_i, p_j)},\\tag{7}\n$$\n\nwhere U is the uniform distribution and d the euclidean distance. The first part computes the Kullback-Leibler divergence of the output class distribution and a uniform distribution [\\(Cao et al.,](#page-9-0) [2022\\)](#page-9-0). It prevents the collapse of the output distribution to collapse to only one or few prototypes in early stages of the training by distributing the datapoints over all prototypes. In cases where information of the prior class distribution is available, one can substitute U by the known prior P. The second part increases the distance between the pairwise closest prototypes to separate the prototypes more sharply. It especially pushes the prototypes of unlabeled classes away from the one of labeled classes such that they get some pseudo-labels assigned during training.\n\n### B ADDITIONAL EXPERIMENTS\n\n#### <span id=\"page-14-0\"></span>B.1 NUMBER OF CLASS ESTIMATION\n\nIn many real-world settings, the assumption that the number of classes is known beforehand may not be true. For this reason, we perform an additional experiment where we estimate the number of classes and use the estimated number as the number of prototypes.\n\nTo estimate the number of classes of the unlabeled node set Vu, we follow the procedure of [Han et al.](#page-10-10) [\\(2019\\)](#page-10-10). The training set is divided into a subset of classes for training Y<sup>t</sup> and probe classes Yp. The model is trained on the subgraph V<sup>t</sup> of nodes containing only classes from Yt. The probing node set V<sup>p</sup> is further divided into an anchor set V<sup>a</sup> and a validation set Vv. We run a constrained k-means [\\(MacQueen,](#page-11-13) [1967\\)](#page-11-13) on Vp∪Vu, with the constrain that nodes of V<sup>a</sup> are forced to match their ground truth label, while considering nodes of D<sup>v</sup> as unlabeled. This procedure is repeated for k ∈ {0, . . . Kmax} values of k on V<sup>p</sup> ∪ Vu. The final number of categories is chosen based on two quality measures. First, we measure the labeled clustering quality on the validation probe set V<sup>v</sup> by average clustering accuracy [\\(Han](#page-10-10) [et al.,](#page-10-10) [2019\\)](#page-10-10). Second, we measure the cluster quality of the unlabeled data V<sup>u</sup> by silhouette score [\\(Rousseeuw,](#page-12-14) [1987\\)](#page-12-14). Afterwards, we select ˆk = 1 2 (k ∗ <sup>v</sup> +k ∗ u ), where k ∗ v is the k that maximized the average clustering accuracy on V<sup>v</sup> and k ∗ u is the k that maximized the silhouette score on Vu. To finally determine the number of classes in Vu, we run k-means one more time with k = ˆk and remove all clusters that contain less than 5% of the data. The number of remaining clusters is set to the number of classes in Vu.\n\n<span id=\"page-14-2\"></span>We conduct experiments where we estimate the number of classes and use it as the number of prototypes for POWN on Photo and OGB-arXiv. The results are presented in Table [4.](#page-14-2)\n\n| Dataset   | Estimate per fold    | Y  | Error rate in percent          |\n|-----------|----------------------|----|--------------------------------|\n| Photo     | {11, 11, 12, 12}     | 8  | {37.5, 37.5, 50.0, 50.0}       |\n| OGB-Arxiv | {51, 51, 51, 51, 51} | 40 | {27.5, 27.5, 27.5, 27.5, 27.5} |\n\nTable 4: The estimated numbers of new classes, the true number of classes, and the relative error for each fold of the datasets.\n\nWe repeat our main experiment with the estimated numbers of new classes for each fold instead of the true number to see how sensitive the model performance is towards this parameter. The results for these experiments are presented in Table [5.](#page-15-1)\n\nThe results with the estimated classes show a small drop of less than 4% on all performance measures, which shows that POWN can also be applied in settings where no prior information on the number of classes is available.\n\n<span id=\"page-15-1\"></span>\n\n|                                                                      |                | Photo          |                |                | OGB-arXiv      |                |\n|----------------------------------------------------------------------|----------------|----------------|----------------|----------------|----------------|----------------|\n|                                                                      | All            | Known          | New            | All            | Known          | New            |\n| POWN + known number of classes<br>POWN + estimated number of classes | 71.27<br>70.05 | 91.05<br>88.43 | 70.76<br>70.76 | 55.51<br>55.65 | 71.24<br>67.80 | 56.11<br>53.56 |\n\nTable 5: The results for POWN with an estimated number of classes.\n\n<span id=\"page-15-2\"></span>![](_page_15_Figure_3.jpeg)\n\nFigure 4: The distribution of edge weights before and after training, separated by homophilic and heterophilic edges.\n\n#### <span id=\"page-15-0\"></span>B.2 ANALYSIS OF EDGE WEIGHTS IN LABEL PROPAGATION\n\nTo analyze the dynamics of our weights, we compare the distribution of the edge weights of Equation [5.](#page-4-0) We trained POWN as described in Section [5.2](#page-6-1) on the first class fold of the Photos dataset. The edge weights before and after training separated by homophilic and heterophilic edges are shown in Figure [4.](#page-15-2)\n\nThe figure shows that the mean edge weights increase and the standard deviation decreases. This shows that the embeddings are closer to their prototypes. Furthermore, the difference of the means before training is 0.0113, and after training it is 0.0299, which is an increase of 164, 6%. Therefore, the edge weights are more likely to inhibit the label flow between heterophilic edges and facilitate the label flow of homophilic edges.\n\n#### B.3 PROTOTYPE AS MEAN\n\nWe conducted experiments, where we treated the prototype as the weighted mean of its class embedding [\\(Ding et al.,](#page-10-9) [2020;](#page-10-9) [Lu et al.,](#page-11-7) [2022;](#page-11-7) [Xu et al.,](#page-12-8) [2023\\)](#page-12-8). Instead of being a trainable parameter, the prototype is the weighted mean of the embeddings with ground truth label or pseudo-label y:\n\n$$\np_c = \\sum_{z_i \\in V_p, y_i = y} \\frac{\\exp{(\\alpha_y)}}{\\sum_{z_k \\in V_p, y_k = y} \\exp{(\\alpha_k)}} z_i ,\n$$\n\nwhere α<sup>j</sup> is the page-rank value of node v<sup>j</sup> to give a higher weight for nodes in central positions of the graph, and z<sup>i</sup> is the embedding of v<sup>i</sup> . Furthermore, the prototype loss of POWN is substituted by a labeled contrastive loss that pushes embeddings with the same label together:\n\n$$\n\\mathcal{L}_{S/P} = -\\frac{1}{|\\mathcal{P}(v)|} \\sum_{z^+ \\in \\mathcal{P}(v)} \\log \\left( \\frac{\\exp(z^T z^+ / \\tau)}{\\sum_{z^- \\in V_l} \\exp(z^T z^- / \\tau)} \\right),\n$$\n\nwhere v is a node, z = f(v) the embedding of v by a model f, P(v) the set of positive samples for v, and τ some smoothness parameter. In this case, the positive set is the set of nodes with the same label, which can be either given as part of the gold standard or from a pseudo label.\n\nIn pre-experiments, we analyzed the difference between using the trainable prototypes POWN versus weighted means with a contrastive loss. The results can be found in Table [6.](#page-16-3) The experiments show that this prototype representation performs worse for our problem. We did not follow up on this direction since the mean variant performed worse than simply using a GCN. However, we acknowledge that there are further optimizations possible such as potentially a better choice of hyperparameter values.\n\n<span id=\"page-16-3\"></span>\n\n| Model     | Cora                  | CiteSeer              | Photo                 | Computers             |\n|-----------|-----------------------|-----------------------|-----------------------|-----------------------|\n| POWN-mean | 43.01 / 10.82 / 51.99 | 35.30 / 15.88 / 38.90 | 46.12 / 10.64 / 59.69 | 39.34 / 8.13 / 53.61  |\n| POWN      | 61.28 / 87.95 / 63.29 | 56.15 / 75.45 / 57.29 | 71.27 / 91.05 / 70.76 | 71.33 / 82.45 / 65.61 |\n\nTable 6: Comparison of POWN as an end-to-end model to POWN-mean. POWN-mean computes the prototypes as the mean of the embeddings per class.\n\n#### <span id=\"page-16-2\"></span>B.4 SENSITIVITY ANALYSIS OF THE TEMPERATURE PARAMETERS\n\nAdditionally to the sensitivity analysis in the main paper, we analyze the effect of the temperature parameter of the supervised loss τ<sup>S</sup> and the pseudo-label loss τ<sup>P</sup> .\n\nWe use the datasets Photo and OGB-arXiv. We fix all hyperparameters to the results of our hyperparameter search and variate τ<sup>S</sup> ∈ {0.01, 0.05, 0.08, 0.1, 0.2, 0.5, 0.8, 1.0, 5.0, 10.0} and τ<sup>P</sup> ∈ {0.01, 0.05, 0.1, 0.5, 0.6, 0.7, 0.8, 1.0, 5.0, 10.0}. The results are presented in Figures [5a](#page-17-1) and [5b.](#page-17-1)\n\nThe sensitivity analysis of our temperature parameters shows that they were chosen in a reasonable range, which is close to optimum. The temperature parameter determines how much randomness is used in the contrastive loss. If the supervised temperature becomes too large, we do not use any label information since all prototype assignments become random. This is similar to not using the supervised loss at all. For the pseudo label temperature, we have an increase in the beginning. Since our pseudo-labels have some noise, the smoothing of the temperature parameters accounts for the uncertainty. For high temperatures, we observe that on Photo the performance converges to some suboptimal value while staying at the value of our main results on OGB-arXiv. Therefore, the pseudo-label assignment is more important for Photo than for OGB-arXiv, which is consistent with the results of our ablation study (see Table [3.](#page-8-1) However, the plot suggests that there could exist a better value for τ<sup>P</sup> on OGB-arXiv, which is larger than the one we used.\n\n### C DATASET DETAILS\n\n#### <span id=\"page-16-0\"></span>C.1 DATASET NODE SPLIT\n\nWe split the nodes for each dataset in the following way. For Cora and CiteSeer, we use the Planetoid split [\\(Yang](#page-12-12) [et al.,](#page-12-12) [2016\\)](#page-12-12). For Photo and Computers, we follow [Shchur et al.](#page-12-10) [\\(2018\\)](#page-12-10) and use a fixed, random Planetoid split with 20 train, 500 validation, and 1, 000 test nodes per class. We use the default temporal splits for OGB-arXiv and Reddit2. For OGB-arXiv, the training nodes are all papers published until 2017, the validation nodes are from 2018, and the test nodes are all papers published since 2019 [\\(Hu et al.,](#page-10-12) [2020a\\)](#page-10-12). Reddit2 has been sampled in October and November 2014. For Reddit2, the first 21 days are used for training. From the remaining days, 30% are used for validation and the rest as test set [\\(Hamilton et al.,](#page-10-4) [2017\\)](#page-10-4).\n\n#### <span id=\"page-16-1\"></span>C.2 DATASET FOLD DETAILS\n\nWe split the datasets into 3 to 5 folds, such that each fold contains a subset of the classes. We ensure that each fold contains roughly 20% of the classes and at least 2 classes. The resulting distributions of the number of classes into the folds can be seen in Table [7.](#page-17-2)\n\n<span id=\"page-17-1\"></span>![](_page_17_Figure_1.jpeg)\n\n(a) Analysis of the temperature parameters for the supervised loss τ<sup>S</sup> and the pseudo-label loss τ<sup>P</sup> for a range between 0.01 and 1. The orange dot marks the temperature used on the experiments.\n\n![](_page_17_Figure_3.jpeg)\n\n(b) Analysis of the temperature parameters for the supervised loss τ<sup>S</sup> and the pseudo-label loss τ<sup>P</sup> for a range between 0.01 and 10. The orange dot marks the temperature used on the experiments.\n\n<span id=\"page-17-2\"></span>\n\n| Figure 5: Ablation study on the temperature parameters of the supervised and pseudo-label loss. |  |  |  |  |\n|-------------------------------------------------------------------------------------------------|--|--|--|--|\n|                                                                                                 |  |  |  |  |\n\n| Dataset   | 1 | 2 | 3 | 4 | 5 | Train F. | Ck    |\n|-----------|---|---|---|---|---|----------|-------|\n| Cora      | 2 | 2 | 3 | - | - | 1        | 2-3   |\n| CiteSeer  | 2 | 2 | 2 | - | - | 1        | 2     |\n| Photo     | 2 | 2 | 2 | 2 | - | 2        | 4     |\n| Computers | 2 | 2 | 2 | 2 | 2 | 3        | 6     |\n| OGB-Arxiv | 8 | 8 | 8 | 8 | 8 | 3        | 24    |\n| Reddit2   | 8 | 8 | 8 | 8 | 9 | 3        | 24-25 |\n\nTable 7: Number of classes per fold, the number of folds used for training, and the number of known classes |Ck| the model is trained on in each fold.\n\n### <span id=\"page-17-0\"></span>D REPRODUCIBILITY AND HYPERPARAMETER\n\nWe trained all models using the Adam optimizer [\\(Kingma & Ba,](#page-11-14) [2015\\)](#page-11-14) with early stopping on the validation accuracy. All runs of k-means are initialized by k-means++ [\\(Arthur & Vassilvitskii,](#page-9-3) [2007\\)](#page-9-3). We set the patience to 30 for the small datatasets, i. e., Cora, CiteSeer, Photo, Computers, and 20 for the larger datasets, i. e., OGB-arXiv and Reddit2. Furthermore, we used the neighborhood sampler [\\(Hamilton et al.,](#page-10-4) [2017\\)](#page-10-4) with a batch size of 4, 096 and an upper bound of 128 for the number of one-hop and 32 for the number of two-hop neighbors for OGB-arXiv and Reddit2.\n\n| Dataset   | Layer | Hidden dim | Dropout | Learning rate | weight decay |\n|-----------|-------|------------|---------|---------------|--------------|\n| Cora      | 2     | 128        | 0.4     | 0.01          | 0.001        |\n| CiteSeer  | 2     | 256        | 0.8     | 0.01          | 0.01         |\n| Photo     | 2     | 64         | 0.8     | 0.01          | 0.001        |\n| Computers | 2     | 64         | 0.8     | 0.01          | 0.001        |\n| OGB-arXiv | 3     | 1024       | 0.6     | 0.01          | 0.001        |\n| Reddit2   | 2     | 128        | 0.2     | 0.001         | 0.0          |\n\n<span id=\"page-18-0\"></span>For the GCN encoder, which is used by all methods that require an encoder in our experiments, we choose the hyperparameters as presented in Table [8.](#page-18-0)\n\n| Table 8: Chosen hyperparameter values for the GCN encoder. |  |  |  |  |\n|------------------------------------------------------------|--|--|--|--|\n|------------------------------------------------------------|--|--|--|--|\n\nFor OpenWGL, we separately tuned the hidden dimension in {16, 32, 64}. The best hidden dimension was 32 for all datasets, except Reddit2, where we used 16.\n\n<span id=\"page-18-1\"></span>POWN has a weight for each loss and a temperature for the prototype losses, i. e., the supervised and pseudo-label loss. We set the temperature for the supervised loss τ<sup>S</sup> = 0.1 and for the pseudo-labels as τ<sup>P</sup> = 0.7, following [Sun &](#page-12-2) [Li](#page-12-2) [\\(2023\\)](#page-12-2). The loss weights λ, µ, ν, and κ are tuned by Bayesian hyperparameter optimization. We performed 1, 000 optimization steps for the small datasets, Cora, CiteSeer, Amazon and Photo, and 100 steps for the large datasets OGB-ArXiv and Reddit2. We select the hyperparameters with the highest average validation accuracy over all folds for each dataset. Each loss weight is tuned on a range between 0 and 1, with a uniform prior distribution. For the label propagation of POWN, we perform 2 iterations and only keep the labels with the lowest 10% entropy. The final parameters are presented in Table [9.](#page-18-1)\n\n| Dataset   | λ        | µ        | ν        | κ        | q        |\n|-----------|----------|----------|----------|----------|----------|\n| Cora      | 0.596017 | 0.652459 | 0.763453 | 0.208553 | 0.333999 |\n| CiteSeer  | 0.550021 | 0.238629 | 0.951837 | 0.021996 | 0.525537 |\n| Photo     | 0.863386 | 0.308698 | 0.461507 | 0.101025 | 0.882899 |\n| Computers | 0.988548 | 0.274850 | 0.362032 | 0.174794 | 0.635788 |\n| OGB-arXiv | 0.549624 | 0.670594 | 0.262712 | 0.139899 | 0.334155 |\n| Reddit2   | 0.983511 | 0.829771 | 0.068015 | 0.022779 | 0.915777 |\n\nTable 9: Final hyperparameter values for the loss weights λ, µ, and ν, the regularization weight κ, and the threshold γ, which determines the subset V<sup>n</sup> of our method POWN.\n\n### E COMPUTING INFRASTRUCTURE\n\nThe results were computed on a server with two AMD EPYC 9534 64-core Processors, two terabytes of RAM, and one Nvidia H 100 80 GB GPU.\n\n### F NOTATION TABLE\n\n| Variable                 | Definition                                                                         |\n|--------------------------|------------------------------------------------------------------------------------|\n| G                        | Graph consisting of nodes and edges                                                |\n| V                        | All nodes of the graph G                                                           |\n| Vl                       | Subset of labeled nodes with labels from known classes                             |\n| Vu                       | Subset of unlabeled nodes from known and new classes                               |\n| Vn                       | Subset of unlabeled nodes which are predicted to belong to a new class             |\n| E                        | All edges of the graph G                                                           |\n| Y                        | Set of all classes                                                                 |\n| Yk                       | Set of known classes, where a label is present in the training set                 |\n| Yn                       | Set of new classes, where no label is available during training                    |\n| P                        | Set of all prototypes                                                              |\n| Pk                       | Subset of prototypes associated with a known class                                 |\n| pi                       | A specific prototype that represents class i                                       |\n| wj                       | Edge weight computed by distance between the source node and the closest prototype |\n| ′<br>, P′<br>Lnll(V<br>) | Negative log likelihood loss of the prototype distances, parameterized by a subset |\n|                          | ′<br>′<br>of the nodes V<br>and a subset of the prototypes P                       |\n| LS                       | Supervised loss applied for the labeled classes                                    |\n| LU                       | Unsupervised loss applied on all datapoints                                        |\n| LP                       | Prototypical loss applied to the prototypes for new classes by using pseudo-labels |\n| R                        | Regularization term based on KL divergence and prototype distance                  |\n| f                        | Encoder model to obtain an embedding for a node                                    |\n| x                        | Feature vector associated with a node v                                            |\n| z                        | Embedding of node v                                                                |\n| y                        | Specific class of a node v                                                         |\n| yˆ                       | Pseudo label assigned to specific node                                             |\n| z˜                       | Corrupted version of the embedding z used for DGI                                  |\n| d(zi<br>, zj<br>)        | Distance between the embeddings zi<br>and zj                                       |\n| τ                        | Temperature hyperparameter                                                         |\n| D                        | Discriminator for the DGI loss represented by a two layer GCN                      |\n| s                        | Summary function required for DGI as mean of the embeddings                        |\n| q                        | Ratio of nodes which are considered as a new class                                 |\n| γ                        | The similarity where q%-percent of the labeled nodes are not contained             |\n| λ                        | Weight of the supervised loss                                                      |\n| µ                        | Weight of the unsupervised loss                                                    |\n| ν                        | Weight of the prototype loss                                                       |\n| κ                        | Weight of the regularization term                                                  |\n\nTable 10: Variables used throughout the paper along with their meaning.\\n，请你分析其研究动机、核心方法与公式推导细节。请结合摘要与正文信息，提取论文背景、问题定义、方法核心流程与理论基础。\n",
        "agent": "论文解读专家\n",
        "status": "started"
    }
]
