[
    {
        "timestamp": "2025-06-10 16:08:52",
        "task_name": "research_task",
        "task": "阅读论文《MALT: Improving Reasoning with Multi-Agent LLM Training》，论文内容如下：# MALT: Improving Reasoning with Multi-Agent LLM Training\n\nSumeet Ramesh Motwani <sup>1</sup> Chandler Smith <sup>2</sup> Rocktim Jyoti Das <sup>3</sup> Rafael Rafailov <sup>4</sup> Ivan Laptev <sup>3</sup> Philip H. S. Torr <sup>1</sup> Fabio Pizzati <sup>3</sup> Ronald Clark <sup>1</sup> Christian Schroeder de Witt <sup>1</sup>\n\n## Abstract\n\nLarge Language Models (LLMs) often produce answers with a single chain-of-thought, which restricts their ability to explore reasoning paths or self-correct flawed outputs in complex tasks. In this paper, we introduce MALT (Multi-Agent LLM Training), a novel post-training strategy that divides the reasoning process into generation, verification, and refinement steps using a sequential pipeline of heterogeneous agents. During data generation, each agent is repeatedly sampled to form a multi-agent search tree, where final outputs are graded against ground-truth data. We then apply value iteration to propagate reward signals back to each role-conditioned model, automatically producing multi-agent post-training data without human or teacher-model supervision. Our off-policy approach allows each agent to specialize by learning from correct and incorrect trajectories, ultimately improving the end-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same baseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40% respectively, making it an important advance towards multi-agent cooperative training.\n\n## 1. Introduction\n\nReasoning with Large Language Models (LLMs) is inherently challenging, particularly for tasks that require multi-step deductions, intermediate computations, or selfcorrection [\\(Xiang et al.,](#page-9-0) [2025\\)](#page-9-0). Recent work on multi-agent approaches—such as debate frameworks [\\(Du et al.,](#page-8-0) [2024\\)](#page-8-0) or orchestrated problem-solving [\\(Fourney et al.,](#page-8-1) [2024\\)](#page-8-1)—has shown promise by assigning different parts of the reasoning process to separate models, allowing for refinement and increased deliberation at inference time [\\(Snell et al.,](#page-9-1)\n\nCopyright 2024 by the author(s).\n\n<span id=\"page-0-0\"></span>![](_page_0_Figure_8.jpeg)\n\nFigure 1. MALT Overview. We sample multiple reasoning trajectories from a base LLM (e.g. Llama 3.1), apply credit assignment to label each step, and then post-train three specialized agents—a Generator, Verifier, and Refiner. At inference, these agents collaborate as a multi-agent system solving complex problems.\n\n[2024\\)](#page-9-1). However, the underlying LLMs are typically the same base model pre-trained on data that lacks exposure to the specialized roles or meta-strategies involved in solving complex problems, which introduces a distribution shift while reasoning at test-time [\\(Xiang et al.,](#page-9-0) [2025;](#page-9-0) [Han et al.,](#page-8-2) [2024\\)](#page-8-2). An open gap thus persists: *How can we jointly train LLMs to specialize in a multi-agent setting to improve reasoning?* Such a gap persists due to several key obstacles: First, in an end-to-end supervised training approach, it is difficult to propagate gradient signals through multiple discrete token-outputs. Second, there is limited role-specific labeled training data. Third, credit assignment is difficult in reinforcement learning with only sparse outcome rewards [\\(Tumer & Agogino,](#page-9-2) [2007\\)](#page-9-2). Finally, it is important to determine what type of multi-agent setup is even useful to implement meta-strategies that can improve reasoning so that more inference compute can be spent efficiently.\n\nIn this paper, we address these challenges by introducing an intuitive strategy to jointly post-train LLMs in a generate-verify-refine pipeline. This is analogous to how humans tackle complex tasks—drafting an initial answer, thoroughly verifying and critiquing it, and refining the solution to match their specifications [\\(Qian et al.,](#page-9-3) [2024\\)](#page-9-3). We propose a method, visualized in Figure [1,](#page-0-0) that automatically generates large-scale, labeled data for each agent via a multi-agent credit assignment approach. With this dataset, our post-training approach enables role-conditioned models\n\n<sup>1</sup>University of Oxford <sup>2</sup>Cooperative AI Foundation <sup>3</sup>MBZUAI 4 Stanford University. Correspondence to: Sumeet Ramesh Motwani <sumeet.motwani@eng.ox.ac.uk>, Christian Schroeder de Witt <cs@robots.ox.ac.uk>.\n\nto learn from both positive and negative reasoning trajectories—providing a path to improve reasoning performance across a range of problems with trained multi-agent setups.\n\nTo the best of our knowledge, we are the first to address this critical gap with Multi-Agent LLM Training (MALT): a strategy involving three models working sequentially on problems—a generator, a verifier, and a refinement model.\n\nMALT employs a sampling procedure that expands a search tree based on each model's outputs with an exponential branching factor, thereby producing large amounts of useful synthetic data. Role-specific data, particularly when augmented with rationales, has been shown in previous work to significantly improve performance [\\(Zelikman et al.,](#page-9-4) [2022\\)](#page-9-4). However, this approach leads to a credit assignment problem where internal reasoning branches may be correct or incorrect and must be labeled solely based on final outcome rewards to post-train models. To address this, we propose a value-iteration-based attribution strategy [\\(Sutton & Barto,](#page-9-5) [2018\\)](#page-9-5). By analyzing only the outputs of the search tree, our method identifies which model introduced an error, enabling credit assignment without requiring human-labeled data or a separate oracle policy to evaluate trajectories. This eliminates the need for human intervention in selecting trajectories, generating role-specific data, or designing value functions, and instead automatically produces reasoning traces from the search tree for post-training via supervised fine-tuning and preference optimization [\\(Rafailov et al.,](#page-9-6) [2023\\)](#page-9-6). MALT integrates all these steps into an intuitive joint-training procedure, providing performance improvements and serving as an early step toward unifying search and learning for multi-agent LLM systems.\n\nOur contributions are threefold:\n\n- We propose Multi-Agent LLM Training (MALT) to cooperatively post-train a generator, verifier, and refinement model on challenging reasoning tasks by leveraging search and subsequent fine-tuning.\n- We propose a search-tree expansion process and use a value iteration technique to propagate binary outcome rewards to automatically attribute correct and incorrect reasoning traces to individual agents. This can be utilized for supervised fine-tuning and preference optimization without human supervision.\n- We apply MALT to math and common sense reasoning questions from MATH, CSQA, and GSM8K, achieving relative improvements of 15.66%, 9.40%, and 7.42% over a single-model baseline, as well as outperforming a comprehensive set of baselines and ablations.\n\n## 2. Related Work\n\nAdvanced Reasoning: Multi-agent architectures have emerged as an effective way to handle complex reasoning by distributing problem-solving roles among distinct models [\\(Xiang et al.,](#page-9-0) [2025;](#page-9-0) [Cobbe et al.,](#page-8-3) [2021\\)](#page-8-3). Frameworks such as AgentQ [\\(Putta et al.,](#page-9-7) [2024\\)](#page-9-7) and AutoGen [\\(Wu et al.,](#page-9-8) [2024\\)](#page-9-8) coordinate solutions through guided search and compositional dialogue, yet they often rely on a single underlying model for multiple tasks or omit a dedicated post-trained verification mechanism. Debate-style [\\(Du et al.,](#page-8-0) [2024\\)](#page-8-0) and orchestrated [\\(Fourney et al.,](#page-8-1) [2024\\)](#page-8-1) multi-agent methods assign different parts of the reasoning process to separate agents, enabling increased deliberation at inference time [\\(Snell et al.,](#page-9-1) [2024\\)](#page-9-1). However, the underlying models lack advanced training for role specification, which may lead to suboptimal performance or distribution shifts during test-time [\\(Xiang et al.,](#page-9-0) [2025\\)](#page-9-0). In contrast, single-agent approaches utilize techniques such as introspection [\\(Qu et al.,](#page-9-9) [2024\\)](#page-9-9) and self-critique [\\(Saunders et al.,](#page-9-10) [2022\\)](#page-9-10) to detect their own errors, but they typically lack the mechanisms to correct those errors in a single inference pass [\\(Ye et al.,](#page-9-11) [2024\\)](#page-9-11).\n\nTraining Data and Preference Optimization: Harnessing synthetic data generation in tandem with preference-based training has emerged as a potent strategy for boosting LLM performance. [Setlur et al.](#page-9-12) [\\(2024\\)](#page-9-12) demonstrates that training on both correct and incorrect synthetic solutions, optimized with Direct Preference Optimization [\\(Rafailov et al.,](#page-9-6) [2023,](#page-9-6) DPO), significantly improves math reasoning performance. [Singh et al.](#page-9-13) [\\(2024\\)](#page-9-13) propose a self-training method that repeatedly generates, filters, and fine-tunes using scalar feedback, surpassing purely human-based fine-tuning on tasks like advanced math and coding. Parallel work by [Pang et al.](#page-8-4) [\\(2024\\)](#page-8-4) shows how preference signals applied across entire chains-of-thought can refine intermediate reasoning steps. Our method builds on these insights by generating large volumes of role-specific data, correct and incorrect, and uses reasoning preferences to train each agent.\n\nInference Time Compute: Strategic use of inference-time compute can also boost accuracy. [Brown et al.](#page-8-5) [\\(2024\\)](#page-8-5) demonstrates that repeated sampling from a single model improves coverage, while [Snell et al.](#page-9-1) [\\(2024\\)](#page-9-1) shows that iterative refinement can outperform naive scaling in certain tasks. Meanwhile, supervised fine-tuning (SFT) remains the backbone of LLM adaptation [\\(Devlin et al.,](#page-8-6) [2019;](#page-8-6) [Howard](#page-8-7) [& Ruder,](#page-8-7) [2018\\)](#page-8-7), but can demand large volumes of humanlabeled data. Quasi-supervised methods [\\(Yang et al.,](#page-9-14) [2024\\)](#page-9-14) address data scarcity by providing supervision for intermediate steps. Finally, [Zelikman et al.](#page-9-4) [\\(2022\\)](#page-9-4) and [Xiang et al.](#page-9-0) [\\(2025\\)](#page-9-0) underscore the effectiveness of structured chain-ofthought prompting for complex tasks, while [Cobbe et al.](#page-8-3) [\\(2021\\)](#page-8-3) confirm that an explicit verifier stage reduces errors on GSM8K. Our work unifies the aforementioned techniques to create a multi-agent pipeline that orchestrates a generator, verifier, and refinement model, leveraging search, synthetic data generation, and preference-based training to enable more robust multi-step reasoning.\n\n## <span id=\"page-2-1\"></span>3. Preliminaries\n\nChain of Thought (CoT) The output of an LLM solving a question q with natural language reasoning steps (s1, . . . , sn) followed by an answer a can be viewed as a CoT process sampled from a distribution:\n\n$$\np_d(a \\mid q) \\propto \\int p_d(a \\mid s_{1:n}, q) \\prod_{t=1}^n p_d(s_t \\mid s_{\n$$\n\nMore generally for complex problems, [\\(Xiang et al.,](#page-9-0) [2025\\)](#page-9-0) describe a true solution-generating process that involves extra latent steps (z1, . . . , zk), which is a meta-generalization of the prior CoT argument:\n\n$$\np_d(a, s_{1:n} | q) \\propto \\int p_d(a, s_{1:n} | z_{1:k}, q) \\prod_{t=1}^k p_d(z_t | z_{< t}, q) dZ\n$$\n\nThese meta-variables could capture iterative or corrective steps, unfolding a more adaptive, multi-stage reasoning trajectory. In a multi-agent setup, we consider a process with three specialized models: a Generator G, a Verifier V, and a Refinement model R producing g, v, and r respectively. This can be mapped onto the meta-CoT setting, where\n\n$$\np(a | q) = \\int p_G(g | q) p_{\\mathcal{V}}(v | g, q) p_R(a | g, v, q) d(g, v)\n$$\n\nHere, g and v, from the standpoint of standard ⟨q, a⟩ datasets, act like latent meta-steps guiding more complex solution processes, akin to z<sup>t</sup> in meta-CoT. By explicitly modeling these roles during joint post-training (see Section [4\\)](#page-2-0), the multi-agent sequential deliberation process allows us to tackle problems beyond a single-pass CoT setting.\n\nSupervised Finetuning (SFT) Given DSFT train containing positive demonstrations (e.g. correct generator outputs or useful verifier critiques), we can carry out SFT:\n\n$$\n\\mathcal{L}_{\\text{SFT}}(\\pi_{\\theta}) = -\\mathbb{E}_{(x,y)\\sim \\mathcal{D}_{\\text{train}}^{\\text{SFT}}}\\sum_{t=1}^T \\log \\pi_{\\theta}(y_t|y_{\n$$\n\nwhich has proved to be a simple and relatively successful technique to improve reasoning performance.\n\nDirect Preference Optimisation (DPO) In SFT, we can exclusively use positive samples in order to improve our test set performance. However, using preference optimization also allows us to leverage negative samples, y <sup>−</sup> ≺ y +, where ≺ means that y <sup>+</sup> is preferred over y <sup>−</sup>. DPO [\\(Rafailov](#page-9-6) [et al.,](#page-9-6) [2023\\)](#page-9-6) is one such method—a more efficient alternative to classical RLHF [\\(Christiano et al.,](#page-8-8) [2017\\)](#page-8-8)—because instead of employing a full reinforcement learning loop, it directly optimizes the following contrastive objective:\n\n$$\n\\mathcal{L}_{\\text{DPO}}(\\pi_{\\theta}) = -\\mathbb{E}_{(x,y^+,y^-)\\sim\\mathcal{D}_{\\text{train}}^{\\text{DPO}}}\\sigma\\bigg(\\beta \\log \\frac{\\pi_{\\theta}(y^+ \\mid x)}{\\pi_{\\text{ref}}(y^+ \\mid x)} - \\beta \\log \\frac{\\pi_{\\theta}(y^- \\mid x)}{\\pi_{\\text{ref}}(y^- \\mid x)}\\bigg).\n$$\n\nWe assume πref is a reference policy, obtained through SFT. DDPO train is a dataset of triplets including both inputs and positive/negative outputs, and β is the hyperparameter scaling reward signal strength. We provide a theoretical link between optimizing the DPO objective and the optimal RL policy in Appendix [A.4.1.](#page-14-0)\n\n## <span id=\"page-2-0\"></span>4. Method: Multi-Agent LLM Training\n\nIn complex reasoning tasks, a single LLM must handle all aspects of the solution generation process - often leading to limited exploration, a lack of self-correction, and difficulty refining partial steps [\\(Ye et al.,](#page-9-11) [2024\\)](#page-9-11). However, the reasoning process can be broken down into a system of decentralized LLMs, where each model has differing objectives and/or partial observability. As described in Section [3,](#page-2-1) this represents a meta-CoT [\\(Xiang et al.,](#page-9-0) [2025\\)](#page-9-0) setting where intermediate outputs in the multi-agent setup can improve the overall reasoning process. Although in fully observable and cooperative cases, systems of LLM agents could technically be simulated by a single centralized LLM, a decomposition into separate heterogeneous LLM agents offers various benefits analogous to those observed in decentralized multiagent learning [\\(Boutilier,](#page-8-9) [1996;](#page-8-9) [Schroeder de Witt et al.,](#page-9-15) [2020\\)](#page-9-15). Decentralization factorizes large joint action spaces, allowing each agent to focus on smaller sub-tasks under its own partial observability, leading to more targeted exploration and clear credit assignment [\\(Tan,](#page-9-16) [1993\\)](#page-9-16). In this section, we present our methodology for a multi-agent setting consisting of a sequential heterogeneous process where agents can be trained based on joint rewards.\n\n#### 4.1. Multi-Agent Inference Setting\n\nWe formulate our multi-agent inference setting as a collaborative reasoning framework designed to solve complex tasks. Let Q denote a dataset of natural language questions, where each q ∈ Q represents a specific task instance. The objective is to generate a prediction a ∈ A for a given input q, where A is the set of all possible answers. During training, there exists a ground truth function f : Q → A, where f(q) = a GT serves as the ideal prediction for evaluating a.\n\nOur framework consists of three specialized LLMs acting as distinct agents, each defined as a function:\n\n- 1. Generator (G : Q × P<sup>G</sup> → OG): Produces an initial response to the question.\n- 2. Verifier (V : O<sup>G</sup> × Q × P<sup>V</sup> → O<sup>V</sup> ): Critiques the generated response for potential errors.\n- 3. Refinement Model (R : O<sup>G</sup> × O<sup>V</sup> ×Q ×P<sup>R</sup> → OR): Integrates feedback to improve the final prediction.\n\nHere, PG,P<sup>V</sup> ,P<sup>R</sup> denote the set of prompts for each model, and OG, O<sup>V</sup> , and O<sup>R</sup> are the sets of possible outputs for the generator, verifier, and refiner respectively. A is the set\n\n<span id=\"page-3-0\"></span>![](_page_3_Figure_0.jpeg)\n\nFigure 2. MALT Method Overview. Given an input, we consider a three-agent system composed of a Generator for initial answer production, a Verifier providing a critique, and a Refinement Model integrating all intermediate reasoning steps into a final output. For questions in the training set, we introduce a tree search and credit assignment process (Left) to generate synthetic datasets with reasoning trajectory preference pairs for each model. These are used to post-train individual models (Center). During inference over the test-set, we perform three parallel sequential passes through the multi-agent setup, and return the final answer obtained via majority voting (Right).\n\nof possible answers extracted from the refiner's output with a fixed deterministic function T. Formally, we define the interaction between these agents as:\n\n$$\ng_o = G(q, p_g) \\quad \\text{where } g_o \\in \\mathcal{O}_G, p_g \\in \\mathcal{P}_G \\tag{1}\n$$\n\n$$\nv_o = \\mathcal{V}(q, p_v, g_o) \\quad \\text{where } v_o \\in \\mathcal{O}_{\\mathcal{V}}, p_v \\in \\mathcal{P}_{\\mathcal{V}} \\tag{2}\n$$\n\n$$\nr_o = R(q, p_r, g_o, v_o) \\quad \\text{where } r_o \\in \\mathcal{O}_R, p_r \\in \\mathcal{P}_R \\quad (3)\n$$\n\n$$\na = T(r_o) \\quad \\text{where } a \\in \\mathcal{A} \\tag{4}\n$$\n\nThis setup is reminiscent of how LLMs are used in production, where they receive initial prompts containing questions, feedback, and are then asked to refine answers [\\(Wang](#page-9-17) [et al.,](#page-9-17) [2024\\)](#page-9-17).We demonstrate in Section [5](#page-5-0) that this inference setting enhances performance compared to single-model approaches. The key insight, however, relies on leveraging this multi-agent inference setting to generate synthetic data that scales exponentially with respect to a branching factor. Below, we discuss out data generation and post-training setup.\n\n#### 4.2. Collecting Reasoning Trajectories\n\nIn standard single-LLM setups, a single model simply produces an answer a for each question q. By contrast, our multi-agent framework uses three specialized agents—Generator, Verifier, and Refiner—sequentially. To enable training of these agents, we need to capture how each agent's output contributes to the final prediction and whether the overall solution is correct or incorrect. A reasoning trace leading to a contains [go, vo, ro], where the\n\nmulti-agent answer generation process will be:\n\n$$\na = T(R(q, p_r, G(q, p_g), V(q, p_v, G(q, p_g))))\\big).\n$$\n (5)\n\nDuring data generation, G, V, and R all share the same base model parameters; in the subsequent training phase (Section [4.3\\)](#page-4-0), each policy is updated independently to specialize in its respective role. An outcome reward model R : A × A → {0, 1}, based on the ground truth in the training set, evaluates a to mark the trajectory as either correct (1) or incorrect (0). Specifically, for a predicted answer a and ground truth a GT, we define:\n\n$$\n\\mathcal{R}(a, a^{\\text{GT}}) = \\begin{cases} 1, & \\text{if } a = a^{\\text{GT}}, \\\\ 0, & \\text{otherwise.} \\end{cases}\n$$\n (6)\n\nTo collect role-specific post-training data, we use < q, aGT > pairs from an initial training set Dtrain associated with each benchmark. Our solution is illustrated in Figure [2](#page-3-0) (left). The following sampling strategy is employed for all models, with a brancing factor of n.\n\n1. For each problem q<sup>i</sup> ∈ Dtrain in the training data, we sample n completions from the generator:\n\n$$\n\\{g_{i,j} \\sim G(x_i)\\}_{j=1}^n.\n$$\n (7)\n\n2. For each G output gi,j , we produce n verifications:\n\n$$\n\\{v_{i,j,k} \\sim \\mathcal{V}(g_{i,j}, q_i)\\}_{k=1}^n.\n$$\n (8)\n\n3. For each V output vi,j,k, we generate n refinements:\n\n$$\n\\{r_{i,j,k,l} \\sim R(g_{i,j}, v_{i,j,k}, q_i)\\}_{l=1}^n.\n$$\n (9)\n\nThis process results in n 3 trajectories for each training example, totaling |Dtrain| · n 3 trajectories. We use the outcome reward model R to label the refinement outputs as correct (✓) or incorrect (×). The exponential branching factor is very useful to collect a large number of diverse training samples (and inference can be parallelized for efficiency while post-training will just rely on the fixed dataset collected).\n\nTo effectively utilize reward signals from the refinement model's outputs, we adopt a value iteration approach to propagate values backward through the reasoning chain. Specifically, we compute the *expected value* of each intermediate output (i.e. generator and verifier outputs) based on the values of downstream outputs from the refiner. This global pooling approach is useful because an intermediate step's utility depends on how it influences downstream outputs and final correctness (see Figure [2,](#page-3-0) Left). Partial solutions can still yield correct answers once refined, so simply providing binary labels is insufficient. By assigning each output an expected value, we better capture the influence on the final reward. This also allows MALT to be generalized to settings where intermediate steps may not carry the same labels, and each intermediate output can now be credited in proportion to its impact on the final solution.\n\nValue Function Definitions We define the value of each *refinement* node ri,j,k,l by directly comparing its final answer a to the ground truth:\n\n$$\n\\mathbf{V}(r_{i,j,k,l}) = \\mathcal{R}(T(r_{i,j,k,l}), a^{\\text{GT}}) \\in \\{0, 1\\}.\n$$\n (10)\n\nThe value of a verifier output vi,j,k is then computed as the expected value of its child refinements:\n\n$$\n\\mathbf{V}(v_{i,j,k}) = \\mathbb{E}_{l}[\\mathbf{V}(r_{i,j,k,l})] \\approx \\frac{1}{n} \\sum_{l=1}^{n} \\mathbf{V}(r_{i,j,k,l}).\n$$\n (11)\n\nSimilarly, the value of a generator output gi,j is computed as the expected value of its child verifier outputs:\n\n$$\n\\mathbf{V}(g_{i,j}) = \\mathbb{E}_k[\\mathbf{V}(v_{i,j,k})] \\approx \\frac{1}{n} \\sum_{k=1}^n \\mathbf{V}(v_{i,j,k}).\n$$\n (12)\n\nEach output state's empirical mean is a *Monte Carlo* approximation (unbiased sample avg.) of its true expected correctness, indicated using \"≈\" for the expected value calculation and for each q. This process allows us to propagate reward signals from final outputs through the tree to each intermediate state, capturing the overall utility of each output. Thresholding and Binarization To prepare the data for training (SFT and DPO), we *binarize* values using a threshold of 0.5, aligning with majority voting principles. Nodes with values greater than 0.5 are labeled as correct (✓), and those with values less than or equal to 0.5 are labeled as incorrect (×). The intuition behind this strategy is discussed in more detail in Appendix [A.4.1,](#page-14-0) where we demonstrate that it ensures the policy's expected value monotonically increases. Formally, for each output state s (i.e. output from any model in the multi-agent setup), we define its label sˆ as:\n\n$$\n\\hat{s} = \\begin{cases} \\checkmark, & \\text{if } \\mathbf{V}(s) > 0.5, \\\\ \\times, & \\text{otherwise.} \\end{cases}\n$$\n (13)\n\n#### <span id=\"page-4-0\"></span>4.3. MALT Post-training\n\nWith each output assigned a value, the goal is to now use these for post-training the models (see Figure [2,](#page-3-0) Center). We first detail how training data for each model is generated: Each refinement output ri,j,k,l has an associated value V(ri,j,k,l) ∈ {0, 1}. We create preference pairs (r <sup>+</sup>, r<sup>−</sup>) where r <sup>+</sup> is a correct refinement (V(r <sup>+</sup>) = 1) and r <sup>−</sup> is an incorrect refinement (V(r <sup>−</sup>) = 0) for the same verifier input vi,j,k. Formally:\n\n$$\n\\mathcal{D}_R = \\left\\{ (r^+, r^-) \\middle| \\begin{aligned} r^+, r^- \\in \\{r_{i,j,k,l}\\}_{l=1}^n, \\\\ \\mathbf{V}(r^+) = 1, \\ \\mathbf{V}(r^-) = 0 \\end{aligned} \\right\\}.\n$$\n (14)\n\nFor each verifier output vi,j,k, we compute its value V(vi,j,k) and binarize it as follows:\n\n$$\n\\hat{v}_{i,j,k} = \\begin{cases} \\checkmark, & \\text{if } \\mathbf{V}(v_{i,j,k}) > 0.5, \\\\ \\times, & \\text{otherwise.} \\end{cases}\n$$\n (15)\n\nPreference pairs for the verifier model are created by comparing outputs under the same generator output gi,j :\n\n$$\n\\mathcal{D}_{\\mathcal{V}} = \\left\\{ (v^+, v^-) \\middle| \\begin{aligned} v^+, v^- &\\in \\{v_{i,j,k}\\}_{k=1}^n, \\\\ \\hat{v}^+ &= \\checkmark, \\ \\hat{v}^- &= \\times \\end{aligned} \\right\\} . \\tag{16}\n$$\n\nA similar process applies to the generator model, where generator outputs gi,j are binarized based on their values V(gi,j ), and preference pairs D<sup>G</sup> are created by comparing outputs under the same query q<sup>i</sup> . Thus, datasets are collected only in pairs, leading to a sufficiently large (see Sec. [5\\)](#page-5-0) and diverse set obtained from the tree expansion process.\n\nRefinement and Verifier Model Training Following the data generation process outlined above and the training setups in Section [3,](#page-2-1) we first perform SFT on the G, V , R models with the questions and positive samples in DG, D<sup>V</sup> , and D<sup>R</sup> respectively, updating each model's policy separately. For G, this is analogous to standard STaR post-training [\\(Ze](#page-9-4)[likman et al.,](#page-9-4) [2022\\)](#page-9-4) with a specialized dataset. Next, on only\n\nthe SFT updated V and R models, we apply DPO with question and chosen-rejected pairs in D<sup>V</sup> and D<sup>R</sup> respectively. For the Generator, we opt for SFT-only because DPO does not improve performance (primarily because the base LLM - Llama 3.1 8B is already extensively post-trained on preferences as a generator [\\(Grattafiori et al.,](#page-8-10) [2024\\)](#page-8-10) and Sec. [5.4\\)](#page-7-0). By combining SFT with preference-based updates, we capture both \"ideal\" behaviors (through correct samples) and \"relative\" preferences (through correct-vs-incorrect pairs). This allows us to not only bootstrap reasoning based on positive traces but also learn generalizable knowledge about useful reasoning trajectories [\\(Chu et al.,](#page-8-11) [2025\\)](#page-8-11). We show our training and inference setups in Figure [2](#page-3-0) and a present a detailed algorithm for MALT in Appendix [A.1.](#page-10-0)\n\n## <span id=\"page-5-0\"></span>5. Experiments\n\nIn this section, we outline the experimental details, including a description of the model, the benchmarks used for evaluation, and the training pipeline. We then present our main experimental results, followed by an empirical analysis and baseline comparisons, along with ablations.\n\n#### 5.1. Experimental Details\n\nBenchmarks and Models We use the Llama-3.1-8B-Instruct model [\\(Grattafiori et al.,](#page-8-10) [2024\\)](#page-8-10), chosen for its opensource nature and balance between competitive baseline performance and size fitting in a limited compute budget. We evaluate MALT and all baselines on three widely-used benchmarks: GSM8K [\\(Cobbe et al.,](#page-8-3) [2021\\)](#page-8-3), which consists of 7.47k training examples and 1.32k test questions, focused on diverse grade school math problems. To evaluate MALT on more challenging mathematical reasoning questions, we use MATH [\\(Hendrycks et al.,](#page-8-12) [2021\\)](#page-8-12), consisting of 7.5k training and 5k test problems. MATH has proven to be consistently difficult for smaller language models, with Llama 3.1 8B performing around 49.50% test-accuracy. Lastly, to extend the scope beyond mathematical tasks, we evaluate on CommonsenseQA (CSQA) [\\(Talmor et al.,](#page-9-18) [2019\\)](#page-9-18) with 9.74k training examples and 1.22k dev-set questions. CSQA is a multiple-choice question answering dataset around commonsense reasoning problems and has been used similarly by prior work [\\(Zelikman et al.,](#page-9-4) [2022;](#page-9-4) [2024;](#page-9-19) [Wei et al.,](#page-9-20) [2023\\)](#page-9-20).\n\nBaselines We compare MALT against eight baselines in 2 primary settings, all using Llama 3.1 8B. First, we implement the *inference-only* setting with (1) a single-agent (SA) naive setting in which a single model is used as a generator to provide an outupt, (2) a multi-agent (MA) setting, where the pre-trained baseline model operates in a sequential way as a generator, verifier, and refinement agent with the same prompts given to MALT post-trained models. Our second setting is with only supervised fine-tuning on all three models with the positive data from the synthetic set collected\n\nfor MALT. We refer to this as our *STaR baseline*. MALT involves a majority-voting based self-consistency mechanism described below and also implemented for all baselines.\n\nMALT Procedure For MALT, we generate synthetic data for each benchmark separately using the tree-based approach (Algorithm [1\\)](#page-10-1) with a branching factor of n = 3, yielding 27 trajectories per question and approximately 2k–6k labeled pairs per model/benchmark. Each final answer is compared against the ground truth in the training set to assign a binary reward, which is then propagated to label the G, V , and R outputs. During this, each model has a fixed role conditioning prompt template that is also used for baselines and for the post-trained models. We first train each role-specific Llama-3.1-8B-Instruct model on positive labels with LoRA-based SFT and then with DPO to incorporate reasoning preference data for reinforcement learning (discussed in Section [4.3.](#page-4-0) We use LoRA adapter-based fine-tuning [\\(Hu et al.,](#page-8-13) [2021\\)](#page-8-13), significantly reducing the computational load for post-training (see Appendix [A.6](#page-17-0) for more details). At inference, MALT follows a simple sequential inference pass through the three heterogeneous models reasoning over questions in the test set. Training small models on long sequences of text can lead to instability or hallucinations [\\(Park et al.,](#page-9-21) [2024\\)](#page-9-21), therefore MALT employs a three-vote self-consistency mechanism, mitigating potential hallucinations.\n\n#### 5.2. Experimental Results\n\nOur experimental results along with all the baselines are presented in Table [1,](#page-6-0) along with Ablations in Tables [2](#page-7-1) and [3.](#page-7-2) Our results are averaged over four runs on random subsets of the large test-sets across seeds, and we report the standard deviation for all our results. Below, we detail our experimental results and associated analysis and discussion.\n\nBaselines comparison We propose baseline results in Table [1.](#page-6-0) Baseline single agent scores on MATH, CSQA, and GSM8K are 49.50%, 74.50%, and 84.25%, approximately in line with scores reported in Llama 3.1 8B's release [\\(Grattafiori et al.,](#page-8-10) [2024\\)](#page-8-10). These scores go up to 52.50%, 75.75%, 86.75% and 53.50%, 79.00%, 87.00% with single model majority voting and multi-agent majority voting respectively. Additionally, after applying STaR (SFT on positive synthetic CoTs), single-agent baselines achieve around 52.25%, 76.25%, and 81.75% on MATH, CSQA, and GSM8K, while the multi-agent STaR variant remains around 52.50%, 75.50%, and 80.00%. Self consistency results for STaR variants surpass purely untrained baselines but still lag behind the reasoning improvements obtained with MALT and described below.\n\nMALT Core Results MALT (Table [1,](#page-6-0) right) achieves an accuracy of 57.25%, 81.50%, and 90.50% on MATH,\n\n<span id=\"page-6-0\"></span>\n\n| Benchmark | Inference-only |       |                                                     |       |    |       | STaR Training                                       |       | MALT     | MALT                      |\n|-----------|----------------|-------|-----------------------------------------------------|-------|----|-------|-----------------------------------------------------|-------|----------|---------------------------|\n|           | SA             | SA+MV | MA                                                  | MA+MV | SA | SA+MV | MA                                                  | MA+MV | (w/o MV) |                           |\n| GSM8K     |                |       | 84.25 ± 2.28 86.75 ± 2.38 84.75 ± 2.86 87.00 ± 4.00 |       |    |       | 81.75 ± 0.83 84.75 ± 2.68 80.00 ± 1.58 86.75 ± 2.28 |       |          | 83.50 ± 2.18 90.50 ± 2.06 |\n| CSQA      |                |       | 74.50 ± 3.35 75.75 ± 5.49 77.50 ± 5.17 79.00 ± 4.69 |       |    |       | 76.25 ± 4.32 78.75 ± 4.26 75.50 ± 2.69 76.00 ± 1.73 |       |          | 77.50 ± 1.12 81.50 ± 2.29 |\n| MATH      |                |       | 49.50 ± 2.06 52.50 ± 2.50 51.75 ± 3.56 53.50 ± 2.87 |       |    |       | 52.25 ± 1.48 54.00 ± 2.73 52.50 ± 3.20 53.75 ± 2.68 |       |          | 52.25 ± 1.79 57.25 ± 1.48 |\n\nTest Accuracy (%) over 4 seeds ↑\n\nTable 1. Benchmark results. We compare MALT with baselines on three different benchmarks. For baselines, we include different setups such as single agent (SA) and multi-agent (MA), both with and without majority voting (MV). *MALT outperforms all baselines.*\n\n<span id=\"page-6-1\"></span>![](_page_6_Figure_3.jpeg)\n\nFigure 3. Self-correction rates. MALT consistently increases the number of correct answers by correcting previously incorrect answers at a much higher rate than introducing new mistakes compared to a single-model baseline (MV@3).\n\nCSQA, and GSM8K. Overall, *MALT significantly outperforms all baselines, including all settings with supervised fine-tuned models. Over the base model's performance as a generator, MALT achieves relative improvements of* 15.66%*,* 9.40%*, and* 7.42% *on MATH, CSQA, and GSM8K.* This demonstrates the reasoning efficacy of our search and attribution based data generation, post-training, and inference pipeline in MALT across benchmarks of varying difficulty.\n\n#### 5.3. Core analysis\n\nSelf-consistency From results in Table [1,](#page-6-0) MALT demonstrates improved reasoning with majority-voting for selfconsistency [\\(Wang et al.,](#page-9-22) [2022\\)](#page-9-22), but its performance without MV remained close to that of the multi-agent inferenceonly setting. A qualitative examination of the reasoning trajectories revealed that while MALT was able to solve questions previously unsolved by all baselines, it experienced self-consistency issues due to hallucinations. Thus, we use a small majority voting factor of 3 and observe that self-consistency reliably yields significantly higher relative improvement for MALT over baselines. For instance, on MATH, self-consistency resulted in a relative improvement of 9.57%, exceeding the 3 − 6% gains in other baselines.\n\nReasoning self-correction rates We measured how often MALT flips an incorrect solution into a correct one versus\n\n<span id=\"page-6-2\"></span>![](_page_6_Figure_9.jpeg)\n\nFigure 4. Improvement over turns. MALT demonstrates consistent improvements at each turn. Test accuracy at each turn from a sequential pass through the post-trained Generator, Verifier, and Refinement models (All with MV@3).\n\nthe opposite (Figure [3\\)](#page-6-1). On MATH, MALT converts 14.25% of wrong answers into correct ones, while only 6.50% of correct answers become wrong. For GSM8K, 9.75% flip to correct and 3.75% flip to incorrect, and CSQA with 11.25% vs. 4.25%. These numbers indicate that MALT's generator–verifier–refiner process shows strong self-improvement with significantly lower mistakes introduced.\n\nImprovement over turns We use GPT-4 to rate the accuracy of each step MALT produced on the test sets to analyze its performance after each agent inference (\"turn\"). Figure [4](#page-6-2) shows how performance evolves. For example, MATH's untrained baseline improves from 49.50% to 52.50% with majority voting. MALT, over its 3 reasoning and multi-agent deliberation turns, increases performance to 53.50% (turn 1 - Generator), 55.75% (turn 2 - Verifier), and 57.25% (turn 3 - Refiner). We observe similar increases across GSM8K and CSQA. This consistent turn-wise improvement highlights how sequential verification and refinement are both very important for improving overall reasoning performance. We also provide ablations to understand why generate-verifyrefine is the most appropriate paradigm in Section [5.4.](#page-7-0)\n\nOverall performance Across all benchmarks, MALT consistently outperforms both zero-shot and fine-tuned baselines, closing gaps on problems previously unsolved by any baseline. Its multi-agent framework not only achieves\n\n<span id=\"page-7-1\"></span>\n\n| MALT Post-training |   |   |       |       |       |\n|--------------------|---|---|-------|-------|-------|\n| G                  | V | R | GSM8K | CSQA  | MATH  |\n| ✗                  | ✓ | ✓ | 87.00 | 78.75 | 54.25 |\n| ✓                  | ✗ | ✓ | 85.75 | 76.75 | 54.50 |\n| ✓                  | ✓ | ✗ | 86.25 | 75.50 | 55.25 |\n| ✓                  | ✓ | ✓ | 90.50 | 81.50 | 57.25 |\n\nTable 2. Ablations with untrained models in MALT (MV@3). This table shows the performance of untrained agents combined with trained ones, demonstrating that all agents perform best when cooperating with peers trained with MALT.\n\nhigher average scores but also corrects systematic errors. For example, the verifier often successfully locates errors by redoing calculations and providing a precise critique, with the pattern learnt automatically from synthetic data generated via our search process. Similarly, for CSQA, the verifier implicitly learns to focus on aspects of the problem overlooked by the generator, allowing for additional inference-compute spent on important gaps. Examples of these reasoning interactions are provided in Appendix [A.2.](#page-10-2)\n\n#### <span id=\"page-7-0\"></span>5.4. Ablation studies\n\nUntrained model ablations We test how useful training each individual agent is by replacing one of them with an untrained baseline and keeping the other two MALT-trained agents. As seen in Table [2,](#page-7-1) this consistently degrades performance on all benchmarks. For instance, using an untrained generator yields mean accuracies of 54.25% (MATH), 87.00% (GSM8K), and 78.75% (CSQA), notably lower than a fully trained system. Similar drops occur when substituting an untrained verifier or refiner. This reinforces that *all* agents benefit from the MALT pipeline, and removing training from any one of them reduces performance (see Table [2\\)](#page-7-1).\n\nImportance of generate-verify-refine Next, we measure the impact of removing one role from our three-agent pipeline and running simpler two-agent systems. Specifically, we compare (i) *generate-refine* (G + R, skipping the verifier) and (ii) *generate-verify* (G + V , skipping the refiner). As shown in Table [3,](#page-7-2) both setups underperform the full pipeline: G + R yields mean accuracies of 54.75% (MATH), 84.75% (GSM8K), and 76.25% (CSQA), while G + V yields 55.75% (MATH), 88.75% (GSM8K), and 78.00% (CSQA), both paradigms below the generate-verifyrefine setup in MALT. This confirms our specific multi-agent setup yields consistently stronger results by enabling more inference spent on sequential deliberation.\n\nEffectiveness of DPO over only SFT As shown in Table [1,](#page-6-0) DPO consistently improves MALT's performance beyond SFT alone (STaR) by leveraging negative data—an approach also observed in [\\(Putta et al.,](#page-9-7) [2024\\)](#page-9-7) and [\\(Setlur et al.,](#page-9-12) [2024\\)](#page-9-12). In particular, purely positive \"rationales\" can introduce spurious correlations and degrade SFT performance,\n\n<span id=\"page-7-2\"></span>\n\n| Configuration    | GSM8K | CSQA  | MATH  |\n|------------------|-------|-------|-------|\n| G only           | 84.75 | 78.75 | 54.00 |\n| G + V            | 88.75 | 78.00 | 55.75 |\n| G + R            | 84.75 | 76.25 | 54.75 |\n| G + V + R (Ours) | 90.50 | 81.50 | 57.25 |\n\nTable 3. Performance of ablated multi-agent setups (MV@3). This table shows that all agents in our pipeline are necessary to achieve the best results.\n\nwhich does indeed occur in our empirical and qualitative results; the contrastive training approach that DPO provides with negative data helps the model identify high-advantage reasoning steps to improve with higher sample-efficiency [\\(Rafailov et al.,](#page-9-23) [2024\\)](#page-9-23).For reasoning problems, SFT tends to memorize the data and rules, which is useful to bootstrap reasoning to a certain extent [\\(Zelikman et al.,](#page-9-4) [2022\\)](#page-9-4). However, our results indicate that this could degrade performance sometimes, and preference learning methods such as DPO (see Appendix [A.4.2](#page-16-0) for a theoretical analysis) exhibit better performance at learning generalizable knowledge around reasoning steps [\\(Chu et al.,](#page-8-11) [2025\\)](#page-8-11). However, for the Generator, we find that SFT+DPO actually *lowers* performance (for e.g. 52.25% on MATH with SFT and 51.25% with SFT+DPO)—likely because Llama 3.1 8B-Instruct already underwent post-training with DPO on a very similar generator data distribution for benchmarks [\\(Grattafiori et al.,](#page-8-10) [2024\\)](#page-8-10), making DPO on a similar distribution prone to overfitting, consistent with observations in [Setlur et al.](#page-9-12) [\\(2024\\)](#page-9-12).\n\n## 6. Discussion and Conclusion\n\nWe presented MALT, a novel post-training strategy dividing CoT reasoning among three specialized LLMs-a generator, verifier, and refiner-to tackle complex reasoning tasks. To our knowledge, MALT is the first approach bridging the gap between prior multi-agent inference methods and fullytrained multi-agent systems, by generating role-specific data using a tree-based sampling and credit assignment mechanism, without human annotations. Crucially, MALT utilizes the negative synthetic data to identify and correct flawed reasoning steps with LLM post-training. Unlike standard single-LLM setups, our design closely mirrors how humans solve complex tasks or even use LLMs—attempting a solution, critiquing errors, and finally refining the result. This process yields consistent gains on benchmarks of varying difficulty across all baselines and ablations, demonstrating the effectiveness of multi-agent post-training. We provide detailed theoretical grounding and additional explanations In Appendix [A.4.1.](#page-14-0) Moreover, in Section [A.5,](#page-16-1) we outline future directions, such as scaling the branching factor and understanding its effect on performance, and moving to an online RL paradigm. These directions require more compute and remain beyond the scope of this work.\n\nBy structuring the reasoning process as a multi-agent system in which specialized models sequentially deliberate over problems, MALT enables role-specific reasoning capabilities in pre-trained models to improve. This allows systems of LLMs to more efficiently utilize meta-strategies such as self-correction and chaining inference steps. With MALT, we introduce a new research paradigm that can be applied to any foundation model to improve reasoning capabilities.\n\n## Impact Statement\n\nThis paper presents a method to enhance the reasoning capabilities of frontier models in multi-agent setups. We believe that our contributions may have significant societal implications. First, our post-trained agents demonstrated substantial performance improvements across competitive benchmarks. This could enable groups of small, open-source models to achieve performance levels comparable to closed-source frontier models. Additionally, MALT has important implications for safety, which we address in [A.7.](#page-17-1) We emphasize the importance of using our contributions responsibly.\n\n## References\n\n- <span id=\"page-8-9\"></span>Boutilier, C. Planning, learning and coordination in multiagent decision processes. In *TARK*, volume 96, pp. 195–210. Citeseer, 1996.\n- <span id=\"page-8-5\"></span>Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: ´ Scaling inference compute with repeated sampling. *arXiv preprint arXiv:2407.21787*, 2024.\n- <span id=\"page-8-8\"></span>Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep Reinforcement Learning from Human Preferences. In *NeurIPS*. Curran Associates, Inc., 2017.\n- <span id=\"page-8-11\"></span>Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q. V., Levine, S., and Ma, Y. Sft memorizes, rl generalizes: A comparative study of foundation model post-training. *arXiv*, 2025.\n- <span id=\"page-8-3\"></span>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. *arXiv*, 2021.\n- <span id=\"page-8-16\"></span>DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., and Gao, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL <https://arxiv.org/abs/2501.12948>.\n- <span id=\"page-8-6\"></span>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\n\nPre-training of deep bidirectional transformers for language understanding. In *NAACL*, 2019.\n\n- <span id=\"page-8-0\"></span>Du, Y., Li, S., Torralba, A., Tenenbaum, J. B., and Mordatch, I. Improving factuality and reasoning in language models through multiagent debate. In *ICML*, 2024.\n- <span id=\"page-8-1\"></span>Fourney, A., Bansal, G., Mozannar, H., Tan, C., Salinas, E., Niedtner, F., Proebsting, G., Bassman, G., Gerrits, J., Alber, J., et al. Magentic-one: A generalist multi-agent system for solving complex tasks. *arXiv*, 2024.\n- <span id=\"page-8-19\"></span>Geva, M., Goldberg, Y., and Berant, J. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. In *EMNLP*, 2019.\n- <span id=\"page-8-10\"></span>Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., and Team, L. The llama 3 herd of models. *arXiv*, 2024.\n- <span id=\"page-8-17\"></span>Greenblatt, R., Shlegeris, B., Sachan, K., and Roger, F. AI Control: Improving Safety Despite Intentional Subversion. In *ICML*, 2024.\n- <span id=\"page-8-2\"></span>Han, S., Zhang, Q., Yao, Y., Jin, W., Xu, Z., and He, C. Llm multi-agent systems: Challenges and open problems. *arXiv*, 2024.\n- <span id=\"page-8-12\"></span>Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. In *NeurIPS*, 2021.\n- <span id=\"page-8-7\"></span>Howard, J. and Ruder, S. Universal language model finetuning for text classification. In *ACL*, 2018.\n- <span id=\"page-8-13\"></span>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In *ICLR*, 2021.\n- <span id=\"page-8-15\"></span>Ivison, H., Wang, Y., Liu, J., Wu, Z., Pyatkin, V., Lambert, N., Smith, N. A., Choi, Y., and Hajishirzi, H. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. In *NeurIPS*, 2024.\n- <span id=\"page-8-14\"></span>Lerer, A., Hu, H., Foerster, J., and Brown, N. Improving policies via search in cooperative partially observable games. In *AAAI*, 2020.\n- <span id=\"page-8-18\"></span>McAleese, N., Pokorny, R. M., Uribe, J. F. C., Nitishinskaya, E., Trebacz, M., and Leike, J. LLM Critics Help Catch LLM Bugs. *arXiv*, 2024.\n- <span id=\"page-8-4\"></span>Pang, R. Y., Yuan, W., Cho, K., He, H., Sukhbaatar, S., and Weston, J. Iterative reasoning preference optimization, 2024.\n- <span id=\"page-9-21\"></span>Park, R., Rafailov, R., Ermon, S., and Finn, C. Disentangling length from quality in direct preference optimization. In *ACL Findings*, 2024.\n- <span id=\"page-9-7\"></span>Putta, P., Mills, E., Garg, N., Motwani, S., Finn, C., Garg, D., and Rafailov, R. Agent q: Advanced reasoning and learning for autonomous ai agents. *arXiv*, 2024.\n- <span id=\"page-9-3\"></span>Qian, C., Liu, W., Liu, H., Chen, N., Dang, Y., Li, J., Yang, C., Chen, W., Su, Y., Cong, X., Xu, J., Li, D., Liu, Z., and Sun, M. ChatDev: Communicative Agents for Software Development. In *ACL*, 2024.\n- <span id=\"page-9-9\"></span>Qu, Y., Zhang, T., Garg, N., and Kumar, A. Recursive introspection: Teaching language model agents how to self-improve. In *NeurIPS*, 2024.\n- <span id=\"page-9-6\"></span>Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. In *NeurIPS*, 2023.\n- <span id=\"page-9-23\"></span>Rafailov, R., Hejna, J., Park, R., and Finn, C. From r to q ∗ : Your language model is secretly a q-function. In *COLM*, 2024.\n- <span id=\"page-9-10\"></span>Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for assisting human evaluators. *arXiv*, 2022.\n- <span id=\"page-9-15\"></span>Schroeder de Witt, C., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P. H. S., Sun, M., and Whiteson, S. Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge? *arXiv*, 2020.\n- <span id=\"page-9-24\"></span>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. *arXiv*, 2017.\n- <span id=\"page-9-12\"></span>Setlur, A., Garg, S., Geng, X., Garg, N., Smith, V., and Kumar, A. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. In *NeurIPS*, 2024.\n- <span id=\"page-9-13\"></span>Singh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Garcia, X., Liu, P. J., Harrison, J., Lee, J., Xu, K., Parisi, A., Kumar, A., Alemi, A., Rizkowsky, A., Nova, A., Adlam, B., Bohnet, B., Elsayed, G., Sedghi, H., Mordatch, I., Simpson, I., Gur, I., Snoek, J., Pennington, J., Hron, J., Kenealy, K., Swersky, K., Mahajan, K., Culp, L., Xiao, L., Bileschi, M. L., Constant, N., Novak, R., Liu, R., Warkentin, T., Qian, Y., Bansal, Y., Dyer, E., Neyshabur, B., Sohl-Dickstein, J., and Fiedel, N. Beyond human data: Scaling self-training for problem-solving with language models. *TMLR*, 2024.\n- <span id=\"page-9-1\"></span>Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time compute optimally can be more effective than scaling model parameters. *arXiv*, 2024.\n- <span id=\"page-9-5\"></span>Sutton, R. S. and Barto, A. G. Reinforcement learning: an introduction, 2nd edn. adaptive computation and machine learning, 2018.\n- <span id=\"page-9-18\"></span>Talmor, A., Herzig, J., Lourie, N., and Berant, J. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In *NAACL*, 2019.\n- <span id=\"page-9-16\"></span>Tan, M. Multi-agent reinforcement learning: Independent vs. cooperative agents. In *Proceedings of the tenth international conference on machine learning*, pp. 330–337, 1993.\n- <span id=\"page-9-2\"></span>Tumer, K. and Agogino, A. Distributed agent-based air traffic flow management. In *Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems*, AAMAS '07, New York, NY, USA, 2007. Association for Computing Machinery. ISBN 9788190426275.\n- <span id=\"page-9-17\"></span>Wang, J., Ma, W., Sun, P., Zhang, M., and Nie, J.-Y. Understanding user experience in large language model interactions. *arXiv*, 2024.\n- <span id=\"page-9-22\"></span>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in lms, 2022.\n- <span id=\"page-9-20\"></span>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. In *NeurIPS*, 2023.\n- <span id=\"page-9-8\"></span>Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. In *COLM*, 2024.\n- <span id=\"page-9-0\"></span>Xiang, V., Snell, C., Gandhi, K., Albalak, A., Singh, A., Blagden, C., Phung, D., Rafailov, R., Lile, N., Mahan, D., Castricato, L., Franken, J.-P., Haber, N., and Finn, C. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though. *arXiv*, 2025.\n- <span id=\"page-9-14\"></span>Yang, G., Li, C., Yao, Y., Wang, G., and Teng, Y. Quasisupervised learning for super-resolution pet. *Computerized Medical Imaging and Graphics*, 2024.\n- <span id=\"page-9-11\"></span>Ye, T., Xu, Z., Li, Y., and Allen-Zhu, Z. Physics of language models: Part 2.2, how to learn from mistakes on gradeschool math problems. *arXiv*, 2024.\n- <span id=\"page-9-4\"></span>Zelikman, E., Wu, Y., Mu, J., and Goodman, N. D. Star: Bootstrapping reasoning with reasoning. In *NeurIPS*, 2022.\n- <span id=\"page-9-19\"></span>Zelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N., and Goodman, N. D. Quiet-star: Language models can teach themselves to think before speaking. In *COLM*, 2024.\n\n# A. Appendix\n\n## <span id=\"page-10-0\"></span>A.1. Algorithm\n\nMALT orchestrates a three-agent system—comprising a Generator for initial answers, a Verifier for critiques, and a Refiner that integrates these steps into a final response. During training, we expand a multi-agent search tree for each question, labeling correct and incorrect branches via a value iteration based credit assignment mechanism. This generates role-specific preference pairs for post-training each agent via supervised fine-tuning and preference optimization. Algorithm [1](#page-10-1) provides a complete description of the data collection and training pipeline for MALT.\n\n# <span id=\"page-10-1\"></span>Algorithm 1 Multi-Agent LLM Training and Synthetic Data Generation (MALT)\n\nRequire: Initial Dataset D, Models G, V , R, Branching factor n GOAL: Trained models G′ , V ′ , R′ 1: Initialize datasets SG, S<sup>V</sup> , S<sup>R</sup> as empty sets 2: for q ∈ D do 3: A<sup>G</sup> ← {g<sup>j</sup> = G(q)} n <sup>j</sup>=1 ▷ Generate n outputs from G 4: for each g<sup>j</sup> ∈ A<sup>G</sup> do 5: A gj <sup>V</sup> ← {vj,k = V (q, g<sup>j</sup> )} n <sup>k</sup>=1 ▷ Generate n outputs from V 6: for each vj,k ∈ A gj V do 7: A g<sup>j</sup> ,vj,k <sup>R</sup> ← {rj,k,l = R(q, g<sup>j</sup> , vj,k)} n <sup>l</sup>=1 ▷ Generate n outputs from R 8: for each rj,k,l ∈ A g<sup>j</sup> ,vj,k <sup>R</sup> do 9: Compute V(rj,k,l) = R(rj,k,l, aGT) ▷ Reward for R output 10: Add (q, g<sup>j</sup> , vj,k, rj,k,l, V(rj,k,l)) to S<sup>R</sup> 11: end for 12: Compute <sup>V</sup>(vj,k) = <sup>1</sup> n P<sup>n</sup> <sup>l</sup>=1 V(rj,k,l) ▷ Value V for V output 13: Binarize vˆj,k = I[V(vj,k) > 0.5] 14: Add (q, g<sup>j</sup> , vj,k, vˆj,k) to S<sup>V</sup> 15: end for 16: Compute <sup>V</sup>(g<sup>j</sup> ) = <sup>1</sup> n P<sup>n</sup> <sup>k</sup>=1 V(vj,k) ▷ Value V for G output 17: Binarize gˆ<sup>j</sup> = I[V(g<sup>j</sup> ) > 0.5] 18: Add (q, g<sup>j</sup> , gˆ<sup>j</sup> ) to S<sup>G</sup> 19: end for 20: end for 21: Training the Models 22: G′ ← SFT(G, SG) ▷ Fine-tune G with supervised data 23: VSFT ← SFT (V, {(q, g<sup>j</sup> , vj,k) | (q, g<sup>j</sup> , vj,k, vˆj,k) ∈ S<sup>V</sup> , vˆj,k = 1}) ▷ Fine-tune V on positive samples 24: V ′ ← DPO(VSFT, S<sup>V</sup> ) ▷ Train V with DPO using preferences 25: RSFT ← SFT (R, {(q, g<sup>j</sup> , vj,k, rj,k,l) | V(rj,k,l) = 1}) ▷ Fine-tune R on positive samples 26: R′ ← DPO(RSFT, SR) ▷ Train R with DPO using preferences 27: return G′ , V ′ , R′\n\n## <span id=\"page-10-2\"></span>A.2. Qualitative Examples\n\nIn this section, we present three key qualitative examples: a synthetic training example for SFT and DPO, a qualitative performance comparison between an untrained and a trained verifier, and a complete MALT setup demonstrating a backtracking and improvement reasoning cycle that generates a correct answer.\n\n## GSM8K Synthetic Dataset Sample for the Refinement Model (GT Answer = 25)\n\n## System prompt\n\n*You are an expert problem solver that refines math solutions based on feedback.*\n\n#### Input\n\n*Question: Marco loves trading cards. He has a total of 500 cards on hand, and a fourth of them are duplicates. Marco decides to trade one-fifth of these duplicates for completely new cards to his friend Josh. How many new cards did Marco get?*\n\n*Initial Answer: I'd be happy to help you solve this problem step by step.*\n\nStep 1: Identify the key information given in the question\n\n\\* Marco has a total of 500 cards. \\* A fourth of them are duplicates.\n\nStep 2: Determine what the question is asking us to find\n\n\\* The question is asking us to find how many new cards Marco got from trading one-fifth of his duplicates with Josh. Step 3: Plan our approach to solving the problem\n\n\\* First, we need to find the number of duplicates Marco has. Since a fourth of his cards are duplicates, we can find the number of duplicates by multiplying 500 by 1/4. \\* Then, we need to find one-fifth of the number of duplicates to find out how many cards Marco traded with Josh. \\* Finally, we'll subtract the number of cards traded from the total number of duplicates to find the remaining duplicates.\n\nStep 4: Execute our plan\n\n\\* Number of duplicates = 500 x 1/4 = 125 \\* One-fifth of the number of duplicates = 125 x 1/5 = 25 \\* Marco traded 25 cards with Josh.\n\nStep 5: State our conclusion and double-check our answer\n\n\\* Marco got 25 new cards from trading with Josh.\n\nFinal Answer: 25\n\n#### Rejected Output\n\n*Based on the initial answer and the verification feedback, I refine the solution as follows:*\n\nStep 1: Identify the key information given in the question\n\n\\* Marco has a total of 500 cards. \\* A fourth of them are duplicates.\n\nStep 2: Determine what the question is asking us to find\n\n\\* The question is asking us to find how many new cards Marco got from trading one-fifth of his duplicates with Josh. Step 3: Plan our approach to solving the problem\n\n\\* First, we need to find the number of duplicates Marco has. Since a fourth of his cards are duplicates, we can find the number of duplicates by multiplying 500 by 1/4. \\* Then, we need to find one-fifth of the number of duplicates to find out how many cards Marco traded with Josh. \\* Finally, we'll subtract the number of cards traded from the total number of duplicates to find the remaining duplicates.\n\nStep 4: Execute our plan\n\n\\* Number of duplicates = 500 x 1/4 = 125 \\* One-fifth of the number of duplicates = 125 x 1/5 = 25 \\* Marco traded 25 cards with Josh. Since he traded one-fifth of his duplicates, we can find the total number of duplicates he has left by adding the number of cards traded back to the number of duplicates: 125 + 25 = 150 \\* To find the number of new cards Marco got, we subtract the number of duplicates he traded from the total number of duplicates: 150 - 25 = 125 Step 5: State our conclusion and double-check our answer\n\n\\* Marco got 125 new cards from trading with Josh.\n\nFinal Answer: Final Answer: 125\n\n#### Accepted Output\n\nRefined Solution:\n\nStep 1: Identify the key information given in the question\n\n\\* Marco has a total of 500 cards. \\* A fourth of them are duplicates. \\* Marco trades one-fifth of his duplicates with Josh.\n\nStep 2: Determine what the question is asking us to find\n\n\\* The question is asking us to find how many new cards Marco got from trading one-fifth of his duplicates with Josh. Step 3: Plan our approach to solving the problem\n\n\\* First, we need to find the number of duplicates Marco has. Since a fourth of his cards are duplicates, we can find the number of duplicates by multiplying 500 by 1/4. \\* Then, we need to find one-fifth of the number of duplicates to find out how many cards Marco traded with Josh. \\* Finally, we'll subtract the number of cards traded from the total number of duplicates to find the remaining duplicates.\n\nStep 4: Execute our plan\n\n\\* Number of duplicates = 500 x 1/4 = 125 \\* One-fifth of the number of duplicates = 125 x 1/5 = 25 \\* Marco traded 25 cards with Josh. \\* Remaining duplicates = 125 - 25 = 100\n\nStep 5: State our conclusion and double-check our answer\n\n\\* Marco got 25 new cards from trading with Josh.\n\nFinal Answer: 25\n\n## Qualitative Example of an Untrained VS Trained Verifier\n\n## Question\n\n*Billy buys a 12-pack of soda from the store. If he has twice as many brothers as sisters, and he has 2 sisters, how many sodas can he give to each of his siblings if he wants to give out the entire 12-pack while giving each the same number?*\n\n## Initial Answer\n\n*I'd be happy to help you solve this problem step by step.*\n\n\\*\\*Step 1: Identify the key information given in the question.\\*\\*\n\n\\* Billy buys a 12-pack of soda. \\* He has twice as many brothers as sisters. \\* He has 2 sisters.\n\n\\*\\*Step 2: Determine what the question is asking us to find.\\*\\*\n\nThe question is asking us to find out how many sodas Billy can give to each of his siblings if he wants to give out the entire 12-pack while giving each the same number.\n\n\\*\\*Step 3: Plan our approach to solving the problem.\\*\\*\n\nLet's let x be the number of siblings (brothers and sisters). We know that Billy has 2 sisters, so the number of brothers is 2 times the number of sisters, which is 2x. The total number of siblings is the sum of the number of sisters and brothers, which is x + 2x = 3x. We are given that Billy has a 12-pack of soda, so we can set up the equation: 12 = x + 2x\n\n\\*\\*Step 4: Execute our plan, showing all work clearly.\\*\\*\n\nTo solve for x, we can combine like terms:\n\n12 = 3x\n\nDivide both sides by 3:\n\n4 = x\n\nSo, Billy has 4 siblings in total (2 sisters and 2 brothers).\n\n\\*\\*Step 5: State our conclusion and double-check our answer.\\*\\*\n\nSince Billy has 12 sodas and 4 siblings, he can give:\n\n12 ÷ 4 = 3 sodas to each of his siblings. Final Answer: 3\n\n## Untrained Verifier\n\n*As a verifier, I will critically evaluate the initial answer provided above.*\n\nThe initial answer states that Billy can give 3 sodas to each of his siblings. Let's verify the answer by performing the calculations.\n\n\\*\\*Verification:\\*\\*\n\n1. We are given that Billy has 12 sodas and 4 siblings (2 sisters and 2 brothers). 2. To find out how many sodas each sibling can get, we can divide the total number of sodas (12) by the number of siblings (4): 12 ÷ 4 = 3\n\nThe initial answer is correct! Billy can indeed give 3 sodas to each of his siblings. \\*\\*Feedback:\\*\\*\n\nThe initial answer is mathematically correct. The calculations are accurate, and the solution is well-explained. The answer is also logically consistent with the given information. Well done!\n\nHowever, it's always a good idea to double-check the calculations to ensure accuracy. In this case, the verification step confirms that the initial answer is correct.\n\n## Trained Verifier\n\n*As a verifier, I will carefully evaluate the initial answer provided above.*\n\nThe initial answer states that Billy can give 3 sodas to each of his siblings. Let's double-check the calculations to ensure the answer is mathematically correct.\n\nThe initial answer claims that Billy has 4 siblings (2 sisters and 2 brothers). However, the question states that Billy has twice as many brothers as sisters, and he has 2 sisters. This implies that the number of brothers is 2 times the number of sisters, which is 2 x 2 = 4 brothers. Therefore, Billy actually has 2 sisters and 4 brothers, making a total of 6 siblings.\n\nThe initial answer also claims that Billy can give 12 sodas to 4 siblings, resulting in 3 sodas per sibling. However, dividing 12 sodas among 6 siblings would actually give 2 sodas per sibling, not 3. \\*\\*Conclusion:\\*\\*\n\nThe initial answer contains a calculation error. The correct number of siblings is 6, not 4. Therefore, Billy can give 2 sodas to each of his siblings, not 3.\n\nAs a verifier, I recommend that the initial answer be corrected to reflect the correct calculation. The correct answer is 2 sodas per sibling.\n\n#### A Complete Reasoning and Improvement Sequence Example with MALT on CommonsenseQA\n\n#### Question\n\nHe looked at the field of pumps, all slowly churning oil out of the what?\n\n#### Choices\n\nA: manual\n\n- B: street\n- C: restaurant\n- D: ground\n\nE: service station\n\n#### Initial Answer (Generator)\n\nReasoning steps: 1. The context of the question is about pumps churning oil, which suggests a location where oil is being dispensed or extracted. 2. A \"manual\" (A) does not typically relate to a location where oil is churning. 3. A \"street\" (B) is a general location, but it's not specific to oil churning. 4. A \"restaurant\" (C) is not associated with oil churning. 5. The \"ground\" (D) could be a possible location, but it's not the most specific or common location for oil churning. 6. A \"service station\" (E) is a common location where oil is churning, as it's where cars are serviced and oil is dispensed.\n\nFinal Answer: E\n\n## Verifier\n\nThe initial answer is E: service station.\n\nThis answer seems to be the most logical choice given the context of the question. The phrase \"field of pumps\" suggests a location where multiple pumps are present, and a service station is a common place where multiple\n\npumps are used to dispense oil and other fuels.\n\nHowever, the question asks what the pumps are churning oil out of, not where the pumps are located. This changes the interpretation of the question.\n\nConsidering the phrase \"churning oil out of,\" it implies that the pumps are extracting or pumping oil from a source. In this case, the most likely source of oil would be the ground, as oil is often extracted from the ground. Therefore, the correct answer is D: ground.\n\n#### Refined Answer\n\nReasoning steps: 1. The context of the question is about pumps churning oil, which suggests a location where oil is being extracted or dispensed. 2. The phrase \"churning oil out of\" implies that the pumps are extracting or pumping oil from a source. 3. The most common source of oil is the ground, as it is often extracted from the earth. 4. Therefore, the pumps are churning oil out of the ground.\n\nFinal Answer: D\n\nGround Truth (not provided to any model) D\n\n#### A.3. Detailed Discussion\n\nWe now discuss key design choices. Simplicity with theoretical grounding—particularly the assurance of a trained policy with a monotonically increasing value—was a primary consideration. We opted for DPO over PPO-based methods due to our focus on a straightforward offline data generation process, treating our approach as an independent learning problem in a centralized setting [\\(Lerer et al.,](#page-8-14) [2020\\)](#page-8-14) with a single iteration (the key difference being that our agent policies post-training differ). In this setting, DPO is more stable than PPO and requires less overhead. While PPO could use the value computed at each branch as a reward for post-training nodes (a promising future direction), it introduces significant computational complexity. Moving from offline to online RL with additional computational overhead might indeed improve performance.\n\nOur value iteration method, when binarized, resembles global majority-based pooling: for a given node and branch, the binary reward of the leaf nodes in the subtree determines the usefulness of the branch, analogous to binarizing values propagated through the tree. In contrast, local pooling computes the binary value of a branch based only on the majority outcomes of its direct children, propagating this process to the leaf nodes. We also leave the choice between MCTS and an expansive tree-based sampling strategy as an open problem. Given our limited tree depth, tree-based sampling proved efficient, supported synthetic data generation with an exponential branching factor, and produces explainable outputs. Our dataset is collected offline, and individual models are trained on this synthetic data. While this approach works empirically, handling any new, out-of-distribution data would require iterative rollout and post-training methods.\n\nBased on our empirical results and the modularity of our algorithmic approach, it is highly plausible that our method will scale to larger models and scenarios with many agents, thus laying the foundations for new state-of-the-art AI agents based on systems of cooperative frontier models. Overall, our multi-agent system is currently composed of a sequence of agents that start out with the same parameters and different prompts. MALT performs joint training to transform this into a heterogeneous agent setting, where agents with different parameters operate cooperatively. Exploring other multi-agent settings is an important direction for subsequent work.\n\n#### A.4. Theoretical Justification for MALT\n\n#### <span id=\"page-14-0\"></span>A.4.1. CREDIT ASSIGNMENT STRATEGY\n\nHere, we provide a theoretical justification for why our framework, when updating the agent policies based on binarized pooled rewards with a threshold at θ = 0.5, leads to policy improvements. We formalize MALT as a three-step MDP, define the pooling operation through value iteration, and demonstrate how off-policy updates increase the expected reward.\n\nThe reasoning process in MALT is modeled as a three-step MDP over a set of M questions {qi}<sup>M</sup> <sup>i</sup>=1 drawn from a distribution Q. For each question q<sup>i</sup> , the process begins at the initial state s<sup>0</sup> = q<sup>i</sup> , where an initial answer gi,j is sampled from the generator policy πG(· | s0) for j = 1, . . . , n. The state then transitions to s<sup>1</sup> = (q<sup>i</sup> , gi,j ). At this second state, a critique vi,j,k is sampled from the verifier policy π<sup>V</sup> (· | s1) for k = 1, . . . , n, leading to the state s<sup>2</sup> = (q<sup>i</sup> , gi,j , vi,j,k). Finally, at this state, a refined answer ri,j,k,l is sampled from the refiner policy πR(· | s2) for l = 1, . . . , n, and a reward R(s2, ri,j,k,l) is assigned: 1 if ri,j,k,l is correct, 0 otherwise. The joint policy is defined as π = (πG, π<sup>V</sup> , πR), and the objective can now be expressed as:\n\n$$\nJ(\\pi) = \\mathbb{E}_{q \\sim Q} \\Big[ \\mathbb{E}_{q \\sim \\pi_G(\\cdot | s_0)} \\mathbb{E}_{v \\sim \\pi_V(\\cdot | s_1)} \\mathbb{E}_{r \\sim \\pi_R(\\cdot | s_2)} [R(s_2, r)] \\Big].\n$$\n\nReasoning trajectories are collected offline under an initial policy π (0) = (π (0) <sup>G</sup> , π (0) V , π (0) <sup>R</sup> ), yielding M · n 3 total samples. Through this tree-sampling method, values propagate backward using value iteration: Leaf nodes have V(ri,j,k,l) = R(ri,j,k,l) ∈ {0, 1}, verifier nodes compute V(vi,j,k) = <sup>1</sup> n P<sup>n</sup> <sup>l</sup>=1 V(ri,j,k,l), and generator nodes estimate V(gi,j ) = 1 n<sup>2</sup> P<sup>n</sup> k=1 P<sup>n</sup> <sup>l</sup>=1 V(ri,j,k,l). The true value V ∗ (ν) = Eπ(0) [R | ν] is approximated by these Monte Carlo estimates.\n\nWe note that our analysis rests on the coverage assumption, where for any relevant action (e.g., g with V ∗ (g) > E[V ∗ (g)]) over π (0) <sup>G</sup> , the initial policy satisfies π (0) <sup>G</sup> (g | q) ≥ α > 0, with analogous conditions for π (0) V and π (0) <sup>R</sup> . Independence holds across levels: refinements ri,j,k,l are i.i.d. given (gi,j , vi,j,k), critiques vi,j,k are conditionally independent given gi,j , answers gi,j are i.i.d. given q<sup>i</sup> , and questions q<sup>i</sup> are from Q. Moreover, our exponential branching factor n allows for sufficient sampling.\n\nThis allows us to show that our value estimates are within a certain ϵ bound of their true values with high probability. For a node ν with m downstream refinements (e.g., m = n 2 for gi,j ), the value estimate V(ν) = <sup>1</sup> m P<sup>m</sup> <sup>l</sup>=1 R<sup>l</sup> , where each R<sup>l</sup> ∼ Bernoulli(V ∗ (ν)) provides an unbiased estimator of V ∗ (ν). Hoeffding's inequality bounds the estimation error:\n\n$$\nP(|\\mathcal{V}(\\nu) - \\mathcal{V}^*(\\nu)| \\ge \\epsilon) \\le 2 \\exp(-2m\\epsilon^2).\n$$\n\nFor generator nodes (m = n 2 ), this becomes:\n\n$$\nP(|\\mathcal{V}(g_{i,j}) - \\mathcal{V}^*(g_{i,j})| \\ge \\epsilon) \\le 2 \\exp(-2n^2 \\epsilon^2).\n$$\n\nThus, applying a union bound over all Mn generator nodes, we find that with probability at least 1 − δ, the estimation error for any given generator node for all questions satisfies\n\n$$\n|\\mathcal{V}(g_{i,j}) - \\mathcal{V}^*(g_{i,j})| \\le \\epsilon, \\quad \\text{where} \\quad \\epsilon = \\sqrt{\\frac{\\ln\\left(\\frac{2Mn}{\\delta}\\right)}{2n^2}}.\n$$\n\nMALT binarizes node values using a threshold of 0.5: Vˆ(ν) = 1 if V(ν) > 0.5, and 0 otherwise. This is analogous to majority-voting, where V(ν) > 0.5 indicates that most refinements are correct, aligns with the Bernoulli decision boundary for binary rewards, and balances misclassification costs for conservative updates. The updated policy π(1) shifts probability mass toward these \"high-value\" nodes using Supervised Finetuning (SFT) and Direct Preference Optimization (DPO). For the refiner, SFT is applied for nodes where Vˆ(ri,j,k,l) = 1 followed by DPO using preference pairs (r <sup>+</sup>, r<sup>−</sup>). Similarly for the verifier, SFT is followed by DPO on its credit assigned preference pairs. For the generator, SFT is performed using answers where Vˆ(gi,j ) = 1. These updates improve the joint policy by prioritizing actions that yield higher expected rewards under π(0). In Section [A.4.2,](#page-16-0) we discuss why policy optimizing our DPO objective based on data collected offline under π(0) is identical to the optimal RL policy.\n\nFinally, we provide an intuitive explanation of our threshold θ used for credit assignment. In iterative settings, θ should be an adaptive factor increasing from 0.5 to 1. However, in our offline setting, 0.5 is a balanced threshold to use for the following reasons:\n\n- Lower Thresholds (θ < 0.5): This allows for greater sample-efficiency as more branches labeled as correct are used as part of training. However, it might introduce noise into the training process with samples that have low values being chosen as correct.\n- Higher Thresholds (θ > 0.5): This would allow for a focus on actions leading to higher-value nodes, reducing variance. However, having θ too high would reduce sample efficiency.\n\nUsing θ = 0.5 provides a balance suitable for a single iteration based on an offline generated dataset. By formalizing our value iteration approach and policy updates, we have shown how MALT increases the probability of selecting outputs leading to higher expected return from the system, and thus increases overall multi-agent performance.\n\n#### <span id=\"page-16-0\"></span>A.4.2. POLICY OPTIMIZING THE DPO OBJECTIVE IS IDENTICAL TO OPTIMAL RL POLICY\n\nTo support our claims in Appendix [A.4.1,](#page-14-0) we leverage Theorem 1 from [\\(Putta et al.,](#page-9-7) [2024\\)](#page-9-7) and Theorem 6.1 from [\\(Setlur](#page-9-12) [et al.,](#page-9-12) [2024\\)](#page-9-12), adjusted for our binarization setting:\n\nTheorem. Consider a policy π that optimizes our objective over trajectories generated by a reference policy πref. At each node (state) ht, preferences between actions during DPO are generated according to:\n\n$$\np(a_t^w \\succ a_t^l \\mid h_t) \\propto \\sigma\\left(\\hat{Q}(h_t, a_t^w) - \\hat{Q}(h_t, a_t^l)\\right),\\tag{17}\n$$\n\nwhere:\n\n- a w t and a l t are two win/loss actions at node ht,\n- Qˆ(ht, a) ∈ {0, 1} is the binarized value function, representing the expected reward of action a at state ht,\n\nThen, the policy that optimizes the Direct Preference Optimization (DPO) objective is identical to the optimal RL policy:\n\n$$\n\\pi^*(a \\mid h_t) \\propto \\pi_{\\text{ref}}(a \\mid h_t) \\exp\\left(\\frac{\\hat{Q}(h_t, a)}{\\beta}\\right),\\tag{18}\n$$\n\nwhere β is the DPO hyperparameter.\n\nThe proof for *Theorem 1* in [\\(Putta et al.,](#page-9-7) [2024\\)](#page-9-7) shows that the policy π ∗ approximates the optimal RL policy. That is, we can approximate the optimal RL policy if we generate preferences under the optimal value function (or an approximation thereof, i.e. our binarized version as shown below).\n\nIn our setting, since Qˆ(ht, a) ∈ {0, 1}, the exponential term simplifies to:\n\n• \n$$\n\\exp\\left(\\frac{1}{\\beta}\\right)\n$$\n when  $\\hat{Q}(h_t, a) = 1$ ,\n\n• 1 when \n$$\n\\hat{Q}(h_t, a) = 0\n$$\n.\n\nTherefore, the optimized policy becomes:\n\n$$\n\\pi^*(a \\mid h_t) \\propto \\begin{cases} \\pi_{\\text{ref}}(a \\mid h_t) \\exp\\left(\\frac{1}{\\beta}\\right), & \\text{if } \\hat{Q}(h_t, a) = 1, \\\\ \\pi_{\\text{ref}}(a \\mid h_t), & \\text{if } \\hat{Q}(h_t, a) = 0. \\end{cases}\n$$\n(19)\n\nThis means that the policy π ∗ increases the probability of selecting actions with Qˆ(ht, a) = 1 by a constant factor relative to the reference policy πref. By optimizing the DPO objective with these binarized preferences, we ensure that the policy increasingly favors actions leading to higher expected rewards, aligning with our credit assignment strategy described in Appendix [A.4.1.](#page-14-0) This supports our claim of (approximate) monotonic improvement, as the policy updates move us closer to the optimal policy by consistently selecting actions associated with higher binarized values.\n\n#### <span id=\"page-16-1\"></span>A.5. Future Directions\n\n#### A.5.1. IMPROVING MULTI-AGENT SYSTEMS\n\nOur findings showcase the potential of multi-agent LLM systems optimized with fine-tuning and collaborative inference techniques. There are several future directions from this line of work: Using PPO [\\(Schulman et al.,](#page-9-24) [2017\\)](#page-9-24) and the exact value propogated backward for each trajectory to update model weights, possibly in an online RL fashion, might produce strong results with additional computational overhead [\\(Ivison et al.,](#page-8-15) [2024\\)](#page-8-15). Moreover, we provide several levers around the number of models (where the three model setup can be used iteratively), controlling the branching factor for data generation, examining the effect of majority voting on more samples, changing the attribution threshold, or treating the attribution threshold as an adaptive parameter when iteratively training and rolling out from the multi-agent system (see Appendix [A.6\\)](#page-17-0). Moreover, prompt-tuning strategies and different roles can be considered or distillation techniques. We note that these are all specific and interesting directions. However, they lie beyond the scope of this paper, where our goal is to introduce a new multi-agent post-training methodology and demonstrate strong empirical performance.\n\n## A.5.2. LLM MID-TRAINING\n\nWith the release of [\\(DeepSeek-AI et al.,](#page-8-16) [2025,](#page-8-16) DeepSeek R1) and similar reasoning models, an open gap remains: *How do we generate cold-start data to enable better exploration of meta-strategies with reinforcement learning based training of LLMs?* MALT could provide an important step in this direction, search trajectories could be distilled into single models. We leave this direction for future work.\n\n## <span id=\"page-17-0\"></span>A.6. Additional Information\n\nFor SFT, we used LoRA with a learning rate multiplier of 0.1 and a batch size of 8 to avoid overfitting. For preference optimization, we used Direct Preference Optimization (DPO) with a preference tuning learning rate multiplier to 0.1, training beta parameter of 0.2, and adapter weight configured to 0.2. We varied the number of epochs between 1 to 10 based on the size of the synthetic dataset for each model and leave a deeper exploration of hyperparameter configurations that could require a significant amount of compute to future work. SFT training was often until convergence. DPO training did not necessarily converge by the end of all iterations.\n\nWe keep our prompts the same for every baseline and trained model on a given benchmark. Our prompts use CoT and zero-shot prompting. We use a temperature of 0.3 for Llama 3.1 8B Instruct since it was qualitatively good enough to prevent hallucinations and still led to diverse enough samples. MALT requires the presence of an initial training set containing question-answer pairs, which led to the use of MATH, CSQA, and GSM8K.\n\nDuring inference for the data collection strategy, using an exponential branching factor does not add significant compute overhead because inference calls can be parallelized when sampling from a model with the exact same input. Moreover, during training, we obtain a fixed dataset upon which LoRA fine-tuning can be conducted. LoRA adapters ensure that the model weights themselves aren't duplicated, thus requiring only minimal additional memory for the adapters themselves while the base models remain the same.\n\n## <span id=\"page-17-1\"></span>A.7. Safety\n\nOur approach can be used not just to enhance the reasoning capabilities of LLM systems, but also address crucial open problems in the safety of multi-agent systems. Importantly, MALT-trained systems of trusted small models could attain better task performance while retaining high degrees of trust, producing more powerful overseers within the AI control setting [\\(Greenblatt et al.,](#page-8-17) [2024\\)](#page-8-17). Another prominent application of our approach would be to train verifiers as safety critics within a multi-agent setup. This could scale up the settings such as OpenAI CriticGPT [\\(McAleese et al.,](#page-8-18) [2024\\)](#page-8-18) to any number of models, resulting in more powerful safety critics and allowing for the legibility of solutions to be improved.\n\n## A.8. Limitations and Ethics Statement\n\nWe note that even at low temperatures, model performance on benchmarks often exhibits high variance. To address this within our computational constraints, we conducted evaluations on subsets of test-sets (size 100) across four seeds. While CommonsenseQA is known to contain many biased or incorrectly labelled questions [\\(Geva et al.,](#page-8-19) [2019\\)](#page-8-19), we utilized it in a manner consistent with prior work. We aim to make all our training data, inference logs, and code publicly available.\n\n### A.9. Acknowledgments\n\nWe would like to thank Aleks Petrov, Dulhan Jayalath, Xingyi Yang, Tala Aljaafari, Markian Rybchuk, Kalyan R, Milind Maiti, Divyansh Garg, and Lewis Hammond for their time and insightful discussions. SM dedicates this work to the memory of his grandmother, Mohini Motwani.，分析其研究动机、核心方法与公式推导细节。请结合摘要与正文信息，提取论文背景、问题定义、方法核心流程与理论基础。\n",
        "agent": "论文解读专家\n",
        "status": "started"
    }
]
