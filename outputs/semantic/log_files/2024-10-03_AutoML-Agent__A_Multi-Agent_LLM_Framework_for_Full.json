[
    {
        "timestamp": "2025-06-10 16:11:25",
        "task_name": "research_task",
        "task": "阅读论文《AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML》，论文内容如下：# <span id=\"page-0-0\"></span>AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML\n\nPatara Trirat <sup>1</sup> Wonyong Jeong <sup>1</sup> Sung Ju Hwang 1 2\n\n### Abstract\n\nAutomated machine learning (AutoML) accelerates AI development by automating tasks in the development pipeline, such as optimal model search and hyperparameter tuning. Existing AutoML systems often require technical expertise to set up complex tools, which is in general timeconsuming and requires a large amount of human effort. Therefore, recent works have started exploiting large language models (LLM) to lessen such burden and increase the usability of AutoML frameworks via a natural language interface, allowing non-expert users to build their data-driven solutions. These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs. This paper proposes *AutoML-Agent*, a novel multi-agent framework tailored for full-pipeline AutoML, i.e., from data retrieval to model deployment. *AutoML-Agent* takes user's task descriptions, facilitates collaboration between specialized LLM agents, and delivers deployment-ready models. Unlike existing work, instead of devising a single plan, we introduce a retrieval-augmented planning strategy to enhance exploration to search for more optimal plans. We also decompose each plan into subtasks (e.g., data preprocessing and neural network design) each of which is solved by a specialized agent we build via prompting executing in parallel, making the search process more efficient. Moreover, we propose a multi-stage verification to verify executed results and guide the code generation LLM in implementing successful solutions. Extensive experiments on seven downstream tasks using fourteen datasets show that *AutoML-Agent* achieves a higher success rate in automating the full AutoML process, yielding systems with good performance throughout the diverse domains.\n\n### 1. Introduction\n\nAutomated machine learning (AutoML) has significantly reduced the need for technical expertise and human labors in developing effective data-driven solutions by automating each process in the AI development pipeline [\\(Yao et al.,](#page-11-0) [2018;](#page-11-0) [Ren et al.,](#page-11-1) [2020;](#page-11-1) [He et al.,](#page-9-0) [2021\\)](#page-9-0), such as feature engineering, model selection, and hyperparameter optimization (HPO). However, current AutoML systems [\\(Gijsbers](#page-9-1) [et al.,](#page-9-1) [2024\\)](#page-9-1) often necessitate programming expertise to configure complex tools and resources, potentially creating barriers for a larger pool of users with limited skills and knowledge, such as domain experts [\\(Sun et al.](#page-11-2) [\\(2023\\)](#page-11-2); [§A\\)](#page-12-0).\n\nTo make AutoML frameworks more accessible to non-expert users, many recent studies [\\(Trirat et al.,](#page-11-3) [2021;](#page-11-3) [Viswanathan](#page-11-4) [et al.,](#page-11-4) [2023;](#page-11-4) [Li et al.,](#page-10-0) [2023;](#page-10-0) [Hollmann et al.,](#page-10-1) [2023b;](#page-10-1) [Liu](#page-10-2) [et al.,](#page-10-2) [2025;](#page-10-2) [Zhang et al.,](#page-11-5) [2023;](#page-11-5) [Shen et al.,](#page-11-6) [2023;](#page-11-6) [Zhang](#page-11-7) [et al.,](#page-11-7) [2024a;](#page-11-7) [Hong et al.,](#page-10-3) [2024a;](#page-10-3) [Guo et al.,](#page-9-2) [2024a;](#page-9-2) [Yang](#page-11-8) [et al.,](#page-11-8) [2025;](#page-11-8) [Chi et al.,](#page-9-3) [2024\\)](#page-9-3) have suggested to use natural language interfaces with large language models (LLM) for machine learning (ML) and data science (DS) tasks. Nevertheless, these previous LLM-based AutoML frameworks only considered a limited number of tasks due to their restricted designs, either only for a process in the pipeline (e.g., feature engineering [\\(Hollmann et al.,](#page-10-1) [2023b;](#page-10-1) [Li et al.,](#page-10-4) [2024;](#page-10-4) [Malberg et al.,](#page-10-5) [2024\\)](#page-10-5), HPO [\\(Liu et al.,](#page-10-6) [2024a;](#page-10-6) [2025;](#page-10-2) [Zhang](#page-11-7) [et al.,](#page-11-7) [2024a\\)](#page-11-7), and model selection [\\(Zhang et al.,](#page-11-5) [2023;](#page-11-5) [Shen](#page-11-6) [et al.,](#page-11-6) [2023\\)](#page-11-6)) or for a specific group of downstream tasks (e.g., natural language processing [\\(Viswanathan et al.,](#page-11-4) [2023\\)](#page-11-4) and computer vision [\\(Yang et al.,](#page-11-8) [2025\\)](#page-11-8)). In addition, most methods overlook the inherent capability of LLMs to search for promising models by performing actual training of the candidate models during the search process, making it prohibitively costly and slow.\n\nFor an AutoML framework to be truly practical, it should perform end-to-end AutoML, considering both the data aspects (retrieval, preprocessing, and feature engineering) and model aspects (selection, HPO, and deployment). This is because a process in one aspect can affect subsequent processes in the other, potentially leading to suboptimal solutions when combining results from different frameworks. Meanwhile, the AutoML framework should be computationally efficient, using strategies to minimize the computational overhead during search. However, there are two main challenges in building such a framework.\n\n<sup>1</sup>DeepAuto.ai <sup>2</sup>KAIST, Seoul, South Korea. Correspondence to: Sung Ju Hwang <sjhwang@deepauto.ai>.\n\n*Proceedings of the* 42 nd *International Conference on Machine Learning*, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s).\n\n<span id=\"page-1-1\"></span>High Complexity of Planning Tasks The planning of the entire AutoML pipeline introduces extra complexities compared to task- or problem-specific planning, primarily due to the inter-dependencies among the steps in the pipeline. For example, types of retrieved datasets affects how to design preprocessing steps and neural networks. Then, the designed network affects which particular hyper-parameters need to be optimized depending on the given downstream task. Such inter-step dependencies result in the enlarged search space since it should consider all possible combinations of interrelated steps. Besides, enabling the framework to operate across various downstream tasks exacerbates these challenges, as each has task-specific requirements.\n\nChallenges in Accurate Implementations To develop a modular and extendable framework that effectively handles diverse ML tasks, it is crucial to enhance the flexibility of the LLM agent in its code generation ability, such as by decoupling the template code from the code for specific datasets. However, using LLMs to autonomously generate complete ML pipelines may lead to hallucination issues, including code incompletion, incorrect or missing dependencies, and potential undiscovered bugs [\\(Hong et al.,](#page-10-7) [2024b\\)](#page-10-7). Furthermore, LLMs often struggle with code generation when prompted with ambiguous task descriptions. Thus, we need accurate analysis of the requirements, and a codegeneration platform that can adaptively generate code based on disambiguated requirements.\n\nTo address these challenges, we propose a novel multi-agent framework, *AutoML-Agent*, for full-pipeline AutoML from data and model search to evaluation, with strategies to tackle the complexity of the planning problem as well as accurate implementation of code. As illustrated in [Figure 1,](#page-1-0) *AutoML-Agent* accepts a user's task description and coordinates multiple specialized agents to collaboratively identify an optimal ML pipeline, ultimately delivering a deploymentready model and its inference endpoint as the output.\n\nSpecifically, to tackle the complex planning problem, we introduce a new *retrieval-augmented planning* strategy equipped with role-specific decomposition and promptingbased execution. This strategy produces multiple plans based on retrieved knowledge for a given task description, facilitating the exploration of promising plans. Moreover, it enables LLM agents to discern global (pipeline-level) and local (process-level) relationships among steps through plan decomposition, which helps them focus on their immediate sub-tasks while aligning with the user's goal. The retrievalaugmented component also simplifies extending LLMs to various downstream tasks using relevant knowledge. The prompting-based execution enhances search efficiency by exploiting LLMs' in-context learning capabilities without any further training, which could introduce additional cost. To enhance the accuracy of the implementation, we adopt structure-based prompt parsing that extracts ML-relevant\n\n![](_page_1_Figure_5.jpeg)\n\n<span id=\"page-1-0\"></span>Figure 1. *AutoML-Agent* receives user's instructions and delivers optimized deployable models.\n\nrequirements from the user's description and *multi-stage verification* that provides feedback between each step in the framework to ensure the quality of instructions when guiding the LLM for code generation. These modules aim to improve the correctness and clarity of the task description for code implementation. Our contributions are as follows.\n\n- We propose a novel multi-agent LLM framework for AutoML, designed to automate the entire AI development pipeline. To the best of our knowledge, this is the first attempt to employ LLMs in a task-agnostic AutoML framework that spans from data retrieval to model deployment.\n- We address the challenges due to the complexity of the planning problem in full-pipeline AutoML by introducing retrieval-augmented planning with role-specific plan decomposition and prompting-based plan execution, enhancing the flexibility and efficiency of the search process.\n- To enhance the accuracy of our full-pipeline implementation, we integrate structure-based prompt parsing and multi-stage verification to ensure the quality of resulting solutions and instructions prior to actual code implementation, thereby improving overall performance.\n- We demonstrate the superiority of the proposed *AutoML-Agent* framework through extensive experiments on seven downstream tasks using fourteen datasets.\n- We have made the source code available at [https://](https://github.com/deepauto-ai/automl-agent) [github.com/deepauto-ai/automl-agent](https://github.com/deepauto-ai/automl-agent).\n\n### 2. Related Work\n\nAutoML is a transformative approach for optimizing ML workflows, enabling both practitioners and researchers to efficiently design models and preprocessing pipelines with minimal manual intervention [\\(Ren et al.,](#page-11-1) [2020;](#page-11-1) [He et al.,](#page-9-0) [2021;](#page-9-0) [Gijsbers et al.,](#page-9-1) [2024\\)](#page-9-1). Despite several advancements in AutoML [\\(Jin et al.,](#page-10-8) [2019;](#page-10-8) [Feurer et al.,](#page-9-4) [2022;](#page-9-4) [Tang et al.,](#page-11-9) [2024\\)](#page-11-9), most of them are designed only for particular elements of the ML pipeline. Only a few works [\\(Bisong,](#page-9-5) [2019;](#page-9-5) [Mukunthu et al.,](#page-11-10) [2019;](#page-11-10) [Microsoft,](#page-10-9) [2021\\)](#page-10-9) support multiple\n\nAutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML\n\n<span id=\"page-2-1\"></span><span id=\"page-2-0\"></span>\n\n| Framework                               | Key Functionality |              |               |               |                      |                |  |  |\n|-----------------------------------------|-------------------|--------------|---------------|---------------|----------------------|----------------|--|--|\n|                                         | Planning          | Verification | Full Pipeline | Task-Agnostic | Training-Free Search | With Retrieval |  |  |\n| AutoML-GPT (Zhang et al., 2023)         | ×                 | ×            | ×             | ✓             | ✓                    | ×              |  |  |\n| Prompt2Model (Viswanathan et al., 2023) | ×                 | ×            | ✓             | ×             | ×                    | ✓              |  |  |\n| HuggingGPT (Shen et al., 2023)          | ✓                 | ×            | ×             | ✓             | ✓                    | ✓              |  |  |\n| CAAFE (Hollmann et al., 2023b)          | ×                 | ✓            | ×             | ×             | ×                    | ×              |  |  |\n| MLCopilot (Zhang et al., 2024a)         | ×                 | ×            | ×             | ✓             | ✓                    | ×              |  |  |\n| AgentHPO (Liu et al., 2025)             | ✓                 | ✓            | ×             | ✓             | ×                    | ×              |  |  |\n| Data Interpreter (Hong et al., 2024a)   | ✓                 | ✓            | ×             | ✓             | ×                    | ×              |  |  |\n| DS-Agent (Guo et al., 2024a)            | ✓                 | ✓            | ×             | ✓             | ×                    | ✓              |  |  |\n| SELA (Chi et al., 2024)                 | ✓                 | ✓            | ×             | ✓             | ×                    | ×              |  |  |\n| Agent K (Grosnit et al., 2024)          | ✓                 | ✓            | ×             | ✓             | ×                    | ✓              |  |  |\n| AutoMMLab (Yang et al., 2025)           | ×                 | ✓            | ✓             | ×             | ×                    | ×              |  |  |\n| AutoML-Agent (Ours)                     | ✓                 | ✓            | ✓             | ✓             | ✓                    | ✓              |  |  |\n\nTable 1. Comparison between *AutoML-Agent* and existing LLM-based frameworks.\n\nsteps of the pipeline. Also, due to the traditional programming interfaces, these systems often have complex configuration procedures and steep learning curves that require substantial coding expertise and an understanding of the underlying ML concepts, limiting their accessibility to nonexperts and being time-consuming even for experienced users. These limitations hinder the widespread adoption of traditional AutoML systems.\n\nLLMs (e.g., GPT-4 [\\(Achiam et al.,](#page-9-7) [2023\\)](#page-9-7)) have recently shown promise in addressing these limitations with the complex problem-solving skills across disciplines via humanfriendly language interfaces, including AI problems [\\(Xi](#page-11-11) [et al.,](#page-11-11) [2025;](#page-11-11) [Narayanan et al.,](#page-11-12) [2024;](#page-11-12) [Fourney et al.,](#page-9-8) [2024;](#page-9-8) [Tang et al.,](#page-11-13) [2025;](#page-11-13) [Kon et al.,](#page-10-10) [2025\\)](#page-10-10). This shift towards natural language-driven interfaces democratizes access and allows users to articulate their needs in a more intuitive manner [\\(Tornede et al.,](#page-11-14) [2024\\)](#page-11-14). Nevertheless, existing LLMbased frameworks can only assist in a specific step of the ML pipeline, such as feature engineering [\\(Hollmann et al.,](#page-10-1) [2023b\\)](#page-10-1), data analysis [\\(Hu et al.,](#page-10-11) [2024b\\)](#page-10-11), model search [\\(Hong](#page-10-3) [et al.,](#page-10-3) [2024a;](#page-10-3) [Guo et al.,](#page-9-2) [2024a;](#page-9-2) [Chi et al.,](#page-9-3) [2024\\)](#page-9-3), or HPO [\\(Liu et al.,](#page-10-6) [2024a;](#page-10-6) [2025;](#page-10-2) [Zhang et al.,](#page-11-7) [2024a\\)](#page-11-7). For example, Agent K [\\(Grosnit et al.,](#page-9-6) [2024\\)](#page-9-6) leverages LLMs to orchestrate ML pipelines; however, it is tailored for Kaggle competition settings and relies on a training-based approach with high search overhead. A few attempts [\\(Viswanathan](#page-11-4) [et al.,](#page-11-4) [2023;](#page-11-4) [Yang et al.,](#page-11-8) [2025\\)](#page-11-8) support the entire ML production pipeline, yet only for a specific type of downstream tasks. Besides, these methods either naively use the LLMs or overlook the inherent capabilities, making their search processes time-consuming for the AutoML pipeline that requires sophisticated planning and verification.\n\nIn contrast, we overcome these limitations by incorporating a new retrieval-augmented planning strategy, coupled with plan decomposition and prompting-based execution techniques, alongside structure-based prompt parsing and multi-stage verification. [Table 1](#page-2-0) summarizes the key differences between *AutoML-Agent* and existing frameworks.\n\n### 3. AutoML-Agent\n\nThis section presents details of the proposed multi-agent framework, *AutoML-Agent*, including agent specifications, a prompt parsing module, a retrieval-augmented planning strategy, a prompting-based plan execution, and a multistage verification. As depicted in [Figure 2,](#page-3-0) all agents are coordinated by an Agent Manager to complete the user's instructions by delivering the deployment-ready model.\n\n#### 3.1. Agent Specifications\n\nWe now provide brief descriptions of the agents in our multiagent AutoML framework.\n\nAgent Manager (Amgr) acts as the core interface between users and other specialized LLM agents within the framework, orchestrating the search process. It is responsible for interacting with the user, devising a set of global plans for subsequent processes with retrieved knowledge, distributing tasks to corresponding agents, verifying executed results with feedback, and tracking the system progress.\n\nPrompt Agent(Ap) is an LLM specifically instructiontuned for parsing the user's instructions into a standardized JSON object with predefined keys. The parsed information is then shared across agents in the framework during the planning, searching, and verification phases.\n\nData Agent(Ad) is an LLM prompted for doing tasks related to data manipulation and analysis. The analysis results from the Data Agent are used to inform the Model Agent about data characteristics during the model search and HPO.\n\nModel Agent(Am) is an LLM prompted for doing tasks related to model search, HPO, model profiling, and candidate ranking. The results produced by the Model Agent are sent back to the Agent Manager for verification before proceeding to the Operation Agent.\n\nOperation Agent (Ao) is an LLM prompted for implementing the solution found by the Data and Model Agents that passes the Agent Manager's verification. The Operation\n\n<span id=\"page-3-2\"></span>![](_page_3_Figure_1.jpeg)\n\nFigure 2. Overview of our *AutoML-Agent* framework. (1) Initialization stage aims to receive a valid user instruction using request verification. (2) Planning stage focuses on extracting ML related information by parsing the user instruction into a standardized form, and uses it to devise plans accordingly. (3) Execution stage executes each action given by the devised plans. Finally, based on the best execution results, *AutoML-Agent* outputs codes containing deployable model to the user.\n\nAgent is responsible for writing effective code for actual runtime execution and recording the execution results for final verification before returning the model to the user.\n\nAfter we define all agents with their corresponding profiles as described above (see [§C.1](#page-20-0) for the complete prompts), the Amgr then assigns relevant tasks to each agent according to the user's task descriptions. Note that we can implement A<sup>d</sup> and A<sup>m</sup> with more than one agent per task based on the degree of parallelism.\n\n#### 3.2. Framework Overview\n\nWe present an overview of our *AutoML-Agent* framework in [Figure 2](#page-3-0) and Algorithm [1.](#page-4-0) In the (1) initialization stage, the Agent Manager (Amgr) receives the user instruction and checks its validity through request verification [\\(Figure 2\\(](#page-3-0)c) and Line 3). In the (2) planning stage, the Prompt Agent (Ap) parses the verified user instruction into a standardized JSON object. Then, Amgr generates plans to solve the given AutoML task using retrieval-augmented planning [\\(Fig](#page-3-0)[ure 2\\(](#page-3-0)a) and Line 11). In the (3) execution stage, the Data (Ad) and Model (Am) Agents decompose these plans and execute them via plan decomposition (PD) and promptingbased plan execution [\\(Figure 2\\(](#page-3-0)b) and Line 13–16), whose results are then verified against the user's requirements via execution verification [\\(Figure 2\\(](#page-3-0)c) and Line 20). Finally, Amgr selects the best plan and sends it to the Operation Agent (Ao) to write code (Line 22). After code generation, implementation verification [\\(Figure 2\\(](#page-3-0)c) and Line 24) is conducted to ensure that the code is deployment-ready. If any of the verification steps fail, *AutoML-Agent* performs revision steps (orange lines in [Figure 2\\)](#page-3-0) to generate new solutions. <span id=\"page-3-0\"></span>In the following subsections, we provide the descriptions of each step more in detail.\n\n#### <span id=\"page-3-1\"></span>3.3. Instruction Data Generation and Prompt Parsing\n\nData Generation For A<sup>p</sup> to generate accurate JSON objects, we need to instruction-tune the LLM first because it can output a valid JSON object but with incorrect keys that are irrelevant to subsequent processes. Following [Xu et al.](#page-11-15) [\\(2024\\)](#page-11-15), we first manually create a set of high-quality seed instructions then automatically generate a larger instruction dataset D = {(I<sup>i</sup> , Ri)} N <sup>i</sup>=1, having N instruction-response pairs. Here, I<sup>i</sup> is the i-th instruction with the corresponding response R<sup>i</sup> . We use the JSON format substantially extended from [Yang et al.](#page-11-8) [\\(2025\\)](#page-11-8) for response R<sup>i</sup> with the following top-level keys to extract the user's requirement from various aspects of an AutoML pipeline.\n\n- User. The user key represents the user intention (e.g., build, consult, or unclear) of the given instruction and their technical expertise in AI.\n- Problem. The problem key indicates the characteristics and requirements of the given task, including area (e.g., computer vision), downstream task (e.g., image classification), application or business domain, and other constraints like expected accuracy and inference latency.\n- Dataset. The dataset key captures the data characteristics and properties, including data modality, requested preprocessing and augmentation techniques, and data source.\n- Model. The model key captures the expected model characteristics and properties, including model name (e.g., ViT), family (e.g., Transformer), and type (e.g., deep neural networks).\n\n<span id=\"page-4-4\"></span>Algorithm 1 Overall Procedure of *AutoML-Agent*\n\n<span id=\"page-4-0\"></span>\n\n| Initialization: Agent Manager Amgr, instruction-tuned |\n|-------------------------------------------------------|\n| Prompt Agent Ap, Data Agent Ad, Model Agent Am,       |\n| Operation Agent Ao, deployment-ready model M⋆<br>,    |\n| and system state S                                    |\n| Input: User instruction I                             |\n| ∅<br>and M⋆ =<br>1: while S ̸= END<br>do              |\n| if S = INIT<br>then<br>2:                             |\n| F ← Amgr(ReqVer(I)) ▷ run ReqVer (§3.6)<br>3:         |\n| if F = ∅<br>then ▷ check if I is valid<br>4:          |\n| R ← Ap(I) ▷ parse user instruction I (§3.3)<br>5:     |\n| S ← PLAN<br>6:                                        |\n|                                                       |\n\n7: else\n\n8: return F ▷ return feedback F to the user. 9: end if 10: else if S = PLAN then 11: P ← Amgr(RAP(R)) ▷ run RAP ([§3.4\\)](#page-4-1) 12: for p<sup>i</sup> in P do ▷ run PD for A<sup>d</sup> and A<sup>m</sup> ([§3.5\\)](#page-4-2) 13: s d <sup>i</sup> ← PD(R, Ad, pi) 14: O<sup>d</sup> <sup>i</sup> ← Ad(s d i ) ▷ run pseudo data analysis ([§3.5\\)](#page-4-2) 15: s m <sup>i</sup> ← PD(R, Am, p<sup>i</sup> , O<sup>d</sup> i ) 16: O<sup>m</sup> <sup>i</sup> ← Am(s m i ) ▷ run search and HPO ([§3.5\\)](#page-4-2) 17: end for 18: O ← {(O<sup>d</sup> i , O<sup>m</sup> i )} P <sup>i</sup>=1 ▷ combine outcomes ([§3.5\\)](#page-4-2) 19: ▷ run execution verification ([§3.6\\)](#page-5-0) 20: if Amgr(ExecVer(O)) is pass then 21: I <sup>⋆</sup> ← Amgr(O) ▷ get code instructions 22: M<sup>⋆</sup> ← Ao(I ⋆ ) ▷ write code for the best plan 23: ▷ run implementation verification ([§3.6\\)](#page-5-0) 24: if Amgr(ImpVer(M<sup>⋆</sup> )) is pass then\n\n25: S ← END ▷ stop the process 26: end if 27: end if 28: end if 29: end while 30: return M<sup>⋆</sup>\n\n- Knowledge. The knowledge key extracts additional knowledge or insights helpful for solving the given problem directly provided by the user, potentially associated with the expertise level.\n- Service. The service key is relevant to the downstream implementation and deployment. It provides information such as a target device and an inference engine.\n\nPrompt Parsing Then, we can use the generated dataset D to train an LLM and use it as Ap. These standardized keys are important for a better control over the LLM agents' behavior within our framework and necessary for effective communication between agents. Moreover, these keys provide contextual information for generating a high-quality AutoML pipeline from various perspectives. After the instruction tuning, we use the A<sup>p</sup> to parse the user's instructions (or task descriptions) and return the parsed requirements R = Ap(I) to Amgr, as shown in [Figure 3](#page-4-3) and [§D.1.](#page-27-0)\n\n![](_page_4_Picture_9.jpeg)\n\nFigure 3. An example of prompt parsing process of an instructionresponse pair {(Ii, Ri)}.\n\n#### <span id=\"page-4-3\"></span><span id=\"page-4-1\"></span>3.4. Retrieval-Augmented Planning\n\nRecent studies [\\(Guo et al.,](#page-9-9) [2024b;](#page-9-9) [Huang et al.,](#page-10-12) [2024;](#page-10-12) [Mas](#page-10-13)[terman et al.,](#page-10-13) [2024;](#page-10-13) [Zhang et al.,](#page-11-16) [2024b;](#page-11-16) [Hu et al.,](#page-10-14) [2024a\\)](#page-10-14) highlights that effective planning and tool utilization are essential for solving complex problems with LLMs, especially in a multi-agent framework. By bridging two techniques in a single module, we propose a retrieval-augmented planning (RAP) strategy to effectively devise a robust and up-to-date set of diverse plans for the AuotML problems.\n\nLet P = {p1, . . . , p<sup>P</sup> } be a set of plans. Based on past knowledge embedded in the LLM, knowledge retrieved via external APIs (such as arXiv papers), and R, RAP generates P multiple end-to-end plans for the entire AutoML pipeline having different scenario p<sup>i</sup> . This strategy enables *AutoML-Agent* to be aware of newer and better solutions. Specifically, we first use the parsed requirements R to acquire a summary of the relevant knowledge and insights via API calls, including web search and paper summary. Amgr then uses this information to devise P different plans, i.e., P = Amgr(RAP(R)). Note that Amgr devises each plan independently to make the subsequent steps parallelizable. The benefit of this strategy is that it enhances exploration for better solutions while allowing parallelization. Examples of generated plans are provided in [§D.2.](#page-29-0)\n\n#### <span id=\"page-4-2\"></span>3.5. Prompting-Based Plan Execution\n\nGiven the generated P, we now describe how A<sup>d</sup> and A<sup>m</sup> execute each p<sup>i</sup> using prompting techniques without actually executing the code. Examples are presented in [§D.4.](#page-35-0)\n\nPlan Decomposition Due to the high complexity of the end-to-end plan, we first need to adaptively decompose the original plan p<sup>i</sup> ∈ P into a smaller set of sub-tasks s<sup>i</sup> relevant to the agent's roles and expertise to increase the effectiveness of LLMs in solving and executing the given plan [\\(Khot et al.,](#page-10-15) [2023\\)](#page-10-15). The plan decomposition (PD) process involves querying the agents about their understanding of the given plan specific to their roles. Formally, s d <sup>i</sup> = PD(R, Ad, pi), where s d i is the *decomposed* plan for Data Agent, containing sub-tasks for the given plan p<sup>i</sup> . Then, the agent executes the decomposed plan towards the user's requirements instead of the original lengthy plan. We define the sub-tasks s m i of A<sup>m</sup> below due to its reliance on Data Agent's outcomes. Examples are presented in [§D.3.](#page-33-0)\n\n<span id=\"page-5-2\"></span>Pseudo Data Analysis In *AutoML-Agent*, A<sup>d</sup> handles subtasks in s d i , including the retrieval, preprocessing, augmentation, and analysis of the specified dataset. During the data retrieval phase, if the dataset is not directly supplied by the user, we initiate an API call to search for potential datasets in repositories, such as HuggingFace and Kaggle, using the dataset name or description. Upon locating a dataset, we augment the prompt with metadata from the dataset's source; if no dataset is found, we rely on the inherent knowledge of the LLM.[1](#page-5-1) We then prompt A<sup>d</sup> to proceed by acting *as if* it actually executes s d i , according to the dataset characteristics and user requirements R. The summarized outcomes of these sub-tasks, O<sup>d</sup> i , are then forwarded to the Am. Hence, O<sup>d</sup> <sup>i</sup> = Ad(s d i ).\n\nTraining-Free Model Search and HPO Like Ad, A<sup>m</sup> uses API calls to complete all sub-tasks s m i , instead of direct code execution. However, in contrast to Ad, the plan decomposition for A<sup>m</sup> incorporates outcomes from the Ad, enabling it to recognize characteristics of the preprocessed dataset, i.e., s m <sup>i</sup> = PD(R, Am, p<sup>i</sup> , O<sup>d</sup> i ). Here, the Am's prompt is enhanced with insights gathered by Amgr about high-performing models and relevant hyperparameters for the specific ML task. This technique allows the A<sup>m</sup> to execute the sub-tasks in s m <sup>i</sup> more efficiently. Using this augmented prompt, the A<sup>m</sup> follows a similar procedure to Ad, undertaking model retrieval, running HPO, and summarizing the results of these sub-tasks, which include expected numerical performance metrics such as accuracy and error, as well as model complexity factors like model size and inference time. To facilitate the subsequent verification step, we also prompt the agent to return results with the top-k most promising models. Formally, O<sup>m</sup> <sup>i</sup> = Am(s m i ).\n\nPlan Implementation To enhance the efficacy of A<sup>o</sup> in code generation, Amgr first verifies all executed results O = {(O<sup>d</sup> i , O<sup>m</sup> i )} P <sup>i</sup>=1 from A<sup>d</sup> and Am. Amgr then selects the best outcome O<sup>⋆</sup> ∈ O and generates the instruction I ⋆ for A<sup>o</sup> to write the actual code accordingly. Formally, M<sup>⋆</sup> = Ao(I ⋆ ), where M<sup>⋆</sup> is the deployment-ready model.\n\n#### <span id=\"page-5-0\"></span>3.6. Multi-Stage Verification\n\nVerification, especially with refinement or feedback, is essential for maintaining the correct trajectory of LLMs [\\(Baek](#page-9-10) [et al.,](#page-9-10) [2025;](#page-9-10) [Madaan et al.,](#page-10-16) [2023;](#page-10-16) [Gou et al.,](#page-9-11) [2024\\)](#page-9-11). Our framework incorporates three verification steps to guarantee its accuracy and effectiveness: request verification, execution verification, and implementation verification.\n\nRequest Verification (ReqVer) Initially, we assess the clarity of the user's instructions to determine if they are relevant and adequate for executing ML tasks and addressing the user's objectives. If the instructions prove insufficient for progressing to the planning stage, Amgr will request additional information, facilitating multi-turn communication. This request verification step, however, often overlooked in existing studies, placing an undue burden on users to formulate a more detailed initial prompt—a challenging task particularly for those who are non-experts or lack experience. Prompts for ReqVer are shown in [§C.5.1.](#page-26-0)\n\nExecution Verification (ExecVer) After executing the plans in [§3.5,](#page-4-2) Amgr then verifies whether any of the pipelines produced by A<sup>d</sup> and A<sup>m</sup> (i.e., O) satisfy the user's requirements via prompting (see [§C.5.2\\)](#page-27-1). If the results are satisfied, the suggested solution is selected as a candidate for implementation. This step effectively mitigates computational overhead in the search process by allocating resources exclusively to the most promising solution.\n\nImplementation Verification (ImpVer) This implementation verification phase closely resembles the ExecVer; however, it differs in that it involves validating outcomes derived from the code that has been executed and compiled by Ao. We present the prompt for this verification in [§C.5.3.](#page-27-2) If the outcomes meet the user's requirements, Amgr provides the model and deployment endpoint to the user.\n\nNote that if any execution or implementation fails to satisfy the user requirements (i.e., does not pass the verification process), these failures are systematically documented. Subsequently, the system transitions to the plan *revision* stage. During this stage, Amgr formulates a revised set of plans, incorporating insights derived from the outcomes of the unsuccessful plans.\n\n### 4. Experiments\n\nWe validate the effectiveness of our full-pipeline AutoML framework by comparing *AutoML-Agent* with handcrafted models, state-of-the-art AutoML variants, and LLM-based frameworks across multiple downstream tasks involving different data modalities.\n\n#### 4.1. Setup\n\nTasks and Datasets As summarized in [Table 2,](#page-6-0) we select seven downstream tasks from five different data modalities, including image, text, tabular, graph, and time series. These datasets are chosen from different sources. Also, we incorporate various evaluation metrics for these tasks, e.g., accuracy for classification and root mean squared log error (RMSLE) for regression.\n\nFor each task, we prepare *two* sets of natural language task descriptions to represent *constraint-aware* and *constraint-*\n\n<span id=\"page-5-1\"></span><sup>1</sup>When we mention relying on the LLM's inherent knowledge, we are referring to a fallback mechanism intended to produce the most plausible output given limited context. However, in the absence of actual data, the pipeline will ultimately fail during final implementation verification due to runtime errors.\n\n<span id=\"page-6-1\"></span><span id=\"page-6-0\"></span>\n\n| Data Modality              | Downstream Task                                        | Dataset Name                         | Evaluation Metric |\n|----------------------------|--------------------------------------------------------|--------------------------------------|-------------------|\n| Image<br>(Computer Vision) | Image Classification                                   | Butterfly Image<br>Shopee-IET        | Accuracy          |\n| Text<br>(NLP)              | Text Classification                                    | Ecommerce Text<br>Textual Entailment | Accuracy          |\n|                            | Tabular Classification                                 | Banana Quality<br>Software Defects   | F1                |\n| Tabular<br>(Classic ML)    | Tabular Regression                                     | Crab Age<br>Crop Price               | RMSLE             |\n|                            | Tabular Clustering                                     | Smoker Status<br>Student Performance | RI                |\n| Time Series                | (Time Series Analysis) Time-Series Forecasting Weather | Electricity                          | RMSLE             |\n| Graph<br>(Graph Mining)    | Node Classification                                    | Cora<br>Citeseer                     | Accuracy          |\n\n*free* requirements (see §[B\\)](#page-13-0) along with a full-pipeline skeleton script. As a result, we extensively evaluate 28 generated models. Note that this setting differs from previous studies [\\(Guo et al.,](#page-9-2) [2024a;](#page-9-2) [Huang et al.,](#page-10-17) [2023\\)](#page-10-17), which require dataset-specific, partially completed code preparation.\n\nEvaluation Metrics For a comprehensive evaluation, we measure the agent's effectiveness in both code generation and task-specific performance aspects by using *comprehensive score* (CS) [\\(Hong et al.,](#page-10-3) [2024a\\)](#page-10-3) to simultaneously evaluate both the success rate (SR) of code generation and the normalized performance score (NPS) of the built pipelines. That is, CS = 0.5 × SR + 0.5 × NPS. Here, NPS = 1 1+s is a transformation of loss-based performance score s, e.g., RMSLE. More detailed explanations are included in [§B.4.](#page-19-0)\n\nAs described above, we evaluate all frameworks under two different settings. To measure SR of each method, we use a grading scale ranging from 0 for total failure to 1 for perfect conformity to the user's requirements. For the *constraintfree* setting, a method can get a score of 0.5 (pass modeling) or 1.0 (pass deployment). For the *constraint-aware* setting, a method can get a score of 0.25 (pass modeling), 0.5 (pass deployment), 0.75 (partially pass the constraints), or 1.0 (pass all cases).\n\nBaselines As we propose a framework for the novel task of full-pipeline AutoML with LLMs, there is no direct baseline available for comparison. We thus compare *AutoML-Agent* against the task-specific manually designed models (see [§B.3\\)](#page-18-0): Human Models, the variants of state-of-the-art AutoML: AutoGluon [\\(Erickson et al.,](#page-9-12) [2020;](#page-9-12) [Shchur et al.,](#page-11-17) [2023;](#page-11-17) [Tang et al.,](#page-11-9) [2024\\)](#page-11-9), a state-the-of-art LLM for data science: DS-Agent [\\(Guo et al.,](#page-9-2) [2024a\\)](#page-9-2), and general-purpose LLMs: GPT-3.5 [\\(Brown et al.,](#page-9-13) [2020\\)](#page-9-13) and GPT-4 [\\(Achiam](#page-9-7) [et al.,](#page-9-7) [2023\\)](#page-9-7) with zero-shot prompting.\n\nImplementation Details Except for the A<sup>p</sup> that is implemented with Mixtral-8x7B (*Mixtral-8x7B-Instructv0.1*) [\\(Jiang et al.,](#page-10-18) [2024\\)](#page-10-18), we use GPT-4 (*gpt-4o-2024-05- 13*) as the backbone model for all agents and LLM-based baselines to ensure an impartial performance evaluation. To instruction tune the A<sup>p</sup> ([§3.3\\)](#page-3-1), we automatically generate about 2.3K instruction-response pairs using EvolInstruct [\\(Xu et al.,](#page-11-15) [2024\\)](#page-11-15). Here, we use LoRA [\\(Hu et al.,](#page-10-19) [2021\\)](#page-10-19) to fine-tune the model with the generated dataset. For RAP ([§3.4\\)](#page-4-1), we set the number of plans P = 3 and the number of candidate models k = 3. All experiments are conducted on an Ubuntu 22.04 LTS server equipped with eight NVIDIA A100 GPUs (CUDA 12.4) and Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz. For running the generated models, we employ the same execution environment as DS-Agent [\\(Guo et al.,](#page-9-2) [2024a\\)](#page-9-2), with all necessary libraries included in the skeleton scripts.\n\n#### 4.2. Main Results\n\nWe report the average scores from five independent runs for all evaluation metrics in [Figure 4.](#page-7-0)\n\nSuccess Rate Figure [4\\(](#page-7-0)left) and Table [6](#page-44-0) present the results for the SR metric. For the constraint-free cases, which can be considered easier tasks, all methods have higher SR than ones in the constraint-aware setting. Notably, *AutoML-Agent* also consistently outperforms the baselines in the constraint-aware setting, achieving an average SR of 87.1%, which underscores the effectiveness of our framework. We conjecture that the knowledge retrieved during the planning process helps the agents identify which areas to focus on in order to meet the given constraints. Regarding DS-Agent, although we use the provided example cases for the relevant tasks, it appears to fail on certain tasks due to its heavy reliance on curated case banks and the inclusion of partially completed code, which is unavailable in our setting.\n\nDownstream Performance We present the performance comparison between the successfully built models in Figure [4\\(](#page-7-0)center) and Table [7.](#page-45-0) To ensure meaningful results and to examine how the performance of LLM-generated models compares to state-of-the-art AutoML techniques and manual ML pipelines crafted by experienced experts, we select top-performing models by evaluating results reported in Papers with Code benchmarks and Kaggle notebooks for the same tasks and datasets, where applicable, as the Human Models baselines. From the results, we can observe that *AutoML-Agent* significantly outperforms other agents, including Human Models, in the NPS metric. In particular, *AutoML-Agent* achieves the best performance across all tasks under the constraint-aware setting. These findings highlight the superiority of *AutoML-Agent* in adapting to various scenarios, attributed to the retrieval-augmented planning (RAP) strategy. This approach enables agents to discover effective pipelines for given constraints. These empirical observations substantiate the efficacy of the proposed RAP, providing up-to-date solutions for various tasks.\n\nComprehensive Score [Figure 4\\(](#page-7-0)right) and [Table 8](#page-45-1) present the weighted quality of each agent based on the CS metric. Overall, *AutoML-Agent* outperforms all other baselines,\n\n<span id=\"page-7-3\"></span>![](_page_7_Figure_1.jpeg)\n\nFigure 4. Performance comparison across all datasets using the SR, NPS, and CS metrics under (a) constraint-free and (b) constraint-aware settings. Higher scores indicate better results.\n\nespecially in more complicated tasks. Interestingly, it is evident that general-purpose LLMs still works relatively well on classical tasks like tabular classification and regression, while more sophisticated methods, such as DS-Agent and our *AutoML-Agent* work significantly better in complex tasks. This finding aligns with previous research [\\(Guo](#page-9-2) [et al.,](#page-9-2) [2024a\\)](#page-9-2), which suggests that tabular tasks typically involve straightforward function calls from the sklearn library [\\(Pedregosa et al.,](#page-11-18) [2011\\)](#page-11-18), and therefore do not demand advanced reasoning or coding abilities from LLM agents, unlike more complex tasks.\n\n#### <span id=\"page-7-4\"></span>4.3. Additional Analysis\n\nAblation Study To validate the effectiveness of each component in *AutoML-Agent*, we conduct the following ablation studies. The results are presented in Figure [5a](#page-7-1) and [Table 9.](#page-45-2) First, we investigate *retrieval-augmented planning (RAP)* alone, where retrieved knowledge from external APIs is directly used without plan decomposition and verification. As expected, this ablation leads to a decline in performance, and in some cases, even fails to generate a runnable model. This outcome highlights the importance of the decomposition and verification modules. Second, we evaluate *RAP with plan decomposition*, where the plan is decomposed for each specific agent. While this variant demonstrates better downstream performance, it still fails to produce runnable models in certain downstream tasks due to the lack of code verification. Finally, we assess the *full framework with multi-stage*\n\n<span id=\"page-7-1\"></span><span id=\"page-7-0\"></span>![](_page_7_Figure_7.jpeg)\n\n<span id=\"page-7-2\"></span>Figure 5. Results of (a) ablation study and (b) hyperparameter study in the CS metric.\n\n*verification*, which provides feedback to agents, thereby enhancing both their planning and coding capabilities. Integrating all components significantly empowers LLM agents to effectively incorporate external knowledge from various sources to build a full-pipeline AutoML system.\n\nHyperparameter Study To further verify the effectiveness of devising multiple plans in our retrieval-augmented planning strategy ([§3.4\\)](#page-4-1), we conduct a hyperparameter study by varying the number of plans P in the constraint-free setting. As shown in Figure [5b](#page-7-2) and [Table 10,](#page-46-0) the number of plans does not significantly affect the success rate, likely due to GPT-4's robust planning capabilities. However, based on the NPS and CS metrics, we observe that the number of plans has a notable impact on downstream task performance. Also, these results also suggest that adding more plans does not necessarily lead to better results, as the model may generate multiple similar plans, resulting in similar outcomes. Consequently, we select 3 as the default number of plans.\n\n<span id=\"page-8-4\"></span><span id=\"page-8-0\"></span>![](_page_8_Figure_1.jpeg)\n\nFigure 6. Results of (a) sensitivity analysis, (b) robustness analysis, and (c) comparison with SELA [\\(Chi et al.,](#page-9-3) [2024\\)](#page-9-3) in the CS metric.\n\nPrompt Sensitivity To understand how different levels of system prompt affect the framework performance, we test five prompt variations for each agent, differing in tone and task specificity (see [§C.2\\)](#page-21-0). Overall, the experimental results in Figure [6a](#page-8-0) show that agents are not highly sensitive to exact phrasing *as long as their roles were clearly defined*, which is also reinforced through the user prompts—making the framework robust to variations in system prompts.\n\nNoise Robustness Since the framework can encounter noisy information from external knowledge sources or APIs during the RAP process, we further evaluate the robustness of *AutoML-Agent* to such noise under two simulated scenarios. First, in the pre-summary injection scenario, unrelated or low-quality examples are injected *before* insight extraction and planning. Second, in the post-summary injection scenario, noisy examples are injected *after* insight extraction but before planning. That is, they are mixed with useful insights. Here, the noisy inputs are generated by an additional adversarial agent prompted to produce \"unhelpful\" and \"fake\" insights intended to disrupt the planning process.\n\nThe results in Figure [6b](#page-8-1) demonstrate that *AutoML-Agent* is robust to such noise. Its built-in error correction and multi-stage verification mechanisms significantly mitigate the impact of noisy inputs, ensuring that final model performance *remains largely unaffected*. Interestingly, we observe that in some cases, noise injection can even improve performance. We conjecture that this may be because the Agent Manager is implicitly prompted to more effectively distinguish between useful and irrelevant information.\n\nComparison with Training-Based Search To examine the effectiveness of our RAP strategy ([§3.4\\)](#page-4-1) in acquiring useful knowledge for planning and prompting-based plan execution ([§3.5\\)](#page-4-2) in enhancing search speed, we compare our *AutoML-Agent* with the recent Monte Carlo tree searchbased SELA method (10 rollouts) using six datasets. These datasets represent the easiest and hardest tasks, as identified by SELA's results. Figure [6c](#page-8-2) and [Table 12](#page-46-1) show that *AutoML-Agent* achieves search times about 8x faster than SELA, highlighting significant computational efficiency in line with our focus on practical applicability. Despite this ef-\n\n<span id=\"page-8-5\"></span><span id=\"page-8-2\"></span><span id=\"page-8-1\"></span>![](_page_8_Figure_7.jpeg)\n\n<span id=\"page-8-3\"></span>Figure 7. Average time and monetary cost breakdown.\n\nficiency, our method achieves comparable or superior performance, with an average score of 0.612 compared to SELA's 0.599. While SELA slightly outperforms *AutoML-Agent* on a few datasets, the gains are marginal and come at a substantial computational cost. This trade-off between efficiency and minor performance gains further justifies our decision to avoid computationally expensive training-based search.\n\nResource Cost As we primarily use closed-source LLMs in this paper, we analyze the costs in terms of time and money. [Figure 7](#page-8-3) presents the average time and monetary costs across different tasks and datasets for a single run, under the constraint-free (upper) and constraint-aware (lower) settings. On average, it takes around 525 seconds and costs 0.30 USD (using GPT-4o) to search for a single model that will be deployable after training. The significant amount of time spent in the planning stage also suggests the difficulty in devising plans for full-pipeline AutoML.\n\n### 5. Conclusions\n\nThis paper presents *AutoML-Agent*, a novel LLM-based multi-agent framework designed for AutoML, covering the entire pipeline from data retrieval to model deployment. *AutoML-Agent* tackles the full-pipeline planning complexity and implementation accuracy challenges in the LLMs for task-agnostic AutoML by leveraging the newly proposed retrieval-augmented planning strategy and multi-stage verification. In addition, we enhance the plan execution efficiency by integrating role-specific decomposition and promptingbased execution techniques into the framework. Our experiments on seven ML tasks demonstrate that *AutoML-Agent* outperforms existing methods in terms of success rate and downstream task performance.\n\n### Acknowledgements\n\nWe thank the anonymous reviewers for their insightful comments and suggestions, which helped improve the quality of this paper.\n\n### Impact Statement\n\nWe expect *AutoML-Agent* to offer significant advantages by promoting AI-driven innovation and enabling individuals with limited AI expertise to effectively utilize AI capabilities. However, we acknowledge the potential misuse of *AutoML-Agent* by malicious users, such as generating offensive content, malicious software, or invasive surveillance tools when exposed to harmful inputs. This vulnerability is not unique to our approach but represents a common challenge faced by existing LLMs with substantial creative and reasoning capabilities, which can occasionally produce undesirable outputs. Although we strictly instruct the LLM to focus on generating positive results for machine learning tasks, there is a possibility of unforeseen glitches that could introduce security issues within the system. Therefore, we recommend running *AutoML-Agent* within a Docker container to ensure isolation from the host's file system. Additionally, due to its integration with external services for retrieval-augmented generation and API-based LLMs like GPT-4, privacy concerns may arise. Users should carefully review any data included in API prompts to prevent unintended data disclosures.\n\n### References\n\n- <span id=\"page-9-7\"></span>Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. GPT-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023. [3,](#page-2-1) [7](#page-6-1)\n- <span id=\"page-9-10\"></span>Baek, J., Jauhar, S. K., Cucerzan, S., and Hwang, S. J. ResearchAgent: Iterative research idea generation over scientific literature with large language models. In *Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies*, pp. 6709– 6738, 2025. [6](#page-5-2)\n- <span id=\"page-9-5\"></span>Bisong, E. *Building Machine Learning and Deep Learning Models on Google Cloud Platform*. Springer, 2019. [2](#page-1-1)\n- <span id=\"page-9-13\"></span>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In *NeurIPS*, pp. 1877–1901, 2020. [7](#page-6-1)\n- <span id=\"page-9-3\"></span>Chi, Y., Lin, Y., Hong, S., Pan, D., Fei, Y., Mei, G., Liu, B., Pang, T., Kwok, J., Zhang, C., et al. SELA: Tree-search\n\nenhanced LLM agents for automated machine learning. *arXiv preprint arXiv:2410.17238*, 2024. [1,](#page-0-0) [3,](#page-2-1) [9,](#page-8-4) [20,](#page-19-1) [45](#page-44-1)\n\n- <span id=\"page-9-14\"></span>Davenport, T. and Bean, R. Five key trends in AI and data science for 2024. *MIT Sloan Management Review*, 2024. [13](#page-12-1)\n- <span id=\"page-9-12\"></span>Erickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., and Smola, A. AutoGluon-Tabular: Robust and accurate AutoML for structured data. *arXiv preprint arXiv:2003.06505*, 2020. [7,](#page-6-1) [20](#page-19-1)\n- <span id=\"page-9-4\"></span>Feurer, M., Eggensperger, K., Falkner, S., Lindauer, M., and Hutter, F. Auto-Sklearn 2.0: Hands-free AutoML via meta-learning. *Journal of Machine Learning Research*, 23(261):1–61, 2022. [2](#page-1-1)\n- <span id=\"page-9-15\"></span>Fey, M. and Lenssen, J. E. Fast graph representation learning with PyTorch Geometric. In *ICLR Workshop on Representation Learning on Graphs and Manifolds*, 2019. [19](#page-18-1)\n- <span id=\"page-9-8\"></span>Fourney, A., Bansal, G., Mozannar, H., Tan, C., Salinas, E., Niedtner, F., Proebsting, G., Bassman, G., Gerrits, J., Alber, J., et al. Magentic-One: A generalist multiagent system for solving complex tasks. *arXiv preprint arXiv:2411.04468*, 2024. [3](#page-2-1)\n- <span id=\"page-9-1\"></span>Gijsbers, P., Bueno, M. L., Coors, S., LeDell, E., Poirier, S., Thomas, J., Bischl, B., and Vanschoren, J. AMLB: an AutoML benchmark. *Journal of Machine Learning Research*, 25(101):1–65, 2024. [1,](#page-0-0) [2](#page-1-1)\n- <span id=\"page-9-11\"></span>Gou, Z., Shao, Z., Gong, Y., yelong shen, Yang, Y., Duan, N., and Chen, W. CRITIC: Large language models can self-correct with tool-interactive critiquing. In *ICLR*, 2024. [6](#page-5-2)\n- <span id=\"page-9-6\"></span>Grosnit, A., Maraval, A., Doran, J., Paolo, G., Thomas, A., Beevi, R. S. H. N., Gonzalez, J., Khandelwal, K., Iacobacci, I., Benechehab, A., et al. Large language models orchestrating structured reasoning achieve Kaggle grandmaster level. *arXiv preprint arXiv:2411.03562*, 2024. [3,](#page-2-1) [45](#page-44-1)\n- <span id=\"page-9-2\"></span>Guo, S., Deng, C., Wen, Y., Chen, H., Chang, Y., and Wang, J. DS-Agent: Automated data science by empowering large language models with case-based reasoning. In *ICML*, 2024a. [1,](#page-0-0) [3,](#page-2-1) [7,](#page-6-1) [8,](#page-7-3) [13,](#page-12-1) [18,](#page-17-0) [19,](#page-18-1) [20](#page-19-1)\n- <span id=\"page-9-9\"></span>Guo, T., Chen, X., Wang, Y., Chang, R., Pei, S., Chawla, N. V., Wiest, O., and Zhang, X. Large language model based multi-agents: A survey of progress and challenges. *arXiv preprint arXiv:2402.01680*, 2024b. [5](#page-4-4)\n- <span id=\"page-9-0\"></span>He, X., Zhao, K., and Chu, X. AutoML: A survey of the state-of-the-art. *Knowledge-Based Systems*, 212:106622, 2021. [1,](#page-0-0) [2](#page-1-1)\n- <span id=\"page-10-22\"></span>Hollmann, N., Muller, S., Eggensperger, K., and Hutter, ¨ F. TabPFN: A Transformer that solves small tabular classification problems in a second. In *ICLR*, 2023a. [20](#page-19-1)\n- <span id=\"page-10-1\"></span>Hollmann, N., Muller, S., and Hutter, F. Large language ¨ models for automated data science: Introducing CAAFE for context-aware automated feature engineering. In *NeurIPS*, volume 36, pp. 44753–44775, 2023b. [1,](#page-0-0) [3](#page-2-1)\n- <span id=\"page-10-3\"></span>Hong, S., Lin, Y., Liu, B., Wu, B., Li, D., Chen, J., Zhang, J., Wang, J., Zhang, L., Zhuge, M., et al. Data Interpreter: An LLM agent for data science. *arXiv preprint arXiv:2402.18679*, 2024a. [1,](#page-0-0) [3,](#page-2-1) [7,](#page-6-1) [13,](#page-12-1) [20](#page-19-1)\n- <span id=\"page-10-7\"></span>Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Wang, J., Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., Ran, C., Xiao, L., Wu, C., and Schmidhuber, J. MetaGPT: Meta programming for multi-agent collaborative framework. In *ICLR*, 2024b. [2](#page-1-1)\n- <span id=\"page-10-19\"></span>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*, 2021. [7](#page-6-1)\n- <span id=\"page-10-14\"></span>Hu, M., Mu, Y., Yu, X. C., Ding, M., Wu, S., Shao, W., Chen, Q., Wang, B., Qiao, Y., and Luo, P. Tree-Planner: Efficient close-loop task planning with large language models. In *ICLR*, 2024a. [5](#page-4-4)\n- <span id=\"page-10-11\"></span>Hu, X., Zhao, Z., Wei, S., Chai, Z., Ma, Q., Wang, G., Wang, X., Su, J., Xu, J., Zhu, M., et al. InfiAgent-DABench: Evaluating agents on data analysis tasks. In *ICML*, pp. 19544–19572, 2024b. [3](#page-2-1)\n- <span id=\"page-10-17\"></span>Huang, Q., Vora, J., Liang, P., and Leskovec, J. Benchmarking large language models as AI research agents. *arXiv preprint arXiv:2310.03302*, 2023. [7](#page-6-1)\n- <span id=\"page-10-12\"></span>Huang, X., Liu, W., Chen, X., Wang, X., Wang, H., Lian, D., Wang, Y., Tang, R., and Chen, E. Understanding the planning of LLM agents: A survey. *arXiv preprint arXiv:2402.02716*, 2024. [5](#page-4-4)\n- <span id=\"page-10-18\"></span>Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*, 2024. [7](#page-6-1)\n- <span id=\"page-10-8\"></span>Jin, H., Song, Q., and Hu, X. Auto-Keras: An efficient neural architecture search system. In *KDD*, pp. 1946– 1956, 2019. [2](#page-1-1)\n- <span id=\"page-10-20\"></span>Karmaker, S. K., Hassan, M. M., Smith, M. J., Xu, L., Zhai, C., and Veeramachaneni, K. AutoML to date and beyond: Challenges and opportunities. *ACM Computing Surveys*, 54(8):1–36, 2021. [13](#page-12-1)\n- <span id=\"page-10-15\"></span>Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. Decomposed Prompting: A modular approach for solving complex tasks. In *ICLR*, 2023. [5](#page-4-4)\n- <span id=\"page-10-10\"></span>Kon, P. T. J., Liu, J., Ding, Q., Qiu, Y., Yang, Z., Huang, Y., Srinivasa, J., Lee, M., Chowdhury, M., and Chen, A. Curie: Toward rigorous and automated scientific experimentation with AI agents. *arXiv preprint arXiv:2502.16069*, 2025. [3](#page-2-1)\n- <span id=\"page-10-21\"></span>Lala, J., O'Donoghue, O., Shtedritski, A., Cox, S., Ro- ´ driques, S. G., and White, A. D. PaperQA: Retrievalaugmented generative agent for scientific research. *arXiv preprint arXiv:2312.07559*, 2023. [14](#page-13-1)\n- <span id=\"page-10-4\"></span>Li, D., Tan, Z., and Liu, H. Exploring large language models for feature selection: A data-centric perspective. *arXiv preprint arXiv:2408.12025*, 2024. [1](#page-0-0)\n- <span id=\"page-10-0\"></span>Li, H., Jiang, H., Zhang, T., Yu, Z., Yin, A., Cheng, H., Fu, S., Zhang, Y., and He, W. TrainerAgent: Customizable and efficient model training through LLM-powered multiagent system. *arXiv preprint arXiv:2311.06622*, 2023. [1](#page-0-0)\n- <span id=\"page-10-2\"></span>Liu, S., Gao, C., and Li, Y. AgentHPO: Large language model agent for hyper-parameter optimization. In *The Second Conference on Parsimony and Learning (Proceedings Track)*, 2025. [1,](#page-0-0) [3](#page-2-1)\n- <span id=\"page-10-6\"></span>Liu, T., Astorga, N., Seedat, N., and van der Schaar, M. Large language models to enhance bayesian optimization. In *ICLR*, 2024a. [1,](#page-0-0) [3](#page-2-1)\n- <span id=\"page-10-23\"></span>Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M. iTransformer: Inverted Transformers are effective for time series forecasting. In *ICLR*, 2024b. [20](#page-19-1)\n- <span id=\"page-10-16\"></span>Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., and Clark, P. Self-Refine: Iterative refinement with self-feedback. In *NeurIPS*, 2023. [6](#page-5-2)\n- <span id=\"page-10-5\"></span>Malberg, S., Mosca, E., and Groh, G. FELIX: Automatic and interpretable feature engineering using LLMs. In *ECML/PKDD*, pp. 230–246, 2024. [1](#page-0-0)\n- <span id=\"page-10-13\"></span>Masterman, T., Besen, S., Sawtell, M., and Chao, A. The landscape of emerging AI agent architectures for reasoning, planning, and tool calling: A survey. *arXiv preprint arXiv:2404.11584*, 2024. [5](#page-4-4)\n- <span id=\"page-10-9\"></span>Microsoft. Neural Network Intelligence, 1 2021. URL <https://github.com/microsoft/nni>. [2](#page-1-1)\n- <span id=\"page-11-10\"></span>Mukunthu, D., Shah, P., and Tok, W. *Practical Automated Machine Learning on Azure: Using Azure Machine Learning to Quickly Build AI Solutions*. O'Reilly Media, 2019. ISBN 9781492055549. [2](#page-1-1)\n- <span id=\"page-11-12\"></span>Narayanan, S., Braza, J. D., Griffiths, R.-R., Ponnapati, M., Bou, A., Laurent, J., Kabeli, O., Wellawatte, G., Cox, S., Rodriques, S. G., et al. Aviary: training language agents on challenging scientific tasks. *arXiv preprint arXiv:2412.21154*, 2024. [3](#page-2-1)\n- <span id=\"page-11-18\"></span>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Edouard Duch- ´ esnay. Scikit-Learn: Machine learning in python. *Journal of Machine Learning Research*, 12(85):2825–2830, 2011. [8](#page-7-3)\n- <span id=\"page-11-1\"></span>Ren, P., Xiao, Y., Chang, X., Huang, P.-Y., Li, Z., Chen, X., and Wang, X. A comprehensive survey of neural architecture search: Challenges and solutions. *arXiv preprint arXiv:2006.02903*, 2020. [1,](#page-0-0) [2](#page-1-1)\n- <span id=\"page-11-17\"></span>Shchur, O., Turkmen, A. C., Erickson, N., Shen, H., Shirkov, A., Hu, T., and Wang, B. AutoGluon–TimeSeries: AutoML for probabilistic time series forecasting. In *International Conference on Automated Machine Learning*. PMLR, 2023. [7,](#page-6-1) [20](#page-19-1)\n- <span id=\"page-11-6\"></span>Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face. In *NeurIPS*, volume 36, pp. 38154–38180, 2023. [1,](#page-0-0) [3](#page-2-1)\n- <span id=\"page-11-2\"></span>Sun, Y., Song, Q., Gui, X., Ma, F., and Wang, T. AutoML in the wild: Obstacles, workarounds, and expectations. In *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*, pp. 1–15, 2023. [1,](#page-0-0) [13](#page-12-1)\n- <span id=\"page-11-13\"></span>Tang, J., Fan, T., and Huang, C. AutoAgent: A fully-automated and zero-code framework for LLM agents, 2025. URL [https://arxiv.org/abs/](https://arxiv.org/abs/2502.05957) [2502.05957](https://arxiv.org/abs/2502.05957). [3](#page-2-1)\n- <span id=\"page-11-9\"></span>Tang, Z., Fang, H., Zhou, S., Yang, T., Zhong, Z., Hu, T., Kirchhoff, K., and Karypis, G. AutoGluon-Multimodal (AutoMM): Supercharging multimodal AutoML with foundation models. *arXiv preprint arXiv:2404.16233*, 2024. [2,](#page-1-1) [7,](#page-6-1) [20](#page-19-1)\n- <span id=\"page-11-14\"></span>Tornede, A., Deng, D., Eimer, T., Giovanelli, J., Mohan, A., Ruhkopf, T., Segel, S., Theodorakopoulos, D., Tornede, T., Wachsmuth, H., and Lindauer, M. AutoML in the age of large language models: Current challenges, future opportunities and risks. *Transactions on Machine Learning Research*, 2024. ISSN 2835-8856. [3,](#page-2-1) [13](#page-12-1)\n- <span id=\"page-11-3\"></span>Trirat, P., Shin, Y., Kim, S., and Kim, M. Generating a machine learning model with a few sentences. *Korea Software Congress*, pp. 688–690, 2021. [1](#page-0-0)\n- <span id=\"page-11-4\"></span>Viswanathan, V., Zhao, C., Bertsch, A., Wu, T., and Neubig, G. Prompt2model: Generating deployable models from natural language instructions. In *EMNLP: System Demonstrations*, pp. 413–421, 2023. [1,](#page-0-0) [3](#page-2-1)\n- <span id=\"page-11-11\"></span>Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. *Science China Information Sciences*, 68(2):121101, 2025. [3](#page-2-1)\n- <span id=\"page-11-15\"></span>Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Lin, Q., and Jiang, D. WizardLM: Empowering large pre-trained language models to follow complex instructions. In *ICLR*, 2024. [4,](#page-3-2) [7,](#page-6-1) [13](#page-12-1)\n- <span id=\"page-11-19\"></span>Yang, C., Wu, Q., Wang, J., and Yan, J. Graph neural networks are inherently good generalizers: Insights by bridging GNNs and MLPs. In *ICLR*, 2023. [20](#page-19-1)\n- <span id=\"page-11-8\"></span>Yang, Z., Zeng, W., Jin, S., Qian, C., Luo, P., and Liu, W. AutoMMLab: Automatically generating deployable models from language instructions for computer vision tasks. In *AAAI*, pp. 22056–22064, 2025. [1,](#page-0-0) [3,](#page-2-1) [4](#page-3-2)\n- <span id=\"page-11-0\"></span>Yao, Q., Wang, M., Chen, Y., Dai, W., Li, Y.-F., Tu, W.- W., Yang, Q., and Yu, Y. Taking human out of learning applications: A survey on automated machine learning. *arXiv preprint arXiv:1810.13306*, 2018. [1](#page-0-0)\n- <span id=\"page-11-7\"></span>Zhang, L., Zhang, Y., Ren, K., Li, D., and Yang, Y. MLCopilot: Unleashing the power of large language models in solving machine learning tasks. In *The 18th Conference of the European Chapter of the Association for Computational Linguistics*, 2024a. [1,](#page-0-0) [3](#page-2-1)\n- <span id=\"page-11-5\"></span>Zhang, S., Gong, C., Wu, L., Liu, X., and Zhou, M. AutoML-GPT: Automatic machine learning with GPT. *arXiv preprint arXiv:2305.02499*, 2023. [1,](#page-0-0) [3](#page-2-1)\n- <span id=\"page-11-16\"></span>Zhang, W., Shen, Y., Wu, L., Peng, Q., Wang, J., Zhuang, Y., and Lu, W. Self-Contrast: Better reflection through inconsistent solving perspectives. In *ACL*, pp. 3602–3622, 2024b. [5](#page-4-4)\n\n## <span id=\"page-12-1\"></span>Appendix\n\nThe appendix of this paper is organized as follows.\n\n- [Appendix A](#page-12-0) discusses our motivation, challenges associated with smaller backbone models, differences from general LLM-agent frameworks, and limitations and promising future work.\n- [Appendix B](#page-13-0) describes details of experimental setup.\n- [Appendix C](#page-20-1) provides the complete prompts used by our *AutoML-Agent*.\n- [Appendix D](#page-27-3) and [Appendix E](#page-38-0) showcase intermediate and final results.\n- [Appendix F](#page-44-2) reports the detailed results with standard deviations.\n\n## <span id=\"page-12-0\"></span>A. Additional Discussions\n\nThis section discusses the motivation behind *AutoML-Agent* in relation to non-expert users, the challenges associated with smaller models, comparisons with general LLM-Agent frameworks, and the limitations of our approach.\n\n### A.1. Motivation and Necessity\n\nIn the era of generative AI, many companies are adopting AI technologies. However, these companies often lack AI experts, leading to software engineers—*who are non-experts in AI*—attempting to implement such solutions and facing significant challenges. Researchers from various domains outside of AI (e.g., economics, chemistry, and healthcare) are increasingly seeking to apply AI models in their work but struggle due to their lack of specialized AI expertise. Besides, the existence of non-expert users in the ML landscape is well-documented in relevant studies [\\(Karmaker et al.,](#page-10-20) [2021;](#page-10-20) [Sun et al.,](#page-11-2) [2023;](#page-11-2) [Tornede et al.,](#page-11-14) [2024;](#page-11-14) [Davenport & Bean,](#page-9-14) [2024\\)](#page-9-14). For example, an early AutoML survey paper [\\(Karmaker et al.,](#page-10-20) [2021\\)](#page-10-20) highlights the growing demand for ML tools among stakeholders across various domains. It emphasizes that AutoML tools aim to make ML accessible to non-experts, improve efficiency, and accelerate research, addressing the pressing need for user-friendly AI solutions. Similarly, as thoroughly illustrated in Section 3 of a recent study [\\(Tornede et al.,](#page-11-14) [2024\\)](#page-11-14), the use of LLMs as interfaces for AutoML and as components of AutoML systems offers significant opportunities for non-ML experts seeking to apply off-the-shelf data-driven solutions to their problems.\n\nTo further illustrate the necessity and practicality of *AutoML-Agent*, consider an academic researcher aiming to evaluate several ML models for a novel dataset within a constrained timeline. Traditionally, this involves significant manual effort in setting up pipelines, selecting models, and debugging code—steps that can be both error-prone and time-consuming. Similarly, in industry settings, ML engineers often need to rapidly prototype models for specific business requirements, such as creating a lightweight and efficient spam detection system for mobile applications. *AutoML-Agent* enables such users to focus on high-level problem formulation by generating deployment-ready models that adhere to specified constraints, like latency or accuracy, directly from natural language task descriptions. This capability reduces setup time and errors, enabling users to focus on innovation rather than implementation logistics.\n\n### A.2. Challenges with Smaller Models\n\nWhile we acknowledge the importance of validating our framework on smaller-scale models, these models exhibit systemic limitations that hinder their utility for complex tasks requiring code execution and extensive planning. During the early stage of development, we found that small language models, including LLaMA-2-7B, Qwen1.5-14B, gemma-7B-it, WizardLM-70B [\\(Xu et al.,](#page-11-15) [2024\\)](#page-11-15), and even code-specialized like CodeQwen1.5-7B failed to generate executable code for tasks requiring extensive planning or interdependent processes (e.g., full-pipeline skeleton script presented in [§B.1\\)](#page-15-0). Commonly, these smaller models exhibit similar issues, such as cutting the given codes, changing comments without code completion, partial completion, and returning code as it is given. As shown in [§F](#page-44-2) below, even vanilla GPT-3.5 and GPT-4o struggle with generating executable code for complex tasks, involving extensive planning or interdependent processes. Similar findings have been corroborated by prior studies [\\(Guo et al.,](#page-9-2) [2024a;](#page-9-2) [Hong et al.,](#page-10-3) [2024a\\)](#page-10-3), suggesting that these challenges are systemic to smaller language models and not unique to our framework.\n\n### A.3. Key Distinctions from General LLM-Agent Frameworks\n\nWhile it is true that generic LLM-agents are versatile and can theoretically execute certain AutoML tasks, they lack critical domain-specific capabilities essential for achieving robust and reliable performance across the structured and interdependent <span id=\"page-13-1\"></span>processes of a full AutoML pipeline. Generic LLM frameworks are typically designed to support broad problem domains and rely heavily on user-defined instructions or augmentations, often leading to inefficiencies, suboptimal performance, and increased failure rates in complex tasks like end-to-end AutoML. In contrast, *AutoML-Agent* is purpose-built with explicit mechanisms, such as retrieval-augmented planning, specialized agents for sub-tasks, and multi-stage verification, all tailored to address the unique challenges of AutoML. Through these enhancements, we can increase plan execution efficiency and support diverse ML tasks with more accurate pipeline implementation.\n\nFurthermore, as highlighted in [§F,](#page-44-2) even when a general-purpose LLM leverages the same backbone model, they consistently underperform compared to specialized methods like DS-Agent or *AutoML-Agent* in both success rates and downstream task metrics. *AutoML-Agent* not only outperforms these frameworks but also uniquely integrates the entire pipeline into a seamless, modular workflow, significantly reducing errors and enhancing usability for diverse downstream tasks. This distinction underscores the importance of designing specialized frameworks like *AutoML-Agent*, which prioritize task-specific optimizations over generality, ensuring both efficiency and reliability in automating full AutoML pipelines.\n\n### A.4. Limitations and Future Work\n\nEven though we offer a flexible module capable of accommodating various machine learning tasks and data modalities, the absence of skeleton code for entirely new tasks may increase the risk of code hallucination. Moreover, in the current version, there remains a noticeable gap in code generation quality when using different backbone models, e.g., GPT-4 vs. GPT-3.5. This discrepancy is not unique to our approach but represents a broader challenge faced by existing LLM-based frameworks. Future work focused on developing a more robust framework that can generate reasonable solutions with reduced dependence on a specific LLM backbone is a promising direction.\n\nAdditionally, to further improve the literature retrieval component during the planning process, future iterations could incorporate tools, such as PaperQA [\\(Lala et al.](#page-10-21) ´ , [2023\\)](#page-10-21). A PaperQA-style scientific querying system could enhance citation relevance and enable more context-aware prompt generation, ultimately leading to improved planning decisions.\n\nOur work also encounters limitations in code generation when applied to machine learning tasks requiring significantly different development pipelines from those evaluated in our experiments, which primarily focused on general supervised and unsupervised settings. Tasks such as reinforcement learning and recommendation systems introduce particular challenges. Consequently, extending *AutoML-Agent* to these domains will necessitate the development of additional agents to handle specialized steps in the target pipeline, such as actor-environment interaction and reward modeling in the case of reinforcement learning.\n\nIn particular, extending *AutoML-Agent* to reinforcement learning would require incorporating domain-specific modules—for instance, agents for environment interaction, action space design, and reward processing.[2](#page-13-2) One could envision an \"RL agent\"\" interfacing with an environment (e.g., OpenAI Gym), while the planner generates RL-specific procedures such as policy training and evaluation. Although this extension is certainly feasible, it could be substantial enough that an RL-specific version of *AutoML-Agent*, perhaps termed AutoRL-Agent, would represent a significant contribution worthy of a separate publication or project.\n\n## <span id=\"page-13-0\"></span>B. Details of Experimental Setup\n\nThis section outlines the detailed experimental setup used in this paper, including the complete instruction prompts for both constraint-free [\\(Table 3\\)](#page-14-0) and constraint-aware [\\(Table 4\\)](#page-15-1) settings, a full-pipeline skeleton script ([§B.1\\)](#page-15-0), dataset ([§B.2\\)](#page-17-1) and baseline descriptions ([§B.3\\)](#page-18-0), as well as evaluation metrics ([§B.4\\)](#page-19-0).\n\n<span id=\"page-13-2\"></span><sup>2</sup> See <https://www.automl.org/autorl-survey>.\n\n<span id=\"page-14-0\"></span>\n\n| Task                       | Dataset                                                   | Instruction Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n|----------------------------|-----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Image<br>Classification    | Butterfly Image<br>Shopee-IET                             | I need a very accurate model to classify images in the Butterfly Image Classification dataset into their respective categories. The<br>dataset has been uploaded with its label information in the labels.csv file.<br>Please provide a classification model that categorizes images into one of four clothing categories. The image path, along with its<br>label information, can be found in the files train labels.csv and test labels.csv.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Text<br>Classification     | Ecommerce Text<br>Textual Entailment                      | We need a state-of-the-art model for text classification based on the Ecommerce Text dataset. The model should be capable of<br>accurately classifying text into four categories: Electronics, Household, Books, and Clothing & Accessories. We have uploaded the<br>entire dataset without splitting it here.<br>You are solving this machine learning tasks of classification: The dataset presented here (the Textual Entailment) comprises a<br>series of labeled text pairs. Given two texts (text1 and text2), your task is to predict the relationship of the text pair of neutral (0),<br>contradiction (1) or entailment (2). The evaluation metric is accuracy. Build a language model to get a good performance.                                                                                                                                                                                                                                                         |\n| Tabular<br>Classification  | Banana Quality<br>Software Defects                        | Build a model to classify banana quality as Good or Bad based on their numerical information about bananas of different quality<br>(size, weight, sweetness, softness, harvest time, ripeness, and acidity). We have uploaded the entire dataset for you here in the<br>banana quality.csv file.<br>You are solving this data science tasks of binary classification: The dataset presented here (the Software Defects Dataset) comprises<br>a lot of numerical features. Please split the dataset into three parts of train, valid and test. Your task is to predict the defects item,<br>which is a binary label with 0 and 1. The evaluation metric is the F1 score. Please train a binary classification model to get a good<br>performance on this task.                                                                                                                                                                                                                       |\n| Tabular<br>Regression      | Crab Age<br>Crop Price                                    | You are solving this data science tasks of regression: The dataset presented here (the Crab Age Dataset) comprises a lot of both<br>categorical and numerical features. Pleae split the dataset into three parts of train, valid and test. Your task is to predict the age item.<br>The evaluation metric is the RMSLE (root mean squared log error). Now train a regression model to get a good performance on this<br>task.<br>I need a regression model to predict crop prices based on features like soil composition, environmental factors, historical yield data,<br>and crop management practices from the dataset I uploaded here.                                                                                                                                                                                                                                                                                                                                         |\n| Tabular<br>Clustering      | Smoker Status<br>Higher Education<br>Students Performance | You are solving this data science tasks of unsupervised clustering: The dataset presented here (the Smoker Status Dataset) comprises<br>a lot of numerical features. Please use the features in the test.csv file. Your task is to create the clustered items, which is a binary<br>label with 0 and 1 (two clusters). The evaluation metric is the Rand index or Rand score, can be tested against 'smoking' labels. Now<br>train an unsupervised clustering model to get a good performance on this task.<br>I want an unsupervised clustering model to group student performances into eight groups. The dataset named 'Higher Education<br>Students Performance Evaluation' (id=856) can be downloaded via ucimlrepo library. The clustering quality can be check against<br>target variable OUTPUT Grade.                                                                                                                                                                      |\n| Time-Series<br>Forecasting | Weather<br>Electricity                                    | I want you to create a model for node classification on the Cora dataset to predict the category of each paper. You need to directly<br>find the Cora dataset from a relevant library.<br>I want you to create a model for node classification on the Citeseer dataset to predict the category of each paper. You need to directly<br>find the Citeseer dataset from a relevant library.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Node<br>Classification     | Cora<br>Citeseer                                          | Build a model to perform time-series forecasting using the Weather dataset uploaded here, evaluating its accuracy with the RMSLE<br>metric. Note that the input is a sequence of past observations with fixed size (INPUT SEQ LEN=96, INPUT DIM=21). The model<br>should predict the next future sequence with a fixed size (PRED SEQ LEN=96, PRED DIM=21).<br>You are solving this machine learning tasks of time series forecasting: The dataset presented here (the Electricity dataset) comprises<br>real-world time series data. Please split the dataset into three parts of train, valid and test. The input is a sequence of past<br>observation with fixed size (INPUT SEQ LEN=96, INPUT DIM=321). Your task is to predict the next future sequence with fixed<br>size (PRED SEQ LEN=96, PRED DIM=321). The evaluation metric is root mean squared log error (RMSLE). Now train a time<br>series forecasting model to get a good performance on the given fixed sequences. |\n\n#### Table 3. User instruction (i.e., task description) for experiments under the *constraint-free* setting.\n\n<span id=\"page-15-1\"></span>\n\n| Table 4. User instruction (i.e., task description) for experiments under the constraint-aware setting. Bold texts indicate constraints used |  |\n|---------------------------------------------------------------------------------------------------------------------------------------------|--|\n| for evaluation.                                                                                                                             |  |\n\n| Task                       | Dataset                                                   | Instruction Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|----------------------------|-----------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Image<br>Classification    | Butterfly Image<br>Shopee-IET                             | I need a highly accurate machine learning model developed to classify images within the Butterfly Image Classification dataset into their correct species<br>categories. The dataset has been uploaded with its label information in the labels.csv file. Please use a convolutional neural network (CNN) architecture for<br>this task, leveraging transfer learning from a pre-trained ResNet-50 model to improve accuracy. Optimize the model using cross-validation on the training<br>split to fine-tune hyperparameters, and aim for an accuracy of at least 0.95 on the test split. Provide the final trained model, a detailed report of the training<br>process, hyperparameter settings, accuracy metrics, and a confusion matrix to evaluate performance across different categories.<br>Please provide a classification model that categorizes images into one of four clothing categories. The image path, along with its label information, can be<br>found in the files train labels.csv and test labels.csv. The model should achieve at least 85% accuracy on the test set and be implemented using PyTorch.<br>Additionally, please include data augmentation techniques and a confusion matrix in the evaluation. |\n| Text<br>Classification     | Ecommerce Text<br>Textual Entailment                      | We require the development of an advanced neural network model for text classification tailored to the Ecommerce Text dataset, with the objective of<br>achieving at least 0.95 classification accuracy. The model should be specifically trained to distinguish text into four defined categories: Electronics,<br>Household, Books, and Clothing & Accessories. To facilitate this, we have uploaded the complete dataset in its entirety, without any prior division into<br>training, validation, or test sets.<br>You are solving this machine learning task of classification: The dataset presented here (the Textual Entailment) comprises a series of labeled text pairs.<br>Given two texts, your task is to predict the relationship of the text pair as neutral (0), contradiction (1), or entailment (2). The evaluation metric is accuracy.<br>Build a language model to get good performance, ensuring the model size does not exceed 200 million parameters and the inference time is less than 200<br>milliseconds per prediction.                                                                                                                                                                                  |\n| Tabular<br>Classification  | Banana Quality<br>Software Defects                        | Build a machine learning model, potentially XGBoost or LightGBM, to classify banana quality as Good or Bad based on their numerical information about<br>bananas of different quality (size, weight, sweetness, softness, harvest time, ripeness, and acidity). We have uploaded the entire dataset for you here in the<br>banana quality.csv file. The model must achieve at least 0.98 accuracy.<br>You are solving this data science task of binary classification: The dataset presented here (the Software Defects Dataset) comprises a lot of numerical features.<br>Please split the dataset into three parts of train, valid, and test. Your task is to predict the defects item, which is a binary label with 0 and 1. The evaluation<br>metric is the F1 score. Please train a binary classification model to get a good performance on this task, ensuring that the model training time does not<br>exceed 30 minutes and the prediction time for each instance is under 5 milliseconds.                                                                                                                                                                                                                                  |\n| Tabular<br>Regression      | Crab Age<br>Crop Price                                    | You are solving this data science task of regression: The dataset presented here (the Crab Age Dataset) comprises a lot of both categorical and numerical<br>features. Please split the dataset into three parts of train, valid, and test. Your task is to predict the age item. The evaluation metric is the RMSLE (root<br>mean squared log error). Now train a regression model to get a good performance on this task, ensuring that the model's training time does not exceed 30<br>minutes and that it can make predictions on the test set within 5 seconds.<br>I need an accurate regression model to predict crop prices based on features like soil composition, environmental factors, historical yield data, and crop<br>management practices from the dataset I uploaded here. You should optimize the model to achieve RMSLE less than 1.0                                                                                                                                                                                                                                                                                                                                                                            |\n| Tabular<br>Clustering      | Smoker Status<br>Higher Education<br>Students Performance | You are solving this data science task of unsupervised clustering: The dataset presented here (the Smoker Status Dataset) comprises a lot of numerical<br>features. Please use the features in test.csv. Your task is to create the clustered items, which is a binary label with 0 and 1 (two clusters). The evaluation<br>metric is the Rand index or Rand score, which can be tested against 'smoking' labels. Now train an unsupervised clustering model to get a good performance<br>on this task, ensuring that the Rand index is at least 0.75 and the model training time does not exceed 10 minutes.<br>I want an unsupervised clustering model to group student performances into eight groups. The dataset named 'Higher Education Students Performance<br>Evaluation' (id=856) can be downloaded via ucimlrepo library. The clustering quality can be checked against the target variable OUTPUT Grade. The model<br>should achieve a Rand Score of at least 0.8 and complete clustering within 10 minutes.                                                                                                                                                                                                              |\n| Time-Series<br>Forecasting | Weather<br>Electricity                                    | Build a state-of-the-art time-series forecasting model for the Weather dataset uploaded here, evaluating its accuracy with the RMSLE metric. Note that the<br>input is a sequence of past observations with fixed size (INPUT SEQ LEN=96, INPUT DIM=21). The model should predict the next future sequence with a<br>fixed size (PRED SEQ LEN=96, PRED DIM=21). We target RMSLE lower than 0.05.<br>You are solving this machine learning task of time series forecasting: The dataset presented here (the Electricity dataset) comprises real-world time series<br>data. Please split the dataset into three parts of train, valid, and test. The input is a sequence of past observation with fixed size (INPUT SEQ LEN=96,<br>INPUT DIM=321). Your task is to predict the next future sequence with fixed size (PRED SEQ LEN=96, PRED DIM=321). The evaluation metric is root<br>mean squared log error (RMSLE). Now train a time series forecasting model to get a good performance on the given fixed sequences. Ensure the model<br>achieves an RMSLE of less than 0.1 and that the training time does not exceed 1 hour on a GPU.                                                                                             |\n| Node<br>Classification     | Cora<br>Citeseer                                          | I want you to develop a node classification model using the Graph Convolutional Network (GCN) algorithm to predict the category of each paper in the<br>Cora dataset. Start by importing the Cora dataset using the 'Planetoid' dataset from the 'torch geometric.datasets' module in PyTorch Geometric. Ensure you<br>preprocess the data to include node features and labels correctly. Train the model using a suitable optimizer and loss function. Then, evaluate its accuracy on<br>the test set. The accuracy on the test set should be over 0.90.<br>I want you to develop a node classification model using the Graph Convolutional Network (GCN) algorithm to predict the category of each paper in the<br>Citeseer dataset. Start by importing the Citeseer dataset using the 'Planetoid' dataset from the 'torch geometric.datasets' module in PyTorch Geometric.<br>Ensure you preprocess the data to include node features and labels correctly. Train the model using a suitable optimizer and loss function. Then, evaluate its<br>accuracy on the test set. The accuracy on the test set should be over 0.80.                                                                                                       |\n\n#### <span id=\"page-15-0\"></span>B.1. Skeleton Python Script\n\n| Skeleton Python Script (e.g., text classification.py)                                                                                                                                                                    |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| # The following code is for \"text classification\" task using PyTorch.<br>import os, random, time, json                                                                                                                   |\n| # define GPU location<br>os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"<br>os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"                                                                                                      |\n| import torch<br>import torch.nn as nn<br>import torch.optim as optim<br>import numpy as np<br>import gradio as gr                                                                                                        |\n| # TODO: import other required library here, including libraries for datasets and (pre-trained) models like<br>HuggingFace and Kaggle APIs. If the required module is not found, you can directly install it by running ' |\n\n```\npip install your_module'.\nfrom torchtext import datasets, data, vocab\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nSEED = 42\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n# Define device for model operations\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATASET_PATH = \"_experiments/datasets\" # path for saving and loading dataset(s) (or the user's uploaded dataset)\n     for preprocessing, training, hyperparamter tuning, deployment, and evaluation\n# Data preprocessing and feature engineering\ndef preprocess_data():\n   # TODO: this function is for data preprocessing and feature engineering\n   # Run data preprocessing\n   # Should return the preprocessed data\n   return processed_data\ndef train_model(model, train_loader):\n   # TODO: this function is for model training loop and optimization on 'train' and 'valid' datasets\n   # TODO: this function is for fine-tuning a given pretrained model (if applicable)\n   # Should return the well-trained or finetuned model.\n   return model\ndef evaluate_model(model, test_loader):\n   # In this task, we use Accuracy and F1 metrics to evaluate the text classification performance.\n   # The 'performance_scores' should be in dictionary format having metric names as the dictionary keys\n   # TODO: the first part of this function is for evaluating a trained or fine-tuned model on the 'test' dataset\n        with respect to the relevant downstream task's performance metrics\n   # Define the 'y_true' for ground truth and 'y_pred' for the predicted classes here.\n   performance_scores = {\n      'ACC': accuracy_score(y_true, y_pred),\n      'F1': f1_score(y_true, y_pred)\n   }\n   # TODO: the second part of this function is for measuring a trained model complexity on a samples with\n       respect to the relevant complexity metrics, such as inference time and model size\n   # The 'complexity_scores' should be in dictionary format having metric names as the dictionary keys\n   # Should return model's performance scores\n   return performance_scores, complexity_scores\ndef prepare_model_for_deployment():\n   # TODO: this function is for preparing an evaluated model using model compression and conversion to deploy\n       the model on a particular platform\n   # Should return the deployment-ready model\n   return deployable_model\ndef deploy_model():\n   # TODO: this function is for deploying an evaluated model with the Gradio Python library\n   # Should return the url endpoint generated by the Gradio library\n   return url_endpoint\n# The main function to orchestrate the data loading, data preprocessing, feature engineering, model training,\n    model preparation, model deployment, and model evaluation\ndef main():\n   \"\"\"\n   Main function to execute the text classification pipeline.\n   \"\"\"\n   # TODO: Step 1. Retrieve or load a dataset from hub (if available) or user's local storage (if given)\n   dataset = None\n   # TODO: Step 2. Create a train-valid-test split of the data by splitting the 'dataset' into train_loader,\n       valid_loader, and test_loader.\n   # Here, the train_loader contains 70% of the 'dataset', the valid_loader contains 20% of the 'dataset', and\n       the test_loader contains 10% of the 'dataset'.\n```\n\n```\ntrain_loader, valid_loader, test_loader = (None, None, None) # corresponding to 70%, 20%, 10% of 'dataset'\n   # TODO: Step 3. With the split dataset, run data preprocessing and feature engineering (if applicable) using\n       the \"preprocess_data\" function you defined\n   processed_data = preprocess_data()\n   # TODO: Step 4. Define required model. You may retrieve model from available hub or library along with\n       pretrained weights (if any).\n   # If pretrained or predefined model is not available, please create the model according to the given user's\n       requirements below using PyTorch and relevant libraries.\n   model = None\n   # TODO: Step 5. train the retrieved/loaded model using the defined \"train_model\" function\n   # TODO: on top of the model training, please run hyperparamter optimization based on the suggested\n       hyperparamters and their values before proceeding to the evaluation step to ensure model's optimality\n   model = train_model()\n   # TODO: evaluate the trained model using the defined \"evaluate_model\" function\n   model_performance, model_complexity = evaluate_model()\n   # TODO: compress and convert the trained model according to a given deployment platform using the defined \"\n       prepare_model_for_deployment\" function\n   deployable_model = prepare_model_for_deployment()\n   # TODO: deploy the model using the defined \"deploy_model\" function\n   url_endpoint = deploy_model()\n   return processed_data, model, deployable_model, url_endpoint, model_performance, model_complexity\nif __name__ == \"__main__\":\n   processed_data, model, deployable_model, url_endpoint, model_performance, model_complexity = main()\n   print(\"Model Performance on Test Set:\", model_performance)\n   print(\"Model Complexity:\", model_complexity)\n```\n#### <span id=\"page-17-1\"></span>B.2. Dataset Descriptions\n\nAs presented in [Table 5,](#page-18-2) we select seven representative downstream tasks, covering five data modalities. We describe the datasets their statistics as follows.\n\n- Butterfly Image (Butterfly). This dataset includes 75 distinct classes of butterflies, featuring over 1,000 labeled images, including validation images. Each image is assigned to a single butterfly category. The dataset is accessible at <https://www.kaggle.com/datasets/phucthaiv02/butterfly-image-classification>.\n- Shopee-IET (Shopee). This dataset is designed for cloth image classification, where each image represents a clothing item, and its corresponding label indicates the clothing category. The available labels include BabyPants, BabyShirt, womencasualshoes, and womenchiffontop. The dataset is available at [https://www.kaggle.com/competitions/](https://www.kaggle.com/competitions/demo-shopee-iet-competition/data) [demo-shopee-iet-competition/data](https://www.kaggle.com/competitions/demo-shopee-iet-competition/data).\n- Ecommerce Text (Ecomm). This dataset is a classification-based E-commerce text dataset comprising four categories: Electronics, Household, Books, and Clothing & Accessories, which together cover approximately 80% of any Ecommerce website. It includes 50,425 instances and can be found at [https://www.kaggle.com/datasets/](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification) [saurabhshahane/ecommerce-text-classification](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification).\n- Textual Entailment (Entail). This dataset consists of labeled pairs of text, where the task is to predict the relationship between each pair as either neutral (0), contradiction (1), or entailment (2). It is divided into a training set containing 4,907 samples and a testing set with 4,908 samples. We use the dataset provided by [Guo et al.](#page-9-2) [\\(2024a\\)](#page-9-2).\n- Banana Quality (Banana). This tabular dataset consists of numerical information on 8,000 samples of bananas, covering various quality attributes such as size, weight, sweetness, softness, harvest time, ripeness, acidity, and overall quality. The primary objective of the dataset is to classify each banana sample as either good or bad. The dataset is available at <https://www.kaggle.com/datasets/l3llff/banana/data>.\n- Software Defects (Software). This dataset consists primarily of numerical features and has been divided into three parts: training, validation, and testing. The goal is to predict a binary defect label (0 or 1). The training set contains 82,428 samples, the validation set contains 9,158 samples, and the test set contains 91,587 samples. We use the dataset provided by [Guo et al.](#page-9-2) [\\(2024a\\)](#page-9-2).\n- Crab Age (Crab). This dataset contains a mix of categorical and numerical features, and has been divided into three parts: training, validation, and test sets. The task is to predict the age of the crabs. The training set consists of 59,981\n\n<span id=\"page-18-2\"></span><span id=\"page-18-1\"></span>\n\n| Data Modality                                             | Downstream Task                          | Dataset Name                                           | # Features         | # Train          | # Valid         | # Test         | # Classes  | Source                                                       | License                | Evaluation Metric |\n|-----------------------------------------------------------|------------------------------------------|--------------------------------------------------------|--------------------|------------------|-----------------|----------------|------------|--------------------------------------------------------------|------------------------|-------------------|\n| Main Datasets                                             |                                          |                                                        |                    |                  |                 |                |            |                                                              |                        |                   |\n| Image<br>(Computer Vision)                                | Image Classification                     | Butterfly Image<br>Shopee-IET                          | 224x224<br>Varying | 4,549<br>640     | 1,299<br>160    | 651<br>80      | 75<br>4    | Kaggle Dataset<br>Kaggle Competition                         | CC0<br>Custom          | Accuracy          |\n| (Natural Language Processing) Text Classification<br>Text |                                          | Ecommerce Text<br>Textual Entailment                   | N/A<br>N/A         | 35,296<br>3,925  | 10,084<br>982   | 5,044<br>4,908 | 4<br>3     | Kaggle Dataset<br>Kaggle Dataset                             | CC BY 4.0<br>N/A       | Accuracy          |\n|                                                           | Tabular Classification                   | Banana Quality<br>Software Defects                     | 7<br>21            | 5,600<br>73,268  | 1,600<br>18,318 | 800<br>91,587  | 2<br>2     | Kaggle Dataset<br>Kaggle Competition                         | Apache 2.0<br>N/A      | F1                |\n| Tabular<br>(Classic Machine Learning)                     | Tabular Clustering                       | Smoker Status<br>Higher Education Students Performance | 22<br>31           | 100,331<br>101   | 28,666<br>29    | 14,334<br>15   | 2<br>8     | Kaggle Competition<br>Research Dataset (UCI ML)              | N/A<br>CC BY 4.0       | RI<br>RI          |\n|                                                           | Tabular Regression                       | Crab Age<br>Crop Price                                 | 8<br>8             | 53,316<br>1,540  | 13,329<br>440   | 66,646<br>220  | N/A<br>N/A | Kaggle Competition<br>Kaggle Dataset                         | CC0<br>MIT             | RMSLE<br>RMSLE    |\n| Graph<br>(Graph Learning)                                 | Node Classification                      | Cora<br>Citeseer                                       | 1,433<br>3,703     | 2,708<br>3,327   | 2,708<br>3,327  | 2,708<br>3,327 | 7<br>6     | Research Dataset (Planetoid)<br>Research Dataset (Planetoid) | CC BY 4.0<br>N/A       | Accuracy          |\n| Time Series<br>(Time Series Analysis)                     | Time-Series Forecasting                  | Weather<br>Electricity                                 | 21<br>321          | 36,887<br>18,412 | 10,539<br>5,260 | 5,270<br>2,632 | N/A<br>N/A | Research Dataset (TSLib)<br>Research Dataset (TSLib)         | CC BY 4.0<br>CC BY 4.0 | RMSLE             |\n| Additional Datasets from SELA                             |                                          |                                                        |                    |                  |                 |                |            |                                                              |                        |                   |\n|                                                           | Binary Classification                    | Smoker Status<br>Click Prediction Small                | 22<br>11           | 85997<br>19174   | 21500<br>4794   | 143331<br>7990 | 2<br>2     | Kaggle Competition<br>OpenML                                 |                        |                   |\n| Tabular<br>(Classic Machine Learning)                     | Multi-Class Classification MFeat Factors | Wine Quality White                                     | 216<br>11          | 960<br>2350      | 240<br>588      | 400<br>980     | 10<br>7    | OpenML<br>OpenML                                             | N/A                    | F1                |\n|                                                           | Regression                               | Colleges<br>House Prices                               | 44<br>80           | 3389<br>700      | 848<br>176      | 1413<br>292    | N/A<br>N/A | OpenML<br>Kaggle Competition                                 |                        | RMSE              |\n\nTable 5. Summary of downstream tasks and benchmark dataset statistics.\n\nsamples, the validation set includes 6,664 samples, and the test set contains 66,646 samples. We use the dataset provided by [Guo et al.](#page-9-2) [\\(2024a\\)](#page-9-2).\n\n- Crop Price (Crop). This new dataset contains 2,200 samples with key features such as nitrogen, phosphorus, and potassium ratios in the soil, temperature (in °C), humidity (in %), soil pH value, and rainfall (in mm), all of which are essential for predicting crop yield values. Crop yield prediction is crucial in modern agriculture, particularly as data-driven methods become more prevalent. This dataset is available at [https://www.kaggle.com/datasets/](https://www.kaggle.com/datasets/varshitanalluri/crop-price-prediction-dataset) [varshitanalluri/crop-price-prediction-dataset](https://www.kaggle.com/datasets/varshitanalluri/crop-price-prediction-dataset).\n- Smoker Status (Smoker). This dataset contains numerous numerical features. The goal is to categorize smoking status of each instance into a cluster. The training set consists of 143,330 samples and the test set includes 143,331 samples. We use the dataset provided by [Guo et al.](#page-9-2) [\\(2024a\\)](#page-9-2).\n- Higher Education Students Performance (Student). The dataset, collected in 2019 from students in the Faculty of Engineering and Faculty of Educational Sciences, was created to predict students' end-of-term performances using machine learning techniques. It is a multivariate dataset with 145 instances and 31 integer-type features, focusing on classification tasks within the domain of social sciences. We adopt this dataset for unsupervised clustering instead of classification. This dataset can be found at [https://archive.ics.uci.edu/dataset/856/higher+](https://archive.ics.uci.edu/dataset/856/higher+education+students+performance+evaluation) [education+students+performance+evaluation](https://archive.ics.uci.edu/dataset/856/higher+education+students+performance+evaluation).\n- Weather. The weather dataset consists of 21 meteorological factors collected every 10 minutes from the Weather Station at the Max Planck Biogeochemistry Institute in 2020, containing 52,603 samples without any pre-splitting. It is accessible at <https://github.com/thuml/Time-Series-Library>.\n- Electricity. This dataset comprises hourly electricity consumption data for 321 customers collected from 2012 to 2014, totaling 26,211 samples. The dataset records the electricity usage of these clients on an hourly basis and is provided without any pre-split. The dataset is available at <https://github.com/thuml/Time-Series-Library>.\n- Cora and Citeseer. The citation network datasets, \"Cora\" and \"CiteSeer,\" consist of nodes representing documents and edges representing citation links between them. Both datasets provide training, validation, and test splits through binary masks. The Cora dataset contains 2,708 nodes, 10,556 edges, 1,433 features, and 7 classes, while CiteSeer consists of 3,327 nodes, 9,104 edges, 3,703 features, and 6 classes. We use the version provided by [Fey & Lenssen](#page-9-15) [\\(2019\\)](#page-9-15).\n\n### <span id=\"page-18-0\"></span>B.3. Baselines\n\nHuman Models We select top-performing models based on evaluations from Papers with Code benchmarks or Kaggle notebooks, where the similar tasks and datasets are applicable. The chosen models for relevant downstream tasks are described below.\n\n• Image Classification. The human models for image classification tasks are obtained from a Kaggle notebook available at [https://www.kaggle.com/code/mohamedhassanali/](https://www.kaggle.com/code/mohamedhassanali/butterfly-classify-pytorch-pretrained-model-acc-99/notebook) [butterfly-classify-pytorch-pretrained-model-acc-99/notebook](https://www.kaggle.com/code/mohamedhassanali/butterfly-classify-pytorch-pretrained-model-acc-99/notebook), utilizing a pretrained ResNet-18 model.\n\n- <span id=\"page-19-1\"></span>• Text Classification. For text classification tasks, two models are employed. A Word2Vec-based XGBoost model is applied to the e-commerce text dataset [https://www.kaggle.com/code/sugataghosh/](https://www.kaggle.com/code/sugataghosh/e-commerce-text-classification-tf-idf-word2vec#Word2Vec-Hyperparameter-Tuning) [e-commerce-text-classification-tf-idf-word2vec#Word2Vec-Hyperparameter-Tuning](https://www.kaggle.com/code/sugataghosh/e-commerce-text-classification-tf-idf-word2vec#Word2Vec-Hyperparameter-Tuning), while the XLM-RoBERTa model is used for the textual entailment dataset [https://www.kaggle.com/code/](https://www.kaggle.com/code/vbookshelf/basics-of-bert-and-xlm-roberta-pytorch) [vbookshelf/basics-of-bert-and-xlm-roberta-pytorch](https://www.kaggle.com/code/vbookshelf/basics-of-bert-and-xlm-roberta-pytorch).\n- Tabular Classification. Due to the absence of a similar model in the repository, we use the state-of-the-art TabPFN model [\\(Hollmann et al.,](#page-10-22) [2023a\\)](#page-10-22) designed for tabular classification tasks.\n- Tabular Regression. For tabular regression tasks, we adopt two models specifically designed for the given datasets, which are available at <https://www.kaggle.com/code/shatabdi5/crab-age-regression> for the crab age dataset and at [https://www.kaggle.com/code/mahmoudmagdyelnahal/](https://www.kaggle.com/code/mahmoudmagdyelnahal/crop-yield-prediction-99/notebook) [crop-yield-prediction-99/notebook](https://www.kaggle.com/code/mahmoudmagdyelnahal/crop-yield-prediction-99/notebook) for the crop yield dataset.\n- Tabular Clustering. For unsupervised clustering tasks, we use manually hyperparameter-tuned KMeans clustering, following the approach outlined in [https://www.kaggle.com/code/samuelcortinhas/](https://www.kaggle.com/code/samuelcortinhas/tps-july-22-unsupervised-clustering) [tps-july-22-unsupervised-clustering](https://www.kaggle.com/code/samuelcortinhas/tps-july-22-unsupervised-clustering), as the baseline.\n- Time-Series Forecasting. In this task, we use the state-of-the-art iTransformer [\\(Liu et al.,](#page-10-23) [2024b\\)](#page-10-23), which is designed for the same task and datasets as the baseline model.\n- Node Classification. For node classification tasks, we also employ a state-of-the-art graph neural network-based model, PMLP [\\(Yang et al.,](#page-11-19) [2023\\)](#page-11-19), as the handcrafted baseline for both datasets.\n\nAutoGluon We adopt AutoGluon as the baseline because it is a state-of-the-art AutoML framework capable of handling various downstream tasks and data modalities, with the exception of graph data. There are three variants of AutoGluon: AutoGluon-TS [\\(Shchur et al.,](#page-11-17) [2023\\)](#page-11-17) for time series, AutoGluon-Tabular [\\(Erickson et al.,](#page-9-12) [2020\\)](#page-9-12) for tabular machine learning, and AutoGluon-Multimodal [\\(Tang et al.,](#page-11-9) [2024\\)](#page-11-9) for computer vision and natural language processing tasks.\n\nGPT-3.5 and GPT-4 For GPT-3.5 and GPT-4, we use the *gpt-3.5-turbo-0125* and *gpt-4-2024-05-13* models via the OpenAI API. We implement the zero-shot baselines using the prompt below.\n\n#### Zero-Shot Prompt for GPT-3.5 and GPT-4 Baselines\n\n```\nYou are a helpful intelligent assistant. Now please help solve the following machine learning task.\n[Task]\n{user instruction}\n[{file_name}.py] '''python\n{full-pipeline skeleton script}\n'''\nStart the python code with \"'''python\". Please ensure the completeness of the code so that it can be run without\n     additional modifications.\n```\nDS-Agent We reproduce the DS-Agent [\\(Guo et al.,](#page-9-2) [2024a\\)](#page-9-2) baseline using the official source code. However, it is important to note that our framework encompasses the entire process from data retrieval/loading to deployment, whereas DS-Agent focuses solely on the modeling aspect, assuming complete data and evaluation codes are provided. In this paper, we utilize the deployment stage of DS-Agent along with its collected case banks and Adapter prompt for the same tasks, as the source code for manual human insights collection during the development stage is unavailable.\n\nSELA As the most recent LLM-based AutoML study for tabular data, we additionally compare SELA [\\(Chi et al.,](#page-9-3) [2024\\)](#page-9-3) using a subset of the suggested datasets that vary in difficulty. SELA integrates Monte Carlo Tree Search with LLM agents to enhance the automation of ML pipelines. The main difference between SELA and *AutoML-Agent* is that SELA iteratively improves results based on experimental feedback, making it time-consuming, whereas *AutoML-Agent* bypasses the training process, making our method more efficient while still maintaining comparable downstream performance, as shown in [§4.3.](#page-7-4)\n\n#### <span id=\"page-19-0\"></span>B.4. Evaluation Metrics\n\nSuccess Rate (SR) We employ the success rate [\\(Guo et al.,](#page-9-2) [2024a;](#page-9-2) [Hong et al.,](#page-10-3) [2024a\\)](#page-10-3), which evaluates whether the models built by an LLM agent are executable in the given runtime environment. Success rate is used to assess code execution.\n\nFor the *constraint-free* setting, we apply a three-level grading scale as follows.\n\n- 0.00: Code cannot be executed.\n- 0.50: Code provides a runnable ML/DL model.\n- 1.00: Code provides a runnable model and an accessible deployment endpoint (e.g., Gradio).\n\nFor the *constraint-aware* setting, we use a five-level grading scale to evaluate whether the code executes successfully and satisfies the given constraints. The grading criteria are as follows.\n\n- 0.00: Code cannot be executed.\n- 0.25: Code provides a runnable ML/DL model.\n- 0.50: Code provides a runnable model and an accessible deployment endpoint (e.g., Gradio).\n- 0.75: Code provides a deployed, runnable model that partially meets constraints (e.g., target performance, inference time, and model size).\n- 1.00: Code provides a deployed, runnable model that fully meets constraints.\n\nNormalized Performance Score (NPS) In this paper, each downstream task is associated with a specific evaluation metric, which may vary between tasks. These metrics include accuracy, F1-score, and RMSLE. For metrics such as accuracy and F1-score, we present the raw values to facilitate comparison across identical data tasks. For performance metrics where lower values indicate better performance, such as loss-based metrics, we normalize all performance values s using the following transformation: NPS = 1 1+s . This transformation ensures that metrics like RMSLE are scaled between 0 and 1, with higher NPS values indicating better performance.\n\nNote that achieving downstream task performance (NPS) requires a runnable model, i.e., SR > 0. If the model cannot run, the NPS is zero by default as it cannot make any predictions.\n\nComprehensive Score (CS) To evaluate both the success rate and the downstream task performance of the generated AutoML pipelines simultaneously, we calculate CS as a weighted sum of SR and NPS, as follows: CS = 0.5×SR+0.5×NPS.\n\n## <span id=\"page-20-1\"></span>C. Prompts for *AutoML-Agent*\n\n### <span id=\"page-20-0\"></span>C.1. Agent Specifications\n\nThis subsection provides the *system prompt* design for agent specifications in *AutoML-Agent*, including Agent Manger [\\(C.1.1\\)](#page-20-2), Prompt Agent [\\(C.1.2\\)](#page-20-3), Data Agent [\\(C.1.3\\)](#page-21-1), Model Agent [\\(C.1.4\\)](#page-21-2), and Operation Agent [\\(C.1.5\\)](#page-21-3).\n\n### <span id=\"page-20-2\"></span>C.1.1. AGENT MANAGER\n\nSystem Message for Agent Manager Specification\n\nYou are an experienced senior project manager of a automated machine learning project (AutoML). You have two main responsibilities as follows.\n\n- 1. Receive requirements and/or inquiries from users through a well-structured JSON object. 2. Using recent knowledge and state-of-the-art studies to devise promising high-quality plans for data\n- scientists, machine learning research engineers, and MLOps engineers in your team to execute subsequent processes based on the user requirements you have received.\n\n### <span id=\"page-20-3\"></span>C.1.2. PROMPT AGENT\n\n![](_page_20_Figure_22.jpeg)\n\n| '''                                                                                       |  |\n|-------------------------------------------------------------------------------------------|--|\n| Your response must begin with \"'''json\" or \"{{\" and end with \"'''\" or \"}}\", respectively. |  |\n\n#### <span id=\"page-21-1\"></span>C.1.3. DATA AGENT\n\n| System Message for Data Agent Specification                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| You are the world's best data scientist of an automated machine learning project (AutoML) that can find the most<br>relevant datasets,run useful preprocessing, perform suitable data augmentation, and make meaningful<br>visulaization to comprehensively understand the data based on the user requirements. You have the following<br>main responsibilities to complete.<br>1.Retrieve a dataset from the user or search for the dataset based on the user instruction.<br>2.Perform data preprocessing based on the user instruction or best practice based on the given tasks.<br>3.Perform data augmentation as neccesary.<br>4.Extract useful information and underlying characteristics of the dataset. |\n\n### <span id=\"page-21-2\"></span>C.1.4. MODEL AGENT\n\n| System Message for Model Agent Specification                                                                                                                                                                                                                                                                                                                                                                                                        |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| You are the world's best machine learning research engineer of an automated machine learning project (AutoML)<br>that can find the optimal candidate machine learning models and artificial intelligence algorithms for the<br>given dataset(s), run hyperparameter tuning to opimize the models, and perform metadata extraction and<br>profiling to comprehensively understand the candidate models or algorithms based on the user requirements. |\n| You have the following main responsibilities to complete.                                                                                                                                                                                                                                                                                                                                                                                           |\n| 1. Retrieve a list of well-performing candidate ML models and AI algorithms for the given dataset based on the<br>user's requirement and instruction.                                                                                                                                                                                                                                                                                               |\n| 2. Perform hyperparameter optimization for those candidate models or algorithms.                                                                                                                                                                                                                                                                                                                                                                    |\n| 3. Extract useful information and underlying characteristics of the candidate models or algorithms using<br>metadata extraction and profiling techniques.                                                                                                                                                                                                                                                                                           |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n\n4. Select the top-k ('k' will be given) well-performing models or algorithms based on the hyperparameter optimization and profiling results.\n\n### <span id=\"page-21-3\"></span>C.1.5. OPERATION AGENT\n\n![](_page_21_Figure_8.jpeg)\n\n#### <span id=\"page-21-0\"></span>C.2. Variations of the System Prompt\n\nThis subsection outlines the *variations* in system prompt design used in the experiment described in [§4.3.](#page-8-5) These include: generic prompts [\\(C.2.1\\)](#page-22-0), generic prompts with instructions [\\(C.2.2\\)](#page-22-1), entry-level role descriptions [\\(C.2.3\\)](#page-22-2), experience-level role descriptions [\\(C.2.4\\)](#page-22-3), and world's-best-level role descriptions [\\(C.2.5\\)](#page-22-4).\n\n#### <span id=\"page-22-0\"></span>C.2.1. GENERIC PROMPT\n\n| Generic System Message       |  |  |\n|------------------------------|--|--|\n| You are a helpful assistant. |  |  |\n\n#### <span id=\"page-22-1\"></span>C.2.2. GENERIC PROMPT WITH INSTRUCTIONS\n\nGeneric System Message with Instructions\n\n```\nYou are a helpful assistant. You have main responsibilities as follows.\n{agent-specific steps}\n```\n#### <span id=\"page-22-2\"></span>C.2.3. ENTRY-LEVEL ROLE DESCRIPTIONS\n\nEntry-Level Role System Message\n\nYou are a {**job position**} of a automated machine learning project (AutoML). You have main responsibilities as follows. {**agent-specific steps**}\n\n#### <span id=\"page-22-3\"></span>C.2.4. EXPERIENCE-LEVEL ROLE DESCRIPTIONS\n\n#### Experience-Level Role System Message\n\n```\nYou are an experienced {job position} of a automated machine learning project (AutoML). You have main\n    responsibilities as follows.\n{agent-specific steps}\n```\n#### <span id=\"page-22-4\"></span>C.2.5. WORLD'S BEST ROLE DESCRIPTIONS\n\nWorld's Best System Message\n\n```\nYou are the world's best {job position} of a automated machine learning project (AutoML). You have main\n    responsibilities as follows.\n{agent-specific steps}\n```\n#### C.3. Prompts for Retrieval-Augmented Planning\n\nThis subsection presents prompts for planning-related processes (Figure [2\\(](#page-3-0)a)), including knowledge retrieval and summary prompts [\\(C.3.1\\)](#page-22-5), planning prompt [\\(C.3.2\\)](#page-24-0), and plan revision prompt [\\(C.3.3\\)](#page-24-1).\n\n#### <span id=\"page-22-5\"></span>C.3.1. KNOWLEDGE RETRIEVAL PROMPT\n\n![](_page_22_Figure_18.jpeg)\n\n#### AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML\n\n| # User's Requirements<br>{user_requirement_summary}                                                                                                                                                                                                                                |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Papers With Code<br>I searched the paperswithcode website to find state-of-the-art models using the keywords: {user_area} and {<br>user_task}. Here is the result:<br>=====================<br>{context}<br>=====================                                                  |\n| Please summarize the given pieces of search content into a single paragraph of useful knowledge and insights. We<br>aim to use your summary to address the following user's requirements.<br># User's Requirements<br>{user_requirement_summary}                                   |\n| arXiv                                                                                                                                                                                                                                                                              |\n| I searched the arXiv papers using the keywords: {task_kw} and {domain_kw}. Here is the result:<br>=====================<br>{context}<br>=====================                                                                                                                      |\n| Please summarize the given pieces of arXiv papers into a single paragraph of useful knowledge and insights. We<br>aim to use your summary to address the following user's requirements.<br># User's Requirements<br>{user_requirement_summary}                                     |\n| Google WebSearch                                                                                                                                                                                                                                                                   |\n| I searched the web using the query: {search_query}. Here is the result:<br>=====================                                                                                                                                                                                   |\n| {context}<br>=====================                                                                                                                                                                                                                                                 |\n| Please summarize the given pieces of search content into a single paragraph of useful knowledge and insights.<br>We aim to use your summary to address the following user's requirements.<br># User's Requirements<br>{user_requirement_summary}                                   |\n|                                                                                                                                                                                                                                                                                    |\n| Summary                                                                                                                                                                                                                                                                            |\n| Please extract and summarize the following group of contents collected from different online sources into a chunk<br>of insightful knowledge. Please format your answer as a list of suggestions. I will use them to address the<br>user's requirements in machine learning tasks. |\n| # Source: Google Web Search<br>{search_summary}<br>=====================                                                                                                                                                                                                           |\n| # Source: arXiv Papers<br>{arxiv_summary}<br>=====================                                                                                                                                                                                                                 |\n| # Source: Kaggle Hub<br>{kaggle_summary}<br>=====================                                                                                                                                                                                                                  |\n| # Source: PapersWithCode<br>{pwc_summary}<br>=====================                                                                                                                                                                                                                 |\n| The user's requirements are summarized as follows.<br>{user_requirement_summary}                                                                                                                                                                                                   |\n|                                                                                                                                                                                                                                                                                    |\n\n#### <span id=\"page-24-0\"></span>C.3.2. PLANNING PROMPT\n\n## Prompt for Retrieval-Augmented Planning Now, I want you to devise an end-to-end actionable plan according to the user's requirements described in the following JSON object. '''json {user\\_requirements} ''' Here is a list of past experience cases and knowledge written by an human expert for a relevant task: {plan\\_knowledge} When devising a plan, follow these instructions and do not forget them: - Ensure that your plan is up-to-date with current state-of-the-art knowledge.\n\n- \n- Ensure that your plan is based on the requirements and objectives described in the above JSON object. - Ensure that your plan is designed for AI agents instead of human experts. These agents are capable of\n- conducting machine learning and artificial intelligence research.\n- Ensure that your plan is self-contained with sufficient instructions to be executed by the AI agents.\n- Ensure that your plan includes all the key points and instructions (from handling data to modeling) so that the AI agents can successfully implement them. Do NOT directly write the code.\n- Ensure that your plan completely include the end-to-end process of machine learning or artificial intelligence model development pipeline in detail (i.e., from data retrieval to model training and evaluation) when applicable based on the given requirements.\n\n#### <span id=\"page-24-1\"></span>C.3.3. PLAN REVISION PROMPT\n\n#### Prompt for Plan Revision\n\n```\nNow, you will be asked to revise and rethink {num2words(n_plans)} different end-to-end actionable plans\n    according to the user's requirements described in the JSON object below.\n'''json\n{user_requirements}\n'''\nPlease use to the following findings and insights summarized from the previously failed plans. Try as much as\n    you can to avoid the same failure again.\n{fail_rationale}\nFinally, when devising a plan, follow these instructions and do not forget them:\n- Ensure that your plan is up-to-date with current state-of-the-art knowledge.\n- Ensure that your plan is based on the requirements and objectives described in the above JSON object.\n- Ensure that your plan is designed for AI agents instead of human experts. These agents are capable of\n    conducting machine learning and artificial intelligence research.\n- Ensure that your plan is self-contained with sufficient instructions to be executed by the AI agents.\n- Ensure that your plan includes all the key points and instructions (from handling data to modeling) so that\n    the AI agents can successfully implement them. Do NOT directly write the code.\n- Ensure that your plan completely include the end-to-end process of machine learning or artificial intelligence\n     model development pipeline in detail (i.e., from data retrieval to model training and evaluation) when\n    applicable based on the given requirements.\n```\n#### C.4. Prompts for Prompting-Based Plan Execution\n\nThis subsection presents prompts for prompting-based plan execution processes (Figure [2\\(](#page-3-0)b)), including plan decomposition (Data Agent [\\(C.4.1\\)](#page-24-2) and Model Agent [\\(C.4.2\\)](#page-25-0)), pseudo data analysis [\\(C.4.3\\)](#page-25-1), and training-free model search and HPO [\\(C.4.4\\)](#page-26-1).\n\n#### <span id=\"page-24-2\"></span>C.4.1. PLAN DECOMPOSITION: DATA AGENT\n\n```\nPrompt for Plan Decomposition: Data Agent\nAs a proficient data scientist, summarize the following plan given by the senior AutoML project manager\n    according to the user's requirements and your expertise in data science.\n# User's Requirements\n'''json\n```\n\n| {user_requirements}<br>'''                                                                                                                                                                                                                                                                                                                                                                                           |\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| # Project Plan<br>{plan}                                                                                                                                                                                                                                                                                                                                                                                             |\n| The summary of the plan should enable you to fulfill your responsibilities as the answers to the following<br>questions by focusing on the data manipulation and analysis.<br>1. How to retrieve or collect the dataset(s)?<br>2. How to preprocess the retrieved dataset(s)?<br>3. How to efficiently augment the dataset(s)?<br>4. How to extract and understand the underlying characteristics of the dataset(s)? |\n| Note that you should not perform data visualization because you cannot see it. Make sure that another data<br>scientist can exectly reproduce the results based on your summary.                                                                                                                                                                                                                                     |\n\n#### <span id=\"page-25-0\"></span>C.4.2. PLAN DECOMPOSITION: MODEL AGENT\n\n| Prompt for Plan Decomposition: Model Agent                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| As a proficient machine learning research engineer, summarize the following plan given by the senior AutoML<br>project manager according to the user's requirements, your expertise in machine learning, and the outcomes<br>from data scientist.                                                                                                                                                                                                                             |\n| **User's Requirements**<br>'''json<br>{user_requirements}<br>'''                                                                                                                                                                                                                                                                                                                                                                                                              |\n| **Project Plan**<br>{project_plan}                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| **Explanations and Results from the Data Scientist**<br>{data_result}                                                                                                                                                                                                                                                                                                                                                                                                         |\n| The summary of the plan should enable you to fulfill your responsibilities as the answers to the following<br>questions by focusing on the modeling and optimization tasks.<br>1. How to retrieve or find the high-performance model(s)?<br>2. How to optimize the hyperparamters of the retrieved models?<br>3. How to extract and understand the underlying characteristics of the dataset(s)?<br>4. How to select the top-k models or algorithms based on the given plans? |\n\n#### <span id=\"page-25-1\"></span>C.4.3. PSEUDO DATA ANALYSIS BY DATA AGENT\n\n#### Prompt for Pseudo Data Analysis As a proficient data scientist, your task is to explain \\*\\*detailed\\*\\* steps for data manipulation and analysis parts by executing the following machine learning development plan. # Plan {decomposed\\_data\\_plan} # Potential Source of Dataset {available\\_sources} Make sure that your explanation follows these instructions: - All of your explanation must be self-contained without using any placeholder to ensure that other data scientists can exactly reproduce all the steps, but do not include any code. - Include how and where to retrieve or collect the data. - Include how to preprocess the data and which tools or libraries are used for the preprocessing. - Include how to do the data augmentation with details and names. - Include how to extract and understand the characteristics of the data. - Include reasons why each step in your explanations is essential to effectively complete the plan. Note that you should not perform data visualization because you cannot see it. Make sure to focus only on the data part as it is your expertise. Do not conduct or perform anything regarding modeling or training. After complete the explanations, explicitly specify the (expected) outcomes and results both quantitative and qualitative of your explanations.\n\n#### <span id=\"page-26-1\"></span>C.4.4. TRAINING-FREE MODEL SEARCH AND HPO BY MODEL AGENT\n\n| Prompt for Training-Free Model Search and HPO                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| As a proficient machine learning research engineer, your task is to explain **detailed** steps for modeling and<br>optimization parts by executing the following machine learning development plan with the goal of finding top<br>-{k} candidate models/algorithms.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| # Suggested Plan<br>{decomposed_model_plan}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| # Available Model Source<br>{available_sources}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| Make sure that your explanation for finding the top-{k} high-performance models or algorithms follows these<br>instructions:<br>- All of your explanations must be self-contained without using any placeholder to ensure that other machine<br>learning research engineers can exactly reproduce all the steps, but do not include any code.<br>- Include how and where to retrieve or find the top-{k} well-performing models/algorithms.<br>- Include how to optimize the hyperparamters of the candidate models or algorithms by clearly specifying which<br>hyperparamters are optimized in detail.<br>- Corresponding to each hyperparamter, explicitly include the actual numerical value that you think it is the<br>optimal value for the given dataset and machine learning task.<br>- Include how to extract and understand the characteristics of the candidate models or algorithms, such as their<br>computation complexity, memory usage, and inference latency. This part is not related to visualization and<br>interpretability.<br>- Include reasons why each step in your explanations is essential to effectively complete the plan.<br>Make sure to focus only on the modeling part as it is your expertise. Do not conduct or perform anything<br>regarding data manipulation or analysis.<br>After complete the explanations, explicitly specify the names and (expected) quantitative performance using<br>relevant numerical performance and complexity metrics (e.g., number of parameters, FLOPs, model size,<br>training time, inference speed, and so on) of the {num2words(k)} candidate models/algorithms potentially to |\n| be the optimal model below.<br>Do not use any placeholder for the quantitative performance. If you do not know the exact values, please use the<br>knowledge and expertise you have to estimate those performance and complexity values.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n\n#### C.5. Prompts for Multi-Stage Verification\n\nThis subsection presents prompts for multi-stage verification (Figure [2\\(](#page-3-0)c)), which ensures the correctness of intermediate results between steps in the framework. These stages include request verification [\\(C.5.1\\)](#page-26-0), execution verification [\\(C.5.2\\)](#page-27-1), and implementation verification [\\(C.5.3\\)](#page-27-2).\n\n#### <span id=\"page-26-0\"></span>C.5.1. REQUEST VERIFICATION\n\nAnswer only 'Yes' or 'No'\n\n## Request Verification (Relevancy) Is the following statement relevant to machine learning or artificial intelligence? '{user instruction}'\n\n#### Request Verification (Adequacy)\n\nGiven the following JSON object representing the user's requirement for a potential ML or AI project, please tell me whether we have essential information (e.g., problem and dataset) to be used for a AutoML project? Please note that our users are not AI experts, you must focus only on the essential requirements, e.g., problem and brief dataset descriptions. You do not need to check every details of the requirements. You must also answer 'yes' even though it lacks detailed and specific information. '''json {parsed user requirements} ''' Please answer with this format: 'a 'yes' or 'no' answer; your reasons for the answer' by using ';' to separate between the answer and its reasons.\n\nIf the answer is 'no', you must tell me the alternative solutions or examples for completing such missing information.\n\n#### <span id=\"page-27-1\"></span>C.5.2. EXECUTION VERIFICATION\n\n```\nExecution Verification\nGiven the proposed solution and user's requirements, please carefully check and verify whether the proposed\n    solution 'pass' or 'fail' the user's requirements.\n**Proposed Solution and Its Implementation**\nData Manipulation and Analysis: {data_agent_outcomes}\nModeling and Optimization: {model_agent_outcomes}\n**User Requirements**\n'''json\n{user_requirements}\n'''\nAnswer only 'Pass' or 'Fail'\n```\n#### <span id=\"page-27-2\"></span>C.5.3. IMPLEMENTATION VERIFICATION\n\n```\nImplementation Verification\nAs the project manager, please carefully verify whether the given Python code and results satisfy the user's\n    requirements.\n- Python Code\n'''python\n{implementation_result['code']}\n'''\n- Code Execution Result\n{implementation_result['action_result']}\n- User's Requirements\n{user_requirements}\nAnswer only 'Pass' or 'Fail'\n```\n### <span id=\"page-27-3\"></span>D. Examples of Intermediate Results\n\nThis section presents examples of intermediate results from the main steps of our framework, including prompt parsing results [\\(D.1\\)](#page-27-0), retrieval-augmented planning results [\\(D.2\\)](#page-29-0), plan decomposition results [\\(D.3\\)](#page-33-0), and prompting-based plan execution results [\\(D.4\\)](#page-35-0).\n\n#### <span id=\"page-27-0\"></span>D.1. Prompt Parsing\n\n#### Task Description and Parsed JSON User Requirements\n\nConstraint-Free User Prompt: Build a model to classify banana quality as Good or Bad based on their numerical information about bananas of different quality (size, weight, sweetness, softness, harvest time, ripeness, and acidity). We have uploaded the entire dataset for you here in the banana quality.csv file. Parsed Output:\n\n```\n{\n   \"user\": {\"intent\": \"build\",\"expertise\": \"medium\"},\n   \"problem\": {\n      \"area\": \"tabular data analysis\",\n      \"downstream_task\": \"tabular classification\",\n      \"application_domain\": \"agriculture\",\n      \"description\": \"Build a model to classify banana quality as Good or Bad based on their numerical\n           information about bananas of different quality (size, weight, sweetness, softness, harvest time,\n           ripeness, and acidity).\",\n      \"performance_metrics\": [],\n      \"complexity_metrics\": []\n   },\n   \"dataset\": [\n```\n\n```\n{\n         \"name\": \"banana_quality\",\n         \"modality\": [\"tabular\"],\n         \"target_variables\": [\"quality\"],\n         \"specification\": null,\n         \"description\": \"A dataset containing numerical information about bananas of different quality,\n              including size, weight, sweetness, softness, harvest time, ripeness, and acidity.\",\n         \"preprocessing\": [],\n         \"augmentation\": [],\n         \"visualization\": [],\n         \"source\": \"user-upload\"\n      }\n   ],\n   \"model\": [\n      {\n         \"name\": \"\",\n         \"family\": \"\",\n         \"type\": \"classical machine learning\",\n         \"specification\": null,\n         \"description\": \"A model to classify banana quality as Good or Bad based on their numerical information.\"\n      }\n   ],\n...\n}\n```\nConstraint-Aware User Prompt: Build a machine learning model, potentially XGBoost or LightGBM, to classify banana quality as Good or Bad based on their numerical information about bananas of different quality (size, weight, sweetness, softness, harvest time, ripeness, and acidity). We have uploaded the entire dataset for you here in the banana quality.csv file. The model must achieve at least 0.98 accuracy. Parsed Output:\n\n```\n{\n   \"user\": {\"intent\": \"build\", \"expertise\": \"medium\"},\n   \"problem\": {\n      \"area\": \"tabular data analysis\",\n      \"downstream_task\": \"tabular classification\",\n      \"application_domain\": \"agriculture\",\n      \"description\": \"Build a machine learning model, potentially XGBoost or LightGBM, to classify banana quality\n            as Good or Bad based on their numerical information about bananas of different quality (size, weight,\n            sweetness, softness, harvest time, ripeness, and acidity). The model must achieve at least 0.98\n           accuracy.\",\n      \"performance_metrics\": [\n         {\"name\": \"accuracy\", \"value\": 0.98}\n      ],\n      \"complexity_metrics\": []\n   },\n   \"dataset\": [\n      {\n         \"name\": \"banana_quality\",\n         \"modality\": [\"tabular\"],\n         \"target_variables\": [\"quality\"],\n         \"specification\": null,\n         \"description\": \"A dataset containing numerical information about bananas of different quality,\n              including size, weight, sweetness, softness, harvest time, ripeness, and acidity.\",\n         \"preprocessing\": [],\n         \"augmentation\": [],\n         \"visualization\": [],\n         \"source\": \"user-upload\"\n      }\n   ],\n   \"model\": [\n      {\n         \"name\": \"XGBoost\",\n         \"family\": \"ensemble models\",\n         \"type\": \"ensemble\",\n         \"specification\": null,\n         \"description\": \"A potential model to classify banana quality as Good or Bad, potentially using XGBoost\n              or LightGBM.\"\n      }\n   ],\n...\n}\n```\n#### <span id=\"page-29-0\"></span>D.2. Retrieval-Augmented Planning\n\n| Example Results from Retrieval-Augmented Planning with P = 3                                                                                                                                                                                                                                                                              |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| User Input: I need a very accurate model to classify images in the Butterfly Image Classification dataset into their<br>respective categories. The dataset has been uploaded with its label information in the labels.csv file.                                                                                                           |\n| Plan #1                                                                                                                                                                                                                                                                                                                                   |\n| ### End-to-End Actionable Plan for Butterfly Image Classification                                                                                                                                                                                                                                                                         |\n| **Objective**: Develop a highly accurate neural network model to classify images in the Butterfly Image<br>Classification dataset into their respective categories.                                                                                                                                                                       |\n| #### 1. Data Collection and Preprocessing<br>1. **Dataset Loading**:<br>- Load the Butterfly Image Classification dataset and the associated labels from 'labels.csv'.<br>- Verify the integrity of the dataset, ensuring all images have corresponding labels.                                                                           |\n| 2. **Data Preprocessing**:<br>- **Image Resizing**: Resize all images to a uniform size (e.g., 224x224 pixels) to ensure consistency.<br>- **Normalization**: Normalize pixel values to the range [0, 1] or standardize to have zero mean and unit<br>variance.                                                                           |\n| 3. **Data Augmentation**:<br>- Apply data augmentation techniques such as cropping, horizontal and vertical flips, rescaling, shear, zoom,<br>and rotation to increase dataset variability.<br>- Utilize libraries like TensorFlow's 'ImageDataGenerator' or PyTorch's 'transforms' for implementation.                                   |\n| #### 2. Model Selection and Architecture<br>1. **Transfer Learning**:<br>- Leverage pre-trained convolutional neural networks (CNNs) such as VGG16, VGG19, MobileNet, Xception,<br>ResNet50, and InceptionV3.<br>- Fine-tune these pre-trained models on the Butterfly Image Classification dataset to utilize their learned<br>features. |\n| 2. **Model Architecture**:<br>- Start with InceptionV3, known for high accuracy in similar tasks.<br>- Explore other models like ResNet, DenseNet, EfficientNet, and Vision Transformers.<br>- Consider combining CNNs with Transformer-based architectures to capture both local and global features.                                    |\n| 3. **Ensemble Methods**:<br>- Implement ensemble methods by combining predictions from multiple models to improve overall classification<br>performance.<br>- Use techniques like bagging or stacking to enhance model robustness.                                                                                                        |\n| #### 3. Model Training<br>1. **Dataset Splitting**:<br>- Split the dataset into training, validation, and testing sets (e.g., 80% training and 20% validation).                                                                                                                                                                           |\n| 2. **Training Configuration**:<br>- Use categorical cross-entropy as the loss function.<br>- Select optimizers like SGD or RMSprop.<br>- Set up callbacks for early stopping and learning rate reduction on plateau to prevent overfitting.                                                                                               |\n| 3. **Hyperparameter Tuning**:<br>- Perform hyperparameter tuning using libraries like PyCaret or Optuna to find the optimal configuration.<br>- Utilize techniques like grid search or random search for hyperparameter optimization.                                                                                                     |\n| #### 4. Model Evaluation and Validation<br>1. **Performance Metrics**:<br>- Regularly monitor accuracy as the primary performance metric.<br>- Additionally, track precision, recall, and F1-score to ensure balanced performance across all classes.                                                                                     |\n| 2. **Cross-Validation**:<br>- Employ k-fold cross-validation to assess the model's robustness and generalization ability.                                                                                                                                                                                                                 |\n| 3. **Handling Class Imbalance**:<br>- Address class imbalance through weighted loss functions or oversampling techniques to ensure fair<br>performance across all categories.                                                                                                                                                             |\n| #### 5. Interpretability and Debugging<br>1. **Model Interpretability**:<br>- Use interpretability methods like Grad-CAM or LIME to visualize which parts of the images influence the<br>model's predictions.                                                                                                                             |\n\n- Ensure the model focuses on relevant biological features of butterflies.\n\n#### 6. Reproducibility and Documentation 1. \\*\\*Reproducibility\\*\\*: - Set random seeds for all operations to ensure reproducibility of results. - Document the entire process, including data splits, model configurations, and training parameters. 2. \\*\\*Model Deployment\\*\\*: - Prepare the model for deployment by exporting it in a suitable format (e.g., TensorFlow SavedModel, ONNX). - Set up an inference engine and deployment endpoint to make predictions on new data. 3. \\*\\*Demonstration\\*\\*: - Provide a demonstration of the model's capabilities by creating a user-friendly interface or visualization to showcase its performance. By following this comprehensive plan, AI agents can develop a robust and accurate neural network model tailored for classifying butterfly images in the given dataset. Plan #2 ## End-to-End Actionable Plan for Butterfly Image Classification ### 1. Data Collection - \\*\\*Dataset Name\\*\\*: Butterfly Image Classification - \\*\\*Source\\*\\*: User-uploaded - \\*\\*Description\\*\\*: A dataset for classifying images of butterflies into their respective categories. - \\*\\*Label Information\\*\\*: Available in 'labels.csv' file. ### 2. Data Preprocessing - \\*\\*Image Preprocessing\\*\\*: - \\*\\*Resize Images\\*\\*: Resize all images to a consistent size (e.g., 224x224 pixels) to match the input size required by pre-trained models. - \\*\\*Normalization\\*\\*: Normalize pixel values to the range [0, 1] or use mean subtraction based on the pretrained model's requirements. - \\*\\*Handling High-Resolution and Illumination Variations\\*\\*: Apply techniques to standardize illumination and handle high-resolution images. ### 3. Data Augmentation - \\*\\*Techniques\\*\\*: - \\*\\*Cropping\\*\\*: Randomly crop sections of the images. - \\*\\*Flipping\\*\\*: Apply horizontal and vertical flips. - \\*\\*Rescaling\\*\\*: Rescale images by a factor. - \\*\\*Shearing\\*\\*: Apply shear transformations. - \\*\\*Zooming\\*\\*: Apply random zoom. - \\*\\*Rotation\\*\\*: Rotate images by random angles. - \\*\\*Libraries\\*\\*: Use TensorFlow's 'ImageDataGenerator' or PyTorch's 'transforms'. ### 4. Dataset Splitting - \\*\\*Training Set\\*\\*: 80% of the dataset - \\*\\*Validation Set\\*\\*: 20% of the dataset ### 5. Model Selection and Architecture - \\*\\*Transfer Learning\\*\\*: - \\*\\*Pre-trained Models\\*\\*: Utilize models such as VGG16, VGG19, MobileNet, Xception, ResNet50, and InceptionV3. - \\*\\*Fine-tuning\\*\\*: Fine-tune these models on the Butterfly Image Classification dataset. - \\*\\*Model Architecture\\*\\*: - \\*\\*Primary Model\\*\\*: Start with InceptionV3 due to its high performance in similar tasks. - \\*\\*Alternative Models\\*\\*: Evaluate ResNet, DenseNet, EfficientNet, and Vision Transformers. - \\*\\*Combined Architectures\\*\\*: Explore combining CNNs with Transformer-based architectures to capture both local and global features. ### 6. Handling Class Imbalance - \\*\\*Techniques\\*\\*: - \\*\\*Weighted Loss Functions\\*\\*: Assign higher weights to underrepresented classes. - \\*\\*Oversampling\\*\\*: Increase the number of samples for underrepresented classes. ### 7. Model Training - \\*\\*Loss Function\\*\\*: Use categorical cross-entropy. - \\*\\*Optimizers\\*\\*: Start with SGD or RMSprop. - \\*\\*Hyperparameter Tuning\\*\\*: - \\*\\*Libraries\\*\\*: Use PyCaret or Optuna for hyperparameter optimization. - \\*\\*Parameters\\*\\*: Tune learning rate, batch size, number of epochs, etc. - \\*\\*Cross-Validation\\*\\*: Use k-fold cross-validation to ensure model robustness and generalization. ### 8. Performance Evaluation - \\*\\*Metrics\\*\\*: - \\*\\*Accuracy\\*\\*: Primary metric for evaluation. - \\*\\*Precision, Recall, F1-Score\\*\\*: Monitor these metrics to ensure balanced performance across classes. - \\*\\*Evaluation Techniques\\*\\*:\n\n- \\*\\*Confusion Matrix\\*\\*: Visualize to understand misclassifications.\n\n```\n- **ROC Curve**: Evaluate model performance across different thresholds.\n### 9. Ensemble Methods and Knowledge Distillation\n- **Ensemble Methods**: Combine predictions from multiple models (e.g., patch-wise and image-wise classifiers).\n- **Knowledge Distillation**: Transfer knowledge from larger teacher models to smaller student models to create\n    compact yet powerful models.\n### 10. Interpretability and Debugging\n- **Techniques**:\n - **Grad-CAM**: Visualize which parts of the images influence the model's predictions.\n - **LIME**: Provide explanations for individual predictions.\n- **Purpose**: Ensure the model focuses on relevant biological features and understand model behavior.\n### 11. Reproducibility\n- **Random Seeds**: Set random seeds for all processes to ensure reproducibility.\n- **Documentation**: Document data splits, model configurations, and training parameters.\n### 12. Deployment\n- **Target Device**: Specify the target device for deployment (e.g., cloud, edge device).\n- **Deployment Endpoint**: Define the endpoint for model deployment.\n- **Inference Engine**: Choose an appropriate inference engine (e.g., TensorFlow Serving, TorchServe).\n- **Demonstration**: Prepare a demonstration of the model's capabilities and performance.\nBy following this comprehensive plan, AI agents can effectively build, train, and deploy a highly accurate\n    neural network model tailored for classifying butterfly images in the specified dataset.\nPlan #3\n### End-to-End Actionable Plan for Butterfly Image Classification using Neural Networks\n#### 1. **Project Initialization**\n**Objective**: Build a highly accurate image classification model for the Butterfly Image Classification dataset\n     in the biology domain.\n**User Expertise**: Medium\n---\n#### 2. **Data Collection and Preprocessing**\n**Dataset Source**: User-uploaded Butterfly Image Classification dataset with label information in 'labels.csv'.\n**Steps**:\n1. **Load Dataset**:\n  - Load images and labels from the specified dataset and 'labels.csv'.\n2. **Data Cleaning**:\n  - Ensure all images are correctly labeled.\n  - Remove any corrupted or unusable images.\n3. **Data Preprocessing**:\n  - Resize images to a consistent size (e.g., 224x224 pixels).\n  - Normalize pixel values to the range [0, 1].\n4. **Data Augmentation**:\n  - Apply augmentation techniques such as cropping, horizontal and vertical flips, rescaling, shear, zoom, and\n       rotation to increase dataset size and variability.\n  - Use libraries like TensorFlow's 'ImageDataGenerator' or PyTorch's 'transforms'.\n---\n#### 3. **Model Development**\n**Model Type**: Neural Networks\n**Steps**:\n1. **Leverage Transfer Learning**:\n  - Use pre-trained CNNs such as VGG16, VGG19, MobileNet, Xception, ResNet50, and InceptionV3.\n  - Fine-tune these models on the Butterfly Image Classification dataset.\n2. **Model Selection and Architecture**:\n  - Start with InceptionV3 due to its high accuracy in similar tasks.\n  - Experiment with other models like ResNet, DenseNet, EfficientNet, and Vision Transformers.\n  - Consider combining CNNs with Transformer-based architectures for improved performance.\n```\n#### AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML\n\n```\n3. **Ensemble Methods and Knowledge Distillation**:\n  - Implement ensemble methods by combining patch-wise and image-wise classifiers.\n  - Use knowledge distillation to transfer knowledge from larger models to smaller, more efficient models.\n---\n#### 4. **Training and Evaluation**\n**Steps**:\n1. **Dataset Splitting**:\n  - Split the dataset into training (80%), validation (10%), and testing (10%) sets.\n2. **Model Training**:\n  - Use categorical cross-entropy as the loss function.\n  - Utilize optimizers such as SGD or RMSprop.\n  - Implement early stopping and learning rate reduction on plateau to prevent overfitting.\n3. **Performance Monitoring**:\n  - Regularly monitor metrics such as accuracy, precision, recall, and F1-score.\n  - Use validation data to fine-tune hyperparameters.\n4. **Handling Class Imbalance**:\n  - Address class imbalance using weighted loss functions or oversampling techniques.\n5. **Hyperparameter Tuning and Cross-Validation**:\n  - Perform hyperparameter tuning with libraries like PyCaret or Optuna.\n  - Use cross-validation to ensure model robustness and generalization.\n---\n#### 5. **Model Interpretability and Debugging**\n**Steps**:\n1. **Interpretability**:\n  - Use Grad-CAM or LIME to visualize important image regions influencing model predictions.\n  - Ensure the model focuses on relevant biological features.\n2. **Debugging**:\n  - Analyze misclassified images to understand model limitations.\n  - Iterate on model improvements based on these insights.\n---\n#### 6. **Reproducibility and Documentation**\n**Steps**:\n1. **Reproducibility**:\n  - Set random seeds for all processes to ensure reproducibility.\n  - Document all steps, including data splits, model configurations, and training parameters.\n2. **Version Control**:\n  - Use version control systems (e.g., Git) to track changes and collaborate efficiently.\n---\n#### 7. **Deployment and Inference**\n**Steps**:\n1. **Model Export**:\n  - Export the trained model in a suitable format (e.g., TensorFlow SavedModel, ONNX).\n2. **Deployment**:\n  - Choose a deployment platform (e.g., AWS SageMaker, Google AI Platform).\n  - Set up the deployment endpoint for real-time inference.\n3. **Inference Engine**:\n  - Optimize the model for the target device (e.g., mobile, edge devices) if necessary.\n  - Implement efficient inference pipelines to handle incoming image data.\n---\n#### 8. **Demonstration and User Feedback**\n**Steps**:\n```\n\n```\n1. **Demonstration**:\n  - Create a demo application to showcase the model's capabilities.\n  - Provide a user-friendly interface for uploading images and viewing classification results.\n2. **User Feedback**:\n  - Collect feedback from users to identify areas for improvement.\n  - Implement iterative updates based on user insights.\n---\nBy following this detailed plan, AI agents can build a robust and accurate neural network model for classifying\n    butterfly images in the biology domain, ensuring high performance and user satisfaction.\n```\n#### <span id=\"page-33-0\"></span>D.3. Plan Decomposition\n\n| Example Results from Plan Decomposition                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Data Agent                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| ### Summary of the Plan for Fashion Image Classification Using PyTorch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| #### 1. Dataset Retrieval and Collection<br>- **Retrieve Dataset**: Load the image paths and corresponding labels from the provided 'train_labels.csv' and '<br>test_labels.csv' files. Ensure the dataset includes images of four clothing categories.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| #### 2. Checking Dataset Types, Shapes, and Properties<br>- **Data Types**: Verify that the images are in the correct format (e.g., JPEG, PNG) and the labels are correctly<br>mapped to the images.<br>- **Shapes**: Check the dimensions of the images. If images are in grayscale, convert them to RGB by replicating<br>the single channel across three channels.<br>- **Properties**: Ensure that the dataset is balanced across the four clothing categories and that there are no<br>missing or corrupted files.                                                                                                                                                                                                                                                 |\n| #### 3. Preprocessing the Dataset<br>- **Normalization**: Scale pixel values to the range [0, 1].<br>- **Resizing**: Resize all images to a consistent size (e.g., 224x224 pixels) suitable for model input.<br>- **Splitting**: Split the training data into training and validation sets (e.g., 80% training, 20% validation)<br>to monitor the model's performance and prevent overfitting.                                                                                                                                                                                                                                                                                                                                                                          |\n| #### 4. Efficient Data Augmentation<br>- **Techniques**: Apply data augmentation techniques to increase the diversity and robustness of the training<br>data. Techniques include:<br>- Random horizontal flip<br>- Normalization<br>- Random cropping<br>- Random erasing<br>- Rotating<br>- Shifting<br>- Zooming<br>- **Pipelines**:<br>- **Pipeline 1**: Random horizontal flip and normalization.<br>- **Pipeline 2**: Random horizontal flip, normalization, random cropping, and random erasing.                                                                                                                                                                                                                                                                  |\n| #### 5. Extracting and Understanding Underlying Characteristics<br>- **Class Distribution**: Analyze the distribution of classes to ensure a balanced dataset.<br>- **Image Quality**: Check for variations in image quality and resolution.<br>- **Data Insights**: Identify any patterns or anomalies in the data that could affect model performance.<br>- **Performance Metrics**: Use metrics such as accuracy and confusion matrix to evaluate the model's performance<br>and understand misclassification patterns.<br>By following these steps, another data scientist should be able to reproduce the results and build a robust<br>image classification model that meets the specified accuracy requirements and provides insightful<br>performance analysis. |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n| Model Agent                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| ### Summary of the Plan for Fashion Image Classification Using PyTorch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| #### User's Requirements<br>The user needs a classification model that categorizes images into one of four clothing categories. The model<br>should achieve at least 85% accuracy on the test set and be implemented using PyTorch. Data augmentation<br>techniques and a confusion matrix for evaluation are also required.                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| ### Project Plan Overview                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n\n#### 1. Data Preparation - \\*\\*Load Dataset\\*\\*: Load image paths and labels from 'train\\_labels.csv' and 'test\\_labels.csv'. - \\*\\*Convert Grayscale to RGB\\*\\*: Convert grayscale images to RGB. - \\*\\*Preprocessing\\*\\*: Normalize pixel values to [0, 1] and resize images to 224x224 pixels. - \\*\\*Split Dataset\\*\\*: Split training data into training (80%) and validation (20%) sets. #### 2. Data Augmentation - \\*\\*Techniques\\*\\*: Apply random horizontal flip, normalization, random cropping, random erasing, rotating, shifting, and zooming. - \\*\\*Pipelines\\*\\*: Develop two preprocessing pipelines with different augmentation techniques. #### 3. Model Architecture - \\*\\*Simple CNN\\*\\*: Start with a simple CNN model. - \\*\\*Advanced Models\\*\\*: Consider ResNet18, EfficientNet, or Vision Transformers (ViTs). - \\*\\*Hybrid Models\\*\\*: Explore hybrid models combining CNNs and ViTs. #### 4. Training - \\*\\*Optimizer and Loss Function\\*\\*: Use SGD or Adam optimizer and Cross-Entropy Loss. - \\*\\*Epochs and Early Stopping\\*\\*: Train for up to 150 epochs with early stopping. - \\*\\*Monitoring\\*\\*: Monitor validation accuracy and loss curves. #### 5. Evaluation - \\*\\*Accuracy\\*\\*: Ensure at least 85% accuracy on the test set. - \\*\\*Confusion Matrix\\*\\*: Generate a confusion matrix. - \\*\\*Top-1 Accuracy\\*\\*: Calculate top-1 accuracy. #### 6. Implementation - \\*\\*Framework\\*\\*: Use PyTorch and torchvision.transforms.v2. - \\*\\*Resources\\*\\*: Utilize resources from Kaggle and PapersWithCode. - \\*\\*Deployment\\*\\*: Prepare the model for deployment if needed. #### 7. Visualization and Reporting - \\*\\*Confusion Matrix\\*\\*: Visualize the confusion matrix. - \\*\\*Performance Metrics\\*\\*: Report accuracy and other metrics. - \\*\\*Documentation\\*\\*: Document the entire process. ### Detailed Steps for Data Manipulation and Analysis #### 1. Dataset Retrieval and Collection - \\*\\*Retrieve Dataset\\*\\*: Load image paths and labels from 'train\\_labels.csv' and 'test\\_labels.csv'. #### 2. Checking Dataset Types, Shapes, and Properties - \\*\\*Data Types\\*\\*: Verify image formats and label mappings. - \\*\\*Shapes\\*\\*: Check and convert image dimensions to RGB if needed. - \\*\\*Properties\\*\\*: Ensure balanced dataset and data integrity. #### 3. Preprocessing the Dataset - \\*\\*Normalization\\*\\*: Scale pixel values to [0, 1]. - \\*\\*Resizing\\*\\*: Resize images to 224x224 pixels. - \\*\\*Splitting\\*\\*: Split data into training and validation sets. #### 4. Efficient Data Augmentation - \\*\\*Techniques\\*\\*: Apply various augmentation techniques. - \\*\\*Pipelines\\*\\*: Create two augmentation pipelines. #### 5. Extracting and Understanding Underlying Characteristics - \\*\\*Class Distribution\\*\\*: Analyze class distribution. - \\*\\*Image Quality\\*\\*: Assess variations in image quality. - \\*\\*Data Insights\\*\\*: Identify patterns or anomalies. - \\*\\*Performance Metrics\\*\\*: Evaluate model performance using accuracy and confusion matrix. ### Expected Outcomes and Results - \\*\\*Quantitative\\*\\*: Balanced dataset, consistent image size, normalized pixel values, augmented data. - \\*\\*Qualitative\\*\\*: Data integrity, improved model generalization, insightful analysis. ### Responsibilities Fulfillment #### 1. How to Retrieve or Find High-Performance Model(s)? - \\*\\*Initial Models\\*\\*: Start with simple CNN architectures. - \\*\\*Advanced Models\\*\\*: Experiment with ResNet18, EfficientNet, and Vision Transformers. - \\*\\*Hybrid Models\\*\\*: Explore combinations of CNNs and ViTs. - \\*\\*Resources\\*\\*: Utilize code repositories from Kaggle and PapersWithCode for implementation and fine-tuning. #### 2. How to Optimize the Hyperparameters of the Retrieved Models? - \\*\\*Optimizers\\*\\*: Use SGD with a learning rate of 0.015 or Adam optimizer. - \\*\\*Hyperparameter Tuning\\*\\*: Experiment with different learning rates, batch sizes, and augmentation techniques. - \\*\\*Early Stopping\\*\\*: Implement early stopping based on validation loss to prevent overfitting. - \\*\\*Monitoring\\*\\*: Continuously monitor validation accuracy and loss curves.\n\n35\n\n#### 3. How to Extract and Understand the Underlying Characteristics of the Dataset(s)? - \\*\\*Class Distribution\\*\\*: Use 'pandas' to analyze the distribution of classes. - \\*\\*Image Quality\\*\\*: Assess image quality and resolution using 'PIL' or 'OpenCV'. - \\*\\*Data Insights\\*\\*: Perform exploratory data analysis (EDA) to identify patterns or anomalies. - \\*\\*Performance Metrics\\*\\*: Use 'sklearn.metrics' to evaluate model performance and generate confusion matrices. #### 4. How to Select the Top-k Models or Algorithms Based on the Given Plans? - \\*\\*Evaluation Metrics\\*\\*: Use accuracy and confusion matrix to evaluate models. - \\*\\*Top-k Selection\\*\\*: Rank models based on their performance metrics and select the top-k models that meet or exceed the 85% accuracy requirement. - \\*\\*Documentation\\*\\*: Document the performance of each model, including accuracy, confusion matrix, and any other relevant metrics. By following this comprehensive plan, you will be able to retrieve high-performance models, optimize their hyperparameters, understand the dataset's characteristics, and select the top-k models based on their performance.\n\n#### <span id=\"page-35-0\"></span>D.4. Agent-Specific Plan Execution Outcomes\n\n#### Example Results from Prompting-Based Plan Execution by Each Agent\n\n#### Data Agent\n\n### Detailed Steps for Data Manipulation and Analysis\n\n#### 1. Dataset Retrieval and Collection\n\n\\*\\*Retrieve Dataset\\*\\*:\n\n- \\*\\*Source\\*\\*: The dataset is available at '\\_experiments/datasets/shopee'.\n- \\*\\*Structure\\*\\*: The directory structure includes 'train', 'test', 'train\\_labels.csv', and 'test\\_labels.csv'. - \\*\\*Loading Data\\*\\*: Load the image paths and corresponding labels from 'train\\_labels.csv' and 'test\\_labels.csv'. This can be done using libraries like 'pandas' to read the CSV files and 'os' to navigate through the directories.\n- \\*\\*Why\\*\\*: This step ensures that we have access to the correct dataset and that the paths and labels are correctly mapped for further processing.\n\n#### 2. Checking Dataset Types, Shapes, and Properties\n\n\\*\\*Data Types\\*\\*:\n\n- \\*\\*Image Format\\*\\*: Verify that images are in the correct format (JPEG, PNG). This can be done using libraries like 'PIL' (Python Imaging Library).\n- \\*\\*Labels\\*\\*: Ensure labels are correctly mapped to images by checking the contents of the CSV files.\n\n\\*\\*Shapes\\*\\*:\n\n- \\*\\*Image Dimensions\\*\\*: Check the dimensions of the images. If images are in grayscale, convert them to RGB by replicating the single channel across three channels using libraries like 'PIL' or 'OpenCV'.\n\n\\*\\*Properties\\*\\*:\n\n- \\*\\*Balanced Dataset\\*\\*: Ensure the dataset is balanced across the four clothing categories by analyzing the label distribution using 'pandas'.\n- \\*\\*Integrity Check\\*\\*: Verify that there are no missing or corrupted files by attempting to open each image file.\n- \\*\\*Why\\*\\*: Ensuring correct data types, shapes, and properties is crucial for accurate model training and prevents errors during preprocessing and augmentation.\n\n#### 3. Preprocessing the Dataset\n\n#### \\*\\*Normalization\\*\\*:\n\n- \\*\\*Scale Pixel Values\\*\\*: Normalize pixel values to the range [0, 1] using libraries like 'PIL' or 'torchvision. transforms'.\n\n\\*\\*Resizing\\*\\*:\n\n- \\*\\*Consistent Size\\*\\*: Resize all images to a consistent size (e.g., 224x224 pixels) suitable for model input using 'PIL' or 'torchvision.transforms'.\n\n\\*\\*Splitting\\*\\*:\n\n- \\*\\*Train-Validation Split\\*\\*: Split the training data into training and validation sets (e.g., 80% training, 20% validation) using 'sklearn.model\\_selection.train\\_test\\_split'.\n- \\*\\*Why\\*\\*: Normalization ensures that the pixel values are on a common scale, which helps in faster convergence during training. Resizing ensures that all images have the same dimensions, which is a requirement for most deep learning models. Splitting the data helps in monitoring the model's performance and prevents overfitting.\n\n#### 4. Efficient Data Augmentation \\*\\*Techniques\\*\\*: - \\*\\*Random Horizontal Flip\\*\\*: Randomly flip images horizontally to increase diversity. - \\*\\*Normalization\\*\\*: Normalize images to have zero mean and unit variance. - \\*\\*Random Cropping\\*\\*: Randomly crop images to introduce variability. - \\*\\*Random Erasing\\*\\*: Randomly erase parts of images to make the model robust to occlusions. - \\*\\*Rotating\\*\\*: Rotate images randomly to make the model invariant to orientation. - \\*\\*Shifting\\*\\*: Shift images randomly to simulate different viewpoints. - \\*\\*Zooming\\*\\*: Zoom in and out of images to introduce scale variability. \\*\\*Pipelines\\*\\*: - \\*\\*Pipeline 1\\*\\*: Apply random horizontal flip and normalization. - \\*\\*Pipeline 2\\*\\*: Apply random horizontal flip, normalization, random cropping, and random erasing. \\*\\*Why\\*\\*: Data augmentation increases the diversity and robustness of the training data, which helps in improving the generalization of the model. #### 5. Extracting and Understanding Underlying Characteristics \\*\\*Class Distribution\\*\\*: - \\*\\*Analyze Distribution\\*\\*: Use 'pandas' to analyze the distribution of classes to ensure a balanced dataset. \\*\\*Image Quality\\*\\*: - \\*\\*Check Variations\\*\\*: Assess variations in image quality and resolution using 'PIL' or 'OpenCV'. \\*\\*Data Insights\\*\\*: - \\*\\*Identify Patterns\\*\\*: Use exploratory data analysis (EDA) techniques to identify any patterns or anomalies in the data that could affect model performance. \\*\\*Performance Metrics\\*\\*: - \\*\\*Evaluate Model\\*\\*: Use metrics such as accuracy and confusion matrix to evaluate the model's performance and understand misclassification patterns. This can be done using 'sklearn.metrics'. \\*\\*Why\\*\\*: Understanding the underlying characteristics of the data helps in identifying potential issues that could affect model performance. Analyzing class distribution ensures that the dataset is balanced, which is crucial for training a fair and unbiased model. ### Expected Outcomes and Results \\*\\*Quantitative\\*\\*: - \\*\\*Balanced Dataset\\*\\*: A dataset with an equal number of images for each clothing category. - \\*\\*Consistent Image Size\\*\\*: All images resized to 224x224 pixels. - \\*\\*Normalized Pixel Values\\*\\*: Pixel values scaled to the range [0, 1]. - \\*\\*Augmented Data\\*\\*: Increased dataset diversity through augmentation techniques. \\*\\*Qualitative\\*\\*: - \\*\\*Data Integrity\\*\\*: No missing or corrupted files. - \\*\\*Improved Model Generalization\\*\\*: Enhanced model robustness due to diverse and augmented training data. - \\*\\*Insightful Analysis\\*\\*: Clear understanding of class distribution, image quality, and potential data patterns or anomalies. By following these detailed steps, another data scientist should be able to reproduce the results and build a robust image classification model that meets the specified accuracy requirements and provides insightful performance analysis. Model Agent ### Detailed Steps for Modeling and Optimization #### 1. Retrieving High-Performance Models \\*\\*Initial Models:\\*\\* - \\*\\*Simple CNN\\*\\*: Start with a basic Convolutional Neural Network (CNN) architecture. This model will serve as a baseline to understand the dataset and the task. \\*\\*Advanced Models:\\*\\* - \\*\\*ResNet18\\*\\*: A well-known deep learning model that has shown excellent performance in image classification tasks. It uses residual connections to ease the training of deeper networks. - \\*\\*EfficientNet\\*\\*: Known for its efficiency in terms of parameter count and FLOPs while maintaining high accuracy. EfficientNet-B0 can be a good starting point. - \\*\\*Vision Transformers (ViTs)\\*\\*: These models have gained popularity for their performance in image classification tasks. A small variant like ViT-Small can be considered. \\*\\*Hybrid Models:\\*\\* - \\*\\*CNN + ViT\\*\\*: Combining the feature extraction capabilities of CNNs with the global context understanding of ViTs. This hybrid approach can potentially leverage the strengths of both architectures.\n\n#### 2. Optimizing Hyperparameters \\*\\*Optimizers:\\*\\* - \\*\\*SGD\\*\\*: Stochastic Gradient Descent with a learning rate of 0.015. - \\*\\*Adam\\*\\*: Adaptive Moment Estimation with a learning rate of 0.001. \\*\\*Hyperparameter Tuning:\\*\\* - \\*\\*Learning Rate\\*\\*: Experiment with learning rates (0.001, 0.005, 0.01, 0.015) to find the optimal rate for convergence. - \\*\\*Batch Size\\*\\*: Test batch sizes (16, 32, 64) to balance between memory usage and training speed. - \\*\\*Data Augmentation Techniques\\*\\*: Apply different augmentation techniques and observe their impact on validation accuracy. - \\*\\*Number of Epochs\\*\\*: Train for up to 150 epochs with early stopping based on validation loss to prevent overfitting. - \\*\\*Weight Decay\\*\\*: Regularization parameter for SGD, set to 0.0005. \\*\\*Optimal Values:\\*\\* - \\*\\*Learning Rate\\*\\*: 0.001 for Adam, 0.015 for SGD. - \\*\\*Batch Size\\*\\*: 32. - \\*\\*Weight Decay\\*\\*: 0.0005. - \\*\\*Number of Epochs\\*\\*: Up to 150 with early stopping. #### 3. Extracting and Understanding Characteristics \\*\\*Computation Complexity:\\*\\* - \\*\\*Number of Parameters\\*\\*: Calculate the total number of trainable parameters in each model. - \\*\\*FLOPs (Floating Point Operations per Second)\\*\\*: Measure the computational complexity of each model. \\*\\*Memory Usage:\\*\\* - \\*\\*Model Size\\*\\*: Determine the size of the model in memory (MB). - \\*\\*Training Memory Usage\\*\\*: Monitor GPU memory usage during training. \\*\\*Inference Latency:\\*\\* - \\*\\*Inference Speed\\*\\*: Measure the time taken for a single forward pass (in milliseconds). \\*\\*Why Each Step is Essential:\\*\\* - \\*\\*Model Selection\\*\\*: Choosing a variety of models ensures that we explore different architectures and their strengths. - \\*\\*Hyperparameter Tuning\\*\\*: Fine-tuning hyperparameters is crucial for achieving optimal performance and ensuring the model generalizes well. - \\*\\*Characteristic Analysis\\*\\*: Understanding the computational and memory requirements helps in selecting models that are not only accurate but also efficient and deployable. ### Candidate Models and Expected Performance \\*\\*1. ResNet18\\*\\* - \\*\\*Accuracy\\*\\*: Expected to achieve around 88% on the test set. - \\*\\*Number of Parameters\\*\\*: Approximately 11.7 million. - \\*\\*FLOPs\\*\\*: Around 1.8 billion. - \\*\\*Model Size\\*\\*: Approximately 44 MB. - \\*\\*Training Time\\*\\*: Approximately 2 hours on a single GPU. - \\*\\*Inference Speed\\*\\*: Around 10 milliseconds per image. \\*\\*2. EfficientNet-B0\\*\\* - \\*\\*Accuracy\\*\\*: Expected to achieve around 87% on the test set. - \\*\\*Number of Parameters\\*\\*: Approximately 5.3 million. - \\*\\*FLOPs\\*\\*: Around 390 million. - \\*\\*Model Size\\*\\*: Approximately 20 MB. - \\*\\*Training Time\\*\\*: Approximately 3 hours on a single GPU. - \\*\\*Inference Speed\\*\\*: Around 8 milliseconds per image. \\*\\*3. Vision Transformer (ViT-Small)\\*\\* - \\*\\*Accuracy\\*\\*: Expected to achieve around 86% on the test set. - \\*\\*Number of Parameters\\*\\*: Approximately 22 million. - \\*\\*FLOPs\\*\\*: Around 4.6 billion. - \\*\\*Model Size\\*\\*: Approximately 85 MB. - \\*\\*Training Time\\*\\*: Approximately 4 hours on a single GPU. - \\*\\*Inference Speed\\*\\*: Around 15 milliseconds per image. By following these detailed steps and focusing on the modeling and optimization parts, you will be able to identify and select the top-3 high-performance models for the given image classification task.\n\n### <span id=\"page-38-0\"></span>E. Showcase of Generated Pipelines\n\n```\nGenerated Pipeline under Constraint-Free Setting by AutoML-Agent\nUser Prompt: Please provide a classification model that categorizes images into one of four clothing categories. The\nimage path, along with its label information, can be found in the files train labels.csv and test labels.csv.\nOutput Pipeline:\nimport os\nimport random\nimport time\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport gradio as gr\n# Define constants\nSEED = 42\nDATASET_PATH = \"_experiments/datasets/shopee\"\nTRAIN_LABELS_CSV = os.path.join(DATASET_PATH, \"train_labels.csv\")\nTEST_LABELS_CSV = os.path.join(DATASET_PATH, \"test_labels.csv\")\nTRAIN_DIR = os.path.join(DATASET_PATH)\nTEST_DIR = os.path.join(DATASET_PATH)\nMODEL_SAVE_PATH = \"./agent_workspace/trained_models/resnet50_finetuned.pth\"\n# Set seeds for reproducibility\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n# Define device for model operations\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Custom dataset class\nclass ShopeeDataset(Dataset):\n   def __init__(self, csv_file, root_dir, transform=None):\n      self.labels_df = pd.read_csv(csv_file)\n      self.root_dir = root_dir\n      self.transform = transform\n   def __len__(self):\n      return len(self.labels_df)\n   def __getitem__(self, idx):\n      img_name = os.path.join(self.root_dir, self.labels_df.iloc[idx, 0])\n      image = Image.open(img_name).convert(\"RGB\")\n      label = self.labels_df.iloc[idx, 1]\n      if self.transform:\n         image = self.transform(image)\n      return image, label\n# Data preprocessing and feature engineering\ndef preprocess_data():\n   transform = transforms.Compose([\n      transforms.Resize((28, 28)),\n      transforms.ToTensor(),\n      transforms.Normalize((0.5,), (0.5,))\n   ])\n   train_dataset = ShopeeDataset(csv_file=TRAIN_LABELS_CSV, root_dir=TRAIN_DIR, transform=transform)\n   test_dataset = ShopeeDataset(csv_file=TEST_LABELS_CSV, root_dir=TEST_DIR, transform=transform)\n   train_size = int(0.7 * len(train_dataset))\n   valid_size = int(0.2 * len(train_dataset))\n   test_size = len(train_dataset) - train_size - valid_size\n```\n\n```\ntrain_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(train_dataset, [train_size,\n       valid_size, test_size])\n   train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n   valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n   test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n   return train_loader, valid_loader, test_loader\n# Model training\ndef train_model(model, train_loader, valid_loader):\n   criterion = nn.CrossEntropyLoss()\n   optimizer = optim.Adam(model.parameters(), lr=0.00001)\n   model.to(device)\n   best_accuracy = 0.0\n   for epoch in range(100):\n      model.train()\n      running_loss = 0.0\n      for inputs, labels in train_loader:\n         inputs, labels = inputs.to(device), labels.to(device)\n         optimizer.zero_grad()\n         outputs = model(inputs)\n         loss = criterion(outputs, labels)\n         loss.backward()\n         optimizer.step()\n         running_loss += loss.item() * inputs.size(0)\n      epoch_loss = running_loss / len(train_loader.dataset)\n      model.eval()\n      correct = 0\n      total = 0\n      with torch.no_grad():\n         for inputs, labels in valid_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n      accuracy = correct / total\n      if accuracy > best_accuracy:\n         best_accuracy = accuracy\n         torch.save(model.state_dict(), MODEL_SAVE_PATH)\n      print(f\"Epoch {epoch+1}/{100}, Loss: {epoch_loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n   model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n   return model\n# Model evaluation\ndef evaluate_model(model, test_loader):\n   model.eval()\n   y_true = []\n   y_pred = []\n   with torch.no_grad():\n      for inputs, labels in test_loader:\n         inputs, labels = inputs.to(device), labels.to(device)\n         outputs = model(inputs)\n         _, predicted = torch.max(outputs, 1)\n         y_true.extend(labels.cpu().numpy())\n         y_pred.extend(predicted.cpu().numpy())\n   performance_scores = {\n      'Accuracy': accuracy_score(y_true, y_pred),\n      'Precision': precision_score(y_true, y_pred, average='weighted'),\n      'Recall': recall_score(y_true, y_pred, average='weighted'),\n      'F1': f1_score(y_true, y_pred, average='weighted')\n   }\n   model_size = os.path.getsize(MODEL_SAVE_PATH) / (1024 * 1024) # in MB\n   start_time = time.time()\n   for _ in range(100):\n      _ = model(inputs)\n```\n\n```\ninference_time = (time.time() - start_time) / 100 # average inference time\n   complexity_scores = {\n      'Model Size (MB)': model_size,\n      'Inference Time (ms)': inference_time * 1000\n   }\n   return performance_scores, complexity_scores\n# Prepare model for deployment\ndef prepare_model_for_deployment(model):\n   model.eval()\n   scripted_model = torch.jit.script(model)\n   scripted_model.save(MODEL_SAVE_PATH.replace(\".pth\", \"_scripted.pt\"))\n   return scripted_model\n# Deploy model using Gradio\ndef deploy_model(model):\n   def predict(image):\n      image = transforms.Compose([\n         transforms.Resize((28, 28)),\n         transforms.ToTensor(),\n         transforms.Normalize((0.5,), (0.5,))\n      ])(image).unsqueeze(0).to(device)\n      with torch.no_grad():\n         outputs = model(image)\n         _, predicted = torch.max(outputs, 1)\n      return predicted.item()\n   interface = gr.Interface(fn=predict, inputs=gr.Image(type=\"pil\"), outputs=\"label\")\n   url_endpoint = interface.launch(share=True)\n   return url_endpoint\n# The main function to orchestrate the data loading, data preprocessing, feature engineering, model training,\n    model preparation, model deployment, and model evaluation\ndef main():\n   # Step 1: Retrieve or load dataset\n   train_loader, valid_loader, test_loader = preprocess_data()\n   # Step 2: Define model\n   model = models.resnet50(pretrained=True)\n   num_ftrs = model.fc.in_features\n   model.fc = nn.Linear(num_ftrs, 4) # Assuming 4 classes\n   # Step 3: Train model\n   model = train_model(model, train_loader, valid_loader)\n   # Step 4: Evaluate model\n   model_performance, model_complexity = evaluate_model(model, test_loader)\n   # Step 5: Prepare model for deployment\n   deployable_model = prepare_model_for_deployment(model)\n   # Step 6: Deploy model\n   url_endpoint = deploy_model(deployable_model)\n   return train_loader, valid_loader, test_loader, model, deployable_model, url_endpoint, model_performance,\n       model_complexity\nif __name__ == \"__main__\":\n   train_loader, valid_loader, test_loader, model, deployable_model, url_endpoint, model_performance,\n       model_complexity = main()\n   print(\"Model Performance on Test Set:\", model_performance)\n   print(\"Model Complexity:\", model_complexity)\n   print(\"Deployment URL:\", url_endpoint)\n```\n#### Generated Pipeline for Constraint-Aware Setting by *AutoML-Agent*\n\nUser Prompt: Please provide a classification model that categorizes images into one of four clothing categories. The image path, along with its label information, can be found in the files train labels.csv and test labels.csv. The model should achieve at least 85% accuracy on the test set and be implemented using PyTorch. Additionally, please include data augmentation techniques and a confusion matrix in the evaluation.\n\n```\nOutput Pipeline:\nimport os\nimport random\nimport time\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gradio as gr\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n# Define device for model operations\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDATASET_PATH = \"_experiments/datasets/shopee\" # path for saving and loading dataset(s)\n# Custom Dataset class\nclass ShopeeDataset(Dataset):\n   def __init__(self, dataframe, transform=None):\n      self.dataframe = dataframe\n      self.transform = transform\n   def __len__(self):\n      return len(self.dataframe)\n   def __getitem__(self, idx):\n      img_path = self.dataframe.iloc[idx]['image']\n      label = self.dataframe.iloc[idx]['label']\n      image = Image.open(img_path).convert('RGB')\n      if self.transform:\n         image = self.transform(image)\n      return image, label\n# Data preprocessing and feature engineering\ndef preprocess_data():\n   train_labels = pd.read_csv(os.path.join(DATASET_PATH, 'train_labels.csv'))\n   test_labels = pd.read_csv(os.path.join(DATASET_PATH, 'test_labels.csv'))\n   train_labels['image'] = train_labels['image'].apply(lambda x: os.path.join(DATASET_PATH, x))\n   test_labels['image'] = test_labels['image'].apply(lambda x: os.path.join(DATASET_PATH, x))\n   # Split the data\n   train_data, val_data = train_test_split(train_labels, test_size=0.2, stratify=train_labels['label'])\n   val_data, test_data = train_test_split(val_data, test_size=0.5, stratify=val_data['label'])\n   # Define transformations\n   train_transforms = transforms.Compose([\n      transforms.RandomResizedCrop(224),\n      transforms.RandomHorizontalFlip(),\n      transforms.RandomRotation(10),\n      transforms.ToTensor(),\n      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n   ])\n   val_test_transforms = transforms.Compose([\n      transforms.Resize(256),\n      transforms.CenterCrop(224),\n      transforms.ToTensor(),\n      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n   ])\n   # Load datasets\n   train_dataset = ShopeeDataset(train_data, transform=train_transforms)\n```\n\n```\nval_dataset = ShopeeDataset(val_data, transform=val_test_transforms)\n   test_dataset = ShopeeDataset(test_data, transform=val_test_transforms)\n   train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n   val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n   test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n   return train_loader, val_loader, test_loader\ndef train_model(model, train_loader, val_loader):\n   criterion = nn.CrossEntropyLoss()\n   optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0001)\n   num_epochs = 100\n   best_val_loss = float('inf')\n   early_stop_count = 0\n   early_stop_patience = 10\n   for epoch in range(num_epochs):\n      model.train()\n      running_loss = 0.0\n      for inputs, labels in train_loader:\n         inputs, labels = inputs.to(device), labels.to(device)\n         optimizer.zero_grad()\n         outputs = model(inputs)\n         loss = criterion(outputs, labels)\n         loss.backward()\n         optimizer.step()\n         running_loss += loss.item() * inputs.size(0)\n      epoch_loss = running_loss / len(train_loader.dataset)\n      # Validation\n      model.eval()\n      val_loss = 0.0\n      with torch.no_grad():\n         for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n      val_loss = val_loss / len(val_loader.dataset)\n      print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n      # Early stopping\n      if val_loss < best_val_loss:\n         best_val_loss = val_loss\n         torch.save(model.state_dict(), './agent_workspace/trained_models/best_model.pth')\n         early_stop_count = 0\n      else:\n         early_stop_count += 1\n         if early_stop_count >= early_stop_patience:\n            print(\"Early stopping\")\n            break\n   return model\ndef evaluate_model(model, test_loader):\n   model.load_state_dict(torch.load('./agent_workspace/trained_models/best_model.pth'))\n   model.eval()\n   all_preds = []\n   all_labels = []\n   with torch.no_grad():\n      for inputs, labels in test_loader:\n         inputs, labels = inputs.to(device), labels.to(device)\n         outputs = model(inputs)\n         _, preds = torch.max(outputs, 1)\n         all_preds.extend(preds.cpu().numpy())\n         all_labels.extend(labels.cpu().numpy())\n   accuracy = accuracy_score(all_labels, all_preds)\n   f1 = f1_score(all_labels, all_preds, average='weighted')\n   performance_scores = {\n      'ACC': accuracy,\n      'F1': f1\n```\n\n```\n}\n   # Measure model complexity\n   start_time = time.time()\n   with torch.no_grad():\n      for inputs, _ in test_loader:\n         inputs = inputs.to(device)\n         _ = model(inputs)\n   inference_time = (time.time() - start_time) / len(test_loader.dataset)\n   model_size = os.path.getsize('./agent_workspace/trained_models/best_model.pth') / 1e6 # in MB\n   complexity_scores = {\n      'Inference Time': inference_time,\n      'Model Size (MB)': model_size\n   }\n   # Generate confusion matrix\n   conf_matrix = confusion_matrix(all_labels, all_preds)\n   plt.figure(figsize=(10, 7))\n   sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n   plt.xlabel('Predicted')\n   plt.ylabel('True')\n   plt.title('Confusion Matrix')\n   plt.show()\n   return performance_scores, complexity_scores\ndef prepare_model_for_deployment(model):\n   # Assuming the model is already saved in the best_model.pth file\n   model.load_state_dict(torch.load('./agent_workspace/trained_models/best_model.pth'))\n   model.eval()\n   return model\ndef deploy_model(model):\n   def predict(image):\n      transform = transforms.Compose([\n         transforms.Resize(256),\n         transforms.CenterCrop(224),\n         transforms.ToTensor(),\n         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      ])\n      image = transform(image).unsqueeze(0).to(device)\n      with torch.no_grad():\n         output = model(image)\n         _, pred = torch.max(output, 1)\n      return pred.item()\n   interface = gr.Interface(fn=predict, inputs=gr.Image(type=\"pil\"), outputs=\"label\")\n   url_endpoint = interface.launch(share=True)\n   return url_endpoint\n# The main function to orchestrate the data loading, data preprocessing, feature engineering, model training,\n    model preparation, model deployment, and model evaluation\ndef main():\n   \"\"\"\n   Main function to execute the image classification pipeline.\n   \"\"\"\n   # Step 1. Retrieve or load a dataset from hub (if available) or user's local storage (if given)\n   train_loader, val_loader, test_loader = preprocess_data()\n   # Step 2. Define required model\n   model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n   num_ftrs = model.fc.in_features\n   model.fc = nn.Linear(num_ftrs, 4)\n   model = model.to(device)\n   # Step 3. Train the retrieved/loaded model\n   model = train_model(model, train_loader, val_loader)\n   # Step 4. Evaluate the trained model\n   model_performance, model_complexity = evaluate_model(model, test_loader)\n   # Step 5. Prepare the model for deployment\n   deployable_model = prepare_model_for_deployment(model)\n   # Step 6. Deploy the model\n   url_endpoint = deploy_model(deployable_model)\n```\n\n```\nreturn (\n      train_loader,\n      val_loader,\n      test_loader,\n      model,\n      deployable_model,\n      url_endpoint,\n      model_performance,\n      model_complexity,\n   )\nif __name__ == \"__main__\":\n   train_loader, val_loader, test_loader, model, deployable_model, url_endpoint, model_performance,\n        model_complexity = main()\n   print(\"Model Performance on Test Set:\", model_performance)\n   print(\"Model Complexity:\", model_complexity)\n   print(\"Gradio URL Endpoint:\", url_endpoint)\n```\n### <span id=\"page-44-2\"></span>F. Detailed Experimental Results\n\nThis section presents the detailed experimental results. Tables [6,](#page-44-0) [7,](#page-45-0) and [8](#page-45-1) show the average scores with standard deviations, computed from five independent runs. Tables [9](#page-45-2) and [10](#page-46-0) report the results of the ablation and hyperparameter studies, respectively. [Table 11](#page-46-2) reports the average time and money used to generate the final code in a single run. Finally, [Table 12](#page-46-1) and [Table 13](#page-47-0) present the search time and performance comparisons between the recent SELA [\\(Chi et al.,](#page-9-3) [2024\\)](#page-9-3), Agent K [\\(Grosnit et al.,](#page-9-6) [2024\\)](#page-9-6), and our *AutoML-Agent*, respectively.\n\nTable 6. Performance comparison with the SR metric. Best results are highlighted in bold.\n\n<span id=\"page-44-0\"></span>\n\n|              | Image Classification |          | Text Classification |          | Tabular Classification |          | Tabular Regression | Tabular Clustering |          | Time-Series Forecasting |          | Node Classification |          |          |          |\n|--------------|----------------------|----------|---------------------|----------|------------------------|----------|--------------------|--------------------|----------|-------------------------|----------|---------------------|----------|----------|----------|\n| Method       | Butterfly            | Shopee   | Ecomm               | Entail   | Banana                 | Software | Crab               | Crop               | Smoker   | Student                 | Weather  | Electricity         | Cora     | Citeseer | Avg.     |\n|              |                      |          |                     |          |                        |          | Constraint-Free    |                    |          |                         |          |                     |          |          |          |\n|              | 0.000                | 0.000    | 0.000               | 0.000    | 0.300                  | 0.100    | 0.000              | 0.000              | 0.400    | 0.000                   | 0.000    | 0.000               | 0.000    | 0.000    | 0.057    |\n| GPT-3.5      | (±0.000)             | (±0.000) | (±0.000)            | (±0.000) | (±0.274)               | (±0.224) | (±0.000)           | (±0.000)           | (±0.224) | (±0.000)                | (±0.000) | (±0.000)            | (±0.000) | (±0.000) | (±0.052) |\n|              | 0.200                | 0.600    | 0.000               | 0.400    | 0.400                  | 0.400    | 0.400              | 0.600              | 0.600    | 0.000                   | 0.000    | 0.000               | 0.400    | 0.400    | 0.314    |\n| GPT-4        | (±0.447)             | (±0.548) | (±0.000)            | (±0.548) | (±0.548)               | (±0.418) | (±0.548)           | (±0.548)           | (±0.418) | (±0.000)                | (±0.000) | (±0.000)            | (±0.548) | (±0.548) | (±0.366) |\n|              | 0.400                | 0.800    | 0.000               | 0.700    | 0.800                  | 0.800    | 0.000              | 0.800              | 0.900    | 0.000                   | 0.000    | 0.000               | 0.600    | 0.600    | 0.457    |\n| DS-Agent     | (±0.548)             | (±0.447) | (±0.000)            | (±0.447) | (±0.447)               | (±0.274) | (±0.000)           | (±0.447)           | (±0.224) | (±0.000)                | (±0.000) | (±0.000)            | (±0.548) | (±0.548) | (±0.281) |\n|              | 1.000                | 1.000    | 1.000               | 1.000    | 1.000                  | 1.000    | 1.000              | 1.000              | 1.000    | 1.000                   | 1.000    | 1.000               | 1.000    | 1.000    | 1.000    |\n| AutoML-Agent | (±0.000)             | (±0.000) | (±0.000)            | (±0.000) | (±0.000)               | (±0.000) | (±0.000)           | (±0.000)           | (±0.000) | (±0.000)                | (±0.000) | (±0.000)            | (±0.000) | (±0.000) | (±0.000) |\n|              |                      |          |                     |          |                        |          | Constraint-Aware   |                    |          |                         |          |                     |          |          |          |\n|              | 0.000                | 0.050    | 0.000               | 0.000    | 0.050                  | 0.150    | 0.100              | 0.050              | 0.150    | 0.000                   | 0.000    | 0.000               | 0.000    | 0.000    | 0.039    |\n| GPT-3.5      | (±0.000)             | (±0.112) | (±0.000)            | (±0.000) | (±0.112)               | (±0.137) | (±0.137)           | (±0.112)           | (±0.137) | (±0.000)                | (±0.000) | (±0.000)            | (±0.000) | (±0.000) | (±0.053) |\n|              | 0.150                | 0.350    | 0.200               | 0.200    | 0.150                  | 0.000    | 0.650              | 0.100              | 0.400    | 0.000                   | 0.000    | 0.000               | 0.400    | 0.500    | 0.221    |\n| GPT-4        | (±0.335)             | (±0.487) | (±0.447)            | (±0.447) | (±0.335)               | (±0.000) | (±0.418)           | (±0.224)           | (±0.335) | (±0.000)                | (±0.000) | (±0.000)            | (±0.335) | (±0.354) | (±0.266) |\n|              | 0.300                | 0.350    | 0.000               | 0.200    | 0.600                  | 0.650    | 0.200              | 0.200              | 0.450    | 0.000                   | 0.000    | 0.150               | 0.200    | 0.450    | 0.268    |\n| DS-Agent     | (±0.411)             | (±0.487) | (±0.000)            | (±0.326) | (±0.335)               | (±0.487) | (±0.447)           | (±0.447)           | (±0.274) | (±0.000)                | (±0.000) | (±0.335)            | (±0.326) | (±0.411) | (±0.306) |\n|              | 0.800                | 1.000    | 1.000               | 1.000    | 0.750                  | 1.000    | 1.000              | 0.750              | 0.750    | 0.750                   | 0.900    | 1.000               | 0.750    | 0.750    | 0.871    |\n| AutoML-Agent | (±0.112)             | (±0.000) | (±0.000)            | (±0.000) | (±0.000)               | (±0.000) | (±0.000)           | (±0.000)           | (±0.000) | (±0.000)                | (±0.224) | (±0.000)            | (±0.000) | (±0.000) | (±0.024) |\n\n<span id=\"page-45-0\"></span>\n\n|              |           | Image Classification |          | Text Classification |          | Tabular Classification |                  | Tabular Regression |          | Tabular Clustering |          | Time-Series Forecasting | Node Classification |          |          |\n|--------------|-----------|----------------------|----------|---------------------|----------|------------------------|------------------|--------------------|----------|--------------------|----------|-------------------------|---------------------|----------|----------|\n| Method       | Butterfly | Shopee               | Ecomm    | Entail              | Banana   | Software               | Crab             | Crop               | Smoker   | Student            | Weather  | Electricity             | Cora                | Citeseer | Avg.     |\n|              |           |                      |          |                     |          |                        | Constraint-Free  |                    |          |                    |          |                         |                     |          |          |\n| Human Models | 0.931     | 0.921                | 0.935    | 0.664               | 0.976    | 0.669                  | 0.328            | 0.476              | 0.513    | 0.750              | 0.970    | 0.916                   | 0.811               | 0.702    | 0.754    |\n|              | (±0.002)  | (±0.012)             | (±0.000) | (±0.039)            | (±0.000) | (±0.000)               | (±0.000)         | (±0.000)           | (±0.000) | (±0.000)           | (±0.000) | (±0.005)                | (±0.005)            | (±0.006) | (±0.005) |\n| AutoGluon    | 0.014     | 0.988                | 0.987    | 0.807               | 0.980    | 0.524                  | 0.875            | 0.479              | N/A      | N/A                | 0.992    | 0.908                   | N/A                 | N/A      | 0.755    |\n|              | (±0.000)  | (±0.000)             | (±0.000) | (±0.000)            | (±0.000) | (±0.000)               | (±0.000)         | (±0.000)           |          |                    | (±0.000) | (±0.002)                |                     |          | (±0.000) |\n| GPT-3.5      | 0.000     | 0.000                | 0.000    | 0.000               | 0.587    | 0.094                  | 0.000            | 0.000              | 0.447    | 0.000              | 0.000    | 0.000                   | 0.000               | 0.000    | 0.081    |\n|              | (±0.000)  | (±0.000)             | (±0.000) | (±0.000)            | (±0.535) | (±0.209)               | (±0.000)         | (±0.000)           | (±0.251) | (±0.000)           | (±0.000) | (±0.000)                | (±0.000)            | (±0.000) | (±0.071) |\n| GPT-4        | 0.169     | 0.545                | 0.000    | 0.196               | 0.390    | 0.285                  | 0.328            | 0.270              | 0.471    | 0.000              | 0.000    | 0.000                   | 0.186               | 0.199    | 0.217    |\n|              | (±0.379)  | (±0.499)             | (±0.000) | (±0.295)            | (±0.534) | (±0.261)               | (±0.450)         | (±0.247)           | (±0.264) | (±0.000)           | (±0.000) | (±0.000)                | (±0.343)            | (±0.328) | (±0.257) |\n| DS-Agent     | 0.305     | 0.735                | 0.000    | 0.500               | 0.766    | 0.523                  | 0.000            | 0.431              | 0.504    | 0.000              | 0.000    | 0.000                   | 0.474               | 0.393    | 0.331    |\n|              | (±0.419)  | (±0.411)             | (±0.000) | (±0.380)            | (±0.428) | (±0.131)               | (±0.000)         | (±0.324)           | (±0.001) | (±0.000)           | (±0.000) | (±0.000)                | (±0.433)            | (±0.360) | (±0.206) |\n| AutoML-Agent | 0.924     | 0.945                | 0.971    | 0.803               | 0.987    | 0.664                  | 0.859            | 0.465              | 0.521    | 0.760              | 0.995    | 0.937                   | 0.831               | 0.592    | 0.804    |\n|              | (±0.020)  | (±0.043)             | (±0.007) | (±0.006)            | (±0.019) | (±0.174)               | (±0.003)         | (±0.020)           | (±0.038) | (±0.021)           | (±0.003) | (±0.093)                | (±0.020)            | (±0.015) | (±0.035) |\n|              |           |                      |          |                     |          |                        | Constraint-Aware |                    |          |                    |          |                         |                     |          |          |\n|              | 0.000     | 0.173                | 0.000    | 0.000               | 0.196    | 0.475                  | 0.356            | 0.081              | 0.338    | 0.000              | 0.000    | 0.000                   | 0.000               | 0.000    | 0.116    |\n| GPT-3.5      | (±0.000)  | (±0.386)             | (±0.000) | (±0.000)            | (±0.439) | (±0.476)               | (±0.488)         | (±0.181)           | (±0.309) | (±0.000)           | (±0.000) | (±0.000)                | (±0.000)            | (±0.000) | (±0.163) |\n|              | 0.157     | 0.335                | 0.197    | 0.064               | 0.153    | 0.000                  | 0.719            | 0.091              | 0.463    | 0.000              | 0.000    | 0.000                   | 0.637               | 0.564    | 0.241    |\n| GPT-4        | (±0.350)  | (±0.463)             | (±0.440) | (±0.144)            | (±0.342) | (±0.000)               | (±0.405)         | (±0.204)           | (±0.260) | (±0.000)           | (±0.000) | (±0.000)                | (±0.356)            | (±0.318) | (±0.234) |\n|              | 0.330     | 0.353                | 0.000    | 0.205               | 0.776    | 0.383                  | 0.173            | 0.183              | 0.505    | 0.000              | 0.000    | 0.093                   | 0.319               | 0.403    | 0.266    |\n| DS-Agent     | (±0.451)  | (±0.485)             | (±0.000) | (±0.301)            | (±0.434) | (±0.214)               | (±0.386)         | (±0.409)           | (±0.001) | (±0.000)           | (±0.000) | (±0.209)                | (±0.437)            | (±0.369) | (±0.264) |\n|              | 0.926     | 0.972                | 0.982    | 0.796               | 0.967    | 0.573                  | 0.861            | 0.473              | 0.582    | 0.769              | 0.982    | 0.978                   | 0.843               | 0.632    | 0.810    |\n| AutoML-Agent | (±0.015)  | (±0.022)             | (±0.002) | (±0.027)            | (±0.002) | (±0.142)               | (±0.002)         | (±0.020)           | (±0.042) | (±0.010)           | (±0.028) | (±0.001)                | (±0.034)            | (±0.037) | (±0.027) |\n\nTable 7. Performance comparison with the NPS metric. Best results are highlighted in bold.\n\nTable 8. Performance comparison with the CS metric. Best results are highlighted in bold.\n\n<span id=\"page-45-1\"></span>\n\n|              | Image Classification |          | Text Classification |          | Tabular Classification |          |                  | Tabular Regression | Tabular Clustering |          | Time-Series Forecasting |             | Node Classification |          |          |\n|--------------|----------------------|----------|---------------------|----------|------------------------|----------|------------------|--------------------|--------------------|----------|-------------------------|-------------|---------------------|----------|----------|\n| Method       | Butterfly            | Shopee   | Ecomm               | Entail   | Banana                 | Software | Crab             | Crop               | Smoker             | Student  | Weather                 | Electricity | Cora                | Citeseer | Avg.     |\n|              |                      |          |                     |          |                        |          | Constraint-Free  |                    |                    |          |                         |             |                     |          |          |\n|              | 0.000                | 0.000    | 0.000               | 0.000    | 0.443                  | 0.097    | 0.000            | 0.000              | 0.424              | 0.000    | 0.000                   | 0.000       | 0.000               | 0.000    | 0.069    |\n| GPT-3.5      | (±0.000)             | (±0.000) | (±0.000)            | (±0.000) | (±0.405)               | (±0.216) | (±0.000)         | (±0.000)           | (±0.237)           | (±0.000) | (±0.000)                | (±0.000)    | (±0.000)            | (±0.000) | (±0.061) |\n|              | 0.185                | 0.573    | 0.000               | 0.298    | 0.395                  | 0.343    | 0.364            | 0.435              | 0.536              | 0.000    | 0.000                   | 0.000       | 0.293               | 0.299    | 0.266    |\n| GPT-4        | (±0.413)             | (±0.523) | (±0.000)            | (±0.413) | (±0.541)               | (±0.329) | (±0.499)         | (±0.397)           | (±0.325)           | (±0.000) | (±0.000)                | (±0.000)    | (±0.417)            | (±0.420) | (±0.305) |\n|              | 0.352                | 0.768    | 0.000               | 0.600    | 0.783                  | 0.661    | 0.000            | 0.616              | 0.702              | 0.000    | 0.000                   | 0.000       | 0.537               | 0.496    | 0.394    |\n| DS-Agent     | (±0.483)             | (±0.429) | (±0.000)            | (±0.353) | (±0.438)               | (±0.172) | (±0.000)         | (±0.361)           | (±0.111)           | (±0.000) | (±0.000)                | (±0.000)    | (±0.490)            | (±0.453) | (±0.235) |\n|              | 0.962                | 0.973    | 0.985               | 0.901    | 0.993                  | 0.832    | 0.929            | 0.732              | 0.761              | 0.880    | 0.998                   | 0.969       | 0.915               | 0.796    | 0.902    |\n| AutoML-Agent | (±0.010)             | (±0.021) | (±0.004)            | (±0.003) | (±0.010)               | (±0.087) | (±0.001)         | (±0.010)           | (±0.019)           | (±0.010) | (±0.002)                | (±0.047)    | (±0.010)            | (±0.007) | (±0.017) |\n|              |                      |          |                     |          |                        |          | Constraint-Aware |                    |                    |          |                         |             |                     |          |          |\n|              | 0.000                | 0.111    | 0.000               | 0.000    | 0.123                  | 0.312    | 0.228            | 0.066              | 0.244              | 0.000    | 0.000                   | 0.000       | 0.000               | 0.000    | 0.077    |\n| GPT-3.5      | (±0.000)             | (±0.249) | (±0.000)            | (±0.000) | (±0.276)               | (±0.302) | (±0.312)         | (±0.147)           | (±0.223)           | (±0.000) | (±0.000)                | (±0.000)    | (±0.000)            | (±0.000) | (±0.108) |\n|              | 0.153                | 0.343    | 0.198               | 0.132    | 0.151                  | 0.000    | 0.685            | 0.096              | 0.432              | 0.000    | 0.000                   | 0.000       | 0.518               | 0.532    | 0.231    |\n| GPT-4        | (±0.343)             | (±0.475) | (±0.444)            | (±0.296) | (±0.339)               | (±0.000) | (±0.394)         | (±0.214)           | (±0.270)           | (±0.000) | (±0.000)                | (±0.000)    | (±0.317)            | (±0.319) | (±0.244) |\n|              | 0.315                | 0.351    | 0.000               | 0.203    | 0.688                  | 0.516    | 0.186            | 0.191              | 0.477              | 0.000    | 0.000                   | 0.122       | 0.260               | 0.427    | 0.267    |\n| DS-Agent     | (±0.431)             | (±0.485) | (±0.000)            | (±0.312) | (±0.385)               | (±0.332) | (±0.417)         | (±0.428)           | (±0.137)           | (±0.000) | (±0.000)                | (±0.272)    | (±0.367)            | (±0.390) | (±0.283) |\n|              | 0.863                | 0.986    | 0.991               | 0.898    | 0.858                  | 0.786    | 0.930            | 0.611              | 0.666              | 0.760    | 0.941                   | 0.989       | 0.796               | 0.691    | 0.841    |\n| AutoML-Agent | (±0.063)             | (±0.011) | (±0.001)            | (±0.013) | (±0.001)               | (±0.071) | (±0.001)         | (±0.010)           | (±0.021)           | (±0.005) | (±0.126)                | (±0.001)    | (±0.017)            | (±0.018) | (±0.026) |\n\nTable 9. Results of ablation study on different variations.\n\n<span id=\"page-45-2\"></span>\n\n| RAP | Plan<br>Decomposition        | Multi-Step<br>Verification | Image<br>Classification | Text<br>Classification | Tabular<br>Classification | Time-Series<br>Forecasting | Node<br>Classification | Average |\n|-----|------------------------------|----------------------------|-------------------------|------------------------|---------------------------|----------------------------|------------------------|---------|\n|     | Success Rate                 |                            |                         |                        |                           |                            |                        |         |\n| ✓   |                              |                            | 1.000                   | 0.000                  | 0.000                     | 0.000                      | 1.000                  | 0.400   |\n| ✓   | ✓                            |                            | 1.000                   | 1.000                  | 1.000                     | 0.000                      | 1.000                  | 0.800   |\n| ✓   | ✓                            | ✓                          | 1.000                   | 1.000                  | 1.000                     | 1.000                      | 1.000                  | 1.000   |\n|     | Normalized Performance Score |                            |                         |                        |                           |                            |                        |         |\n| ✓   |                              |                            | 0.929                   | 0.000                  | 0.000                     | 0.000                      | 0.734                  | 0.333   |\n| ✓   | ✓                            |                            | 0.928                   | 0.982                  | 0.975                     | 0.000                      | 0.748                  | 0.727   |\n| ✓   | ✓                            | ✓                          | 0.936                   | 0.971                  | 1.000                     | 0.991                      | 0.812                  | 0.942   |\n|     | Comprehensive Score          |                            |                         |                        |                           |                            |                        |         |\n| ✓   |                              |                            | 0.965                   | 0.000                  | 0.000                     | 0.000                      | 0.867                  | 0.366   |\n| ✓   | ✓                            |                            | 0.964                   | 0.991                  | 0.988                     | 0.000                      | 0.874                  | 0.763   |\n| ✓   | ✓                            | ✓                          | 0.968                   | 0.986                  | 1.000                     | 0.996                      | 0.906                  | 0.971   |\n\n<span id=\"page-46-0\"></span>\n\n| Number of Plans              | Image<br>Classification | Text<br>Classification | Tabular<br>Classification | Time-Series<br>Forecasting | Node<br>Classification | Average |\n|------------------------------|-------------------------|------------------------|---------------------------|----------------------------|------------------------|---------|\n| Success Rate                 |                         |                        |                           |                            |                        |         |\n| 1                            | 1.000                   | 1.000                  | 1.000                     | 1.000                      | 1.000                  | 1.000   |\n| 3                            | 1.000                   | 1.000                  | 1.000                     | 1.000                      | 1.000                  | 1.000   |\n| 5                            | 1.000                   | 1.000                  | 1.000                     | 1.000                      | 1.000                  | 1.000   |\n| Normalized Performance Score |                         |                        |                           |                            |                        |         |\n| 1                            | 0.831                   | 0.966                  | 0.958                     | 0.998                      | 0.800                  | 0.911   |\n| 3                            | 0.936                   | 0.971                  | 1.000                     | 0.999                      | 0.812                  | 0.944   |\n| 5                            | 0.916                   | 0.964                  | 0.973                     | 0.998                      | 0.805                  | 0.931   |\n| Comprehensive Score          |                         |                        |                           |                            |                        |         |\n| 1                            | 0.916                   | 0.983                  | 0.979                     | 0.999                      | 0.900                  | 0.955   |\n| 3                            | 0.968                   | 0.986                  | 1.000                     | 0.999                      | 0.906                  | 0.972   |\n| 5                            | 0.958                   | 0.982                  | 0.986                     | 0.999                      | 0.903                  | 0.966   |\n\nTable 10. Comparison between the different numbers of plans.\n\n<span id=\"page-46-2\"></span>Table 11. Time and monetary costs averaged across different tasks and datasets for a single run under the constraint-free and constraintaware settings.\n\n| Cost                   | Prompt Parsing | Request Verification | Retrieval & Planning | Plan Execution  | Execution Verification | Selection and Summarization | Code Generation | Total          |  |  |\n|------------------------|----------------|----------------------|----------------------|-----------------|------------------------|-----------------------------|-----------------|----------------|--|--|\n|                        |                |                      |                      | Constraint-Free |                        |                             |                 |                |  |  |\n| Money (\\$)             | N/A            | 0.00                 | 0.02                 | 0.14            | 0.00                   | 0.06                        | 0.04            | 0.27           |  |  |\n| Time (s)               | 10.78          | 1.91                 | 187.71               | 136.34          | 1.04                   | 17.88                       | 182.60          | 538.25         |  |  |\n| Constraint-Aware       |                |                      |                      |                 |                        |                             |                 |                |  |  |\n| Money (\\$)<br>Time (s) | N/A<br>14.21   | 0.00<br>3.63         | 0.00<br>182.38       | 0.11<br>98.62   | 0.00<br>1.37           | 0.15<br>20.25               | 0.06<br>191.90  | 0.32<br>512.35 |  |  |\n\n<span id=\"page-46-1\"></span>\n\n| Table 12. Comparison of search time and normalized performance score between MCTS-based SELA and our AutoML-Agent. |  |\n|--------------------------------------------------------------------------------------------------------------------|--|\n\n| Dataset                    |             | ↓<br>Search Time (seconds) | ↑<br>Normalized Performance Score |                        |  |  |\n|----------------------------|-------------|----------------------------|-----------------------------------|------------------------|--|--|\n|                            | SELA (MCTS) | AutoML-Agent<br>(Ours)     | SELA (MCTS)                       | AutoML-Agent<br>(Ours) |  |  |\n| Binary Classification      |             |                            |                                   |                        |  |  |\n| smoker-status              | 2736.78     | 206.48                     | 0.785                             | 0.762                  |  |  |\n| click-prediction-small     | 2227.85     | 256.24                     | 0.238                             | 0.352                  |  |  |\n| Multi-Class Classification |             |                            |                                   |                        |  |  |\n| mfeat-factors              | 1304.40     | 219.45                     | 0.957                             | 0.940                  |  |  |\n| wine-quality-white         | 2372.77     | 206.41                     | 0.650                             | 0.652                  |  |  |\n| Regression                 |             |                            |                                   |                        |  |  |\n| colleges                   | 674.29      | 232.03                     | 0.876                             | 0.878                  |  |  |\n| house-prices               | 2906.95     | 378.85                     | 0.090                             | 0.090                  |  |  |\n| Average                    | 2037.18     | 249.91                     | 0.599                             | 0.612                  |  |  |\n\n<span id=\"page-47-0\"></span>Table 13. Comparison of leaderboard quantile and task-specific performance between Agent K and our *AutoML-Agent*. (↓ indicates lower task-specific performance is better)\n\n| Competition ID                                | Learderboard Quantile      |              | Task-Specific Performance     |              |  |  |\n|-----------------------------------------------|----------------------------|--------------|-------------------------------|--------------|--|--|\n|                                               | Agent K<br>(From Figure 8) | AutoML-Agent | Agent K<br>(Derive from Rank) | AutoML-Agent |  |  |\n| No Medal                                      |                            |              |                               |              |  |  |\n| restaurant-revenue-prediction (↓)             | 8∼9                        | 57           | 2279272.777∼2280826.272       | 1859766.392  |  |  |\n| playground-series-s3e14 (↓)                   | 88∼89                      | 91           | 331.167∼331.173               | 330.141      |  |  |\n| Bronze                                        |                            |              |                               |              |  |  |\n| nlp1000-ml-challenge                          | 75∼76                      | 29           | 0.993∼0.994                   | 0.720        |  |  |\n| dogs-vs-cats-redux-kernels-edition (↓)        | 93∼94                      | 82           | 0.054∼0.055                   | 0.079        |  |  |\n| Silver                                        |                            |              |                               |              |  |  |\n| nlpsci                                        | 89∼90                      | 89           | 0.809∼0.810                   | 0.808        |  |  |\n| home-data-for-ml-course (↓)                   | 98∼99                      | 91           | 13187.602∼13193.958           | 14869.145    |  |  |\n| Gold                                          |                            |              |                               |              |  |  |\n| world-championship-2023-embryo-classification | 90∼91                      | 100          | 0.571∼0.571                   | 0.609        |  |  |\n| sign-language-image-classification            | 99∼100                     | 88           | 0.977∼0.978                   | 0.971        |  |  |，分析其研究动机、核心方法与公式推导细节。请结合摘要与正文信息，提取论文背景、问题定义、方法核心流程与理论基础。\n",
        "agent": "论文解读专家\n",
        "status": "started"
    }
]
