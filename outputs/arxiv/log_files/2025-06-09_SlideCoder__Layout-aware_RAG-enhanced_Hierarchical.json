[
    {
        "timestamp": "2025-06-10 16:12:01",
        "task_name": "research_task",
        "task": "阅读论文《SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design》，论文内容如下：![](_page_0_Picture_0.jpeg)\n\nWenxin Tang 1 \\*, Jingyu Xiao 2 \\*, Wenxuan Jiang 3 , Xi Xiao 1 , Yuhang Wang 4 , Xuxin Tang 5 , Qing Li 6 , Yuehe Ma 7 , Junliang Liu 8 , Shisong Tang 5 , Michael R. Lyu 2 <sup>1</sup>Tsinghua University, <sup>2</sup>The Chinese University of Hong Kong, <sup>3</sup>Northeastern University <sup>4</sup>Southwest University, <sup>5</sup>Kuaishou Technology, <sup>6</sup>Peng Cheng Laboratory <sup>7</sup>BNU-HKBU United International College, <sup>8</sup>Dalian Maritime University twx24@mails.tsinghua.edu.cn, jyxiao@link.cuhk.edu.hk, xiaox@sz.tsinghua.edu.cn\n\n## Abstract\n\nManual slide creation is labor-intensive and requires expert prior knowledge. Existing natural language-based LLM generation methods struggle to capture the visual and structural nuances of slide designs. To address this, we formalize the Reference Image to Slide Generation task and propose Slide2Code, the first benchmark with difficulty-tiered samples based on a novel Slide Complexity Metric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework for generating editable slides from reference images. SlideCoder integrates a Color Gradient-based Segmentation algorithm and a Hierarchical Retrieval-Augmented Generation method to decompose complex tasks and enhance code generation. We also release SlideMaster, a 7B open-source model fine-tuned with improved reverse-engineered data. Experiments show that SlideCoder outperforms state-of-the-art baselines by up to 40.5 points, demonstrating strong performance across layout fidelity, execution accuracy, and visual consistency. Our code is available at [https://github.com/](https://github.com/vinsontang1/SlideCoder) [vinsontang1/SlideCoder](https://github.com/vinsontang1/SlideCoder) .\n\n## 1 Introduction\n\nSlide creation is essential in academic and professional communication for visually conveying complex ideas. However, manual design is laborintensive and time-consuming [\\(Al Masum et al.,](#page-8-0) [2005\\)](#page-8-0). While templates offer some relief, they enforce fixed layouts and styles, limiting flexibility.\n\nRecent progress in Large Language Models (LLMs) [\\(Nam et al.](#page-9-0) , [2024](#page-9-0) ; [Ge et al.](#page-8-1) , [2023\\)](#page-8-1) has sparked interest in automatic slide creation. AutoPresent [\\(Ge et al.,](#page-8-2) [2025\\)](#page-8-2), an early study on the Natural Language (NL) to slide generation task, fine-tunes a LLAMA-based model [\\(Grattafiori](#page-8-3) [et al.](#page-8-3) , [2024\\)](#page-8-3) on the diversified SLIDESBENCH\n\n<span id=\"page-0-0\"></span>![](_page_0_Figure_8.jpeg)\n\nFigure 1: Illustration of slide generation scenarios from design and mistakes made by MLLMs.\n\ndataset. It translates NL instructions into Python code, which invokes SLIDESLIB, a high-level API built on python-pptx [\\(Canny](#page-8-4) , [2023\\)](#page-8-4), to construct each slide. This pipeline reduces manual effort and streamlines design workflows.\n\nDespite Autopresent's capability to generate slides from natural language input, several significant challenges remain unaddressed.\n\nFirst, natural language inherently lacks an accurate description of slide visual design (e.g., color, layout, and style) and users sometimes directly input the design image for slide generation. For example, as shown in Figure [1,](#page-0-0) a user sees a nice design from non-editable slides (png and pdf format) or other source like webpage design, and hopes to convert it into an editable slide (pptx format). Or the user lacks the skills to make slides, they can generate the slide by input their design image. In these scenarios, the Multimodal Large Language Models (MLLMs) are needed to understand the design and generate slides.\n\nSecond, MLLMs face limitations when handling complex slides, particularly those incorporating diverse element types and high element density. As illustrated in Figure [1,](#page-0-0) these discrepancies can be divided into three categories: *miss*, which stands for the complete omission of certain\n\n<sup>\\*</sup>These authors contributed equally.\n\nvisual or textual elements (e.g., the top left corner of the shape is missing); *incorrect*, referring to deviations in visual styles or attributes from those specified or expected in the reference slides (e.g., title is not bold); and *disorder*, which describes significant differences in spatial arrangements and alignment of elements compared to the original layout (e.g., the three subheadings are not properly positioned and aligned.).\n\nThird, MLLMs' insufficient comprehension of the python-pptx library leads to the generation of syntactically invalid or non-executable code. Autopresent [\\(Ge et al.,](#page-8-2) [2025\\)](#page-8-2) attempts to address this issue by constructing SLIDESLIB, a simplified library built upon python-pptx, encapsulating commonly used operations into a set of high-level APIs. However, this operation inherently restricts the flexibility and comprehensiveness of slide generation. Specifically, SLIDESLIB currently supports only five basic operation types, which neglects more intricate layouts and design requirements commonly encountered in realistic scenarios. Consequently, presentations produced by this approach tend to be overly simplistic, inadequately capturing complex human intentions and detailed visual expectations.\n\nTo address the aforementioned limitations, we introduce SlideCoder, a layout-aware RAG-enhanced hierarchical slide generation framework, which can understand the complex slides and python-pptx library accurately. First, we formulate a novel task, Reference Image (RI) to slide generation, i.e., automatically generating the code for replicating the slide, which is visually consistent with RI. To evaluate the performance of SlideCoder under complex slide scenarios, we propose a novel Slide Complexity Metric (SCM), and construct a new benchmark Slide2Code with different difficulty levels based on SCM. Second, we develop a novel Color Gradients-based Segmentation algorithm (CGSeg) that effectively decomposes slide images into semantically meaningful regions. Besides, we propose the Layout-aware Prompt, which integrates the position information of elements to enhance MLLM's understanding of slide layout. Third, we propose a novel Hierarchical Retrieval-Augmented Generation (H-RAG)-based Code Generation method, which employs a dual-level retrieval-augmented knowledge base [\\(Cuconasu](#page-8-5) [et al.,](#page-8-5) [2024;](#page-8-5) [Fan et al.,](#page-8-6) [2024\\)](#page-8-6) to explicitly enhance MLLMs' understanding of the python-pptx library. At the higher level, a Shape Type Knowledge Base\n\n(TS-KB) systematically classifies slide elements and standardizes their descriptions using pythonpptx API terminologies. At the lower level, a Operation Function Knowledge Base (OF-KB) captures precise syntactic patterns and invocation paradigms of python-pptx library functions.\n\nTo further enhance the MLLM's ability to generate high-quality slides, we build a PPTX reverseengineering tool to construct high quality training data for fine-tuning a 7B model SlideMaster based on Qwen-VL-7B [\\(Bai et al.,](#page-8-7) [2025\\)](#page-8-7), which can approaches the performance of the closed-sourced model GPT-4o [\\(Achiam et al.,](#page-8-8) [2023\\)](#page-8-8). Our contributions are summarized as follows:\n\n- We define reference image (RI) to slide generation task and propose a novel Slide Complexity Metric (SCM), based on which we construct Slide2Code, the first difficulty-leveled benchmark with 300 samples.\n- We propose SlideCoder, which consists of a novel Color Gradients-based Segmentation algorithm (CGSeg), a Layout-aware Prompt and a Hierarchical Retrieval-Augmented Generation (H-RAG)-based Code Generation method for enhancing the MLLM's understanding on the complex slides and python-pptx library.\n- We train SlideMaster, a 7B open-source model approaching the performance of GPT-4o. To enable effective fine-tuning, we also build a comprehensive PPTX reverse-engineering tool for precise code generation.\n\n## 2 Related Work\n\n# 2.1 Multimodal Large Language Models for Code Generation\n\nThe multimodal large model demonstrates excellent capabilities in visually rich code generation scenarios, such as UI code generation [\\(Xiao et al.,](#page-9-1) [2024,](#page-9-1) [2025;](#page-9-2) [Yun et al.,](#page-9-3) [2024;](#page-9-3) [Wan et al.,](#page-9-4) [2024\\)](#page-9-4), SVG code generation [\\(Rodriguez et al.,](#page-9-5) [2025;](#page-9-5) [Nishina and Matsui,](#page-9-6) [2024;](#page-9-6) [Wu et al.,](#page-9-7) [2024;](#page-9-7) [Xing](#page-9-8) [et al.,](#page-9-8) [2024\\)](#page-9-8), and visually rich programming questions [\\(Li et al.,](#page-9-9) [2024;](#page-9-9) [Zhang et al.,](#page-9-10) [2024a;](#page-9-10) [Ma et al.,](#page-9-11) [2025\\)](#page-9-11). However, MLLMs are not yet capable of plug-and-play use across tasks and still produce subtle errors, therefore, some studies explore their code repair abilities [\\(Yang et al.,](#page-9-12) [2024;](#page-9-12) [Yuan et al.,](#page-9-13) [2024;](#page-9-13) [Zhang et al.,](#page-9-14) [2024b\\)](#page-9-14).\n\n#### 2.2 Slide Generation and Understanding\n\nPrevious work on slide generation has predominantly focused on basic content extraction from input documents. With the recent advancements in large language models [\\(Fu et al.,](#page-8-9) [2022;](#page-8-9) [Hu and](#page-8-10) [Wan,](#page-8-10) [2014;](#page-8-10) [Kan,](#page-8-11) [2007;](#page-8-11) [Sefid and Wu,](#page-9-15) [2019\\)](#page-9-15), several studies have begun to explore LLM-based slide generation. For example, [\\(Zheng et al.,](#page-9-16) [2025\\)](#page-9-16) utilizes LLMs to generate slides based on pre-defined slide templates and user-provided text. [\\(Ge et al.,](#page-8-2) [2025\\)](#page-8-2) introduces the task of natural language (NL) to slide code generation, aiming to organize visual slide content through textual input. However, its use of coarse-grained natural language descriptions and a native agent design significantly limits the quality of the generated slides.\n\n## 3 Slide2Code Benchmark\n\nWe construct the Slide2Code benchmark to evaluate the performance of multimodal large language models (MLLMs) on the Reference Image (RI) to slide generation task. Each instance includes a reference slide image and its corresponding PPTX slide. Slide2Code enables comparison of MLLM backbones under varying complexity. [§3.1](#page-2-0) formally defines the task, [§3.2](#page-2-1) describes our unified complexity scoring system based on element quantity, diversity, and visual density, and [§3.3](#page-2-2) details data collection and sampling.\n\n#### <span id=\"page-2-0\"></span>3.1 Task Description\n\nThis work addresses the task of Reference Image (RI) to slide generation, where the input is a slide's reference image I<sup>0</sup> and the goal is to generate Python code using the python-pptx library. Let F<sup>0</sup> denote the original slide file corresponding to I0. Given a generation framework G and Multimodal Large Language Models (MLLMs) M, the generated code C<sup>g</sup> = GM(I0) can be executed to obtain a new slide file Fg, whose rendered image is denoted as Ig. As the original code C<sup>0</sup> for F<sup>0</sup> is unavailable, we assess the performance of G and M by comparing (I0, F0) and (Ig, Fg).\n\n#### <span id=\"page-2-1\"></span>3.2 Slide Complexity Metric\n\nTo evaluate slide complexity, we propose a Tri-Metric Slide Complexity Metric (SCM) that integrates production difficulty and visual complexity. Due to the mismatch between visual appearance and construction effort, for example, inserting a visually complex image may require minimal op-\n\nerations. To adress this, we assess slides using: (1) element count, (2) element type count (e.g., textbox, placeholder), and (3) Element Coverage Ratio. The first two reflect operational cost, the third captures visual richness. Since reference complexity labels are not available, we evaluate the relative complexity of sample i within a collection Y = {1, 2, 3, ..., N}.\n\nLet c<sup>i</sup> be the number of elements and e<sup>i</sup> the number of distinct element types in sample i. The Element Coverage Ratio v<sup>i</sup> is the proportion of activated color grids to total grids in the image of sample i, computed via the gradient-based segmentation algorithm CGSeg (see [§4.1](#page-3-0) for details).\n\nEach raw dimension score x<sup>i</sup> ∈ {c<sup>i</sup> , e<sup>i</sup> , vi} is normalized as x˜<sup>i</sup> = σ xi−µ √ σ2+ϵ , where µ and σ 2 denote the mean and variance over all samples in set Y , respectively. Here, σ(·) is the sigmoid function [\\(Han and Moraga,](#page-8-12) [1995\\)](#page-8-12), and ϵ is a small constant for numerical stability. The final complexity score for slide i is computed via a weighted aggregation: z<sup>i</sup> = α · c˜<sup>i</sup> + β · e˜<sup>i</sup> + γ · v˜<sup>i</sup> , where α+β+γ = 1 and the weights α, β, γ reflect the importance of production effort and visual complexity. This metric shows a strong correlation with human judgment, as detailed in Section [§5.4.](#page-6-0)\n\n#### <span id=\"page-2-2\"></span>3.3 Data Collection\n\nTo construct a comprehensive benchmark that captures diverse slide characteristics, we randomly sample approximately 32,000 Zenodo10k [\\(Zheng](#page-9-16) [et al.,](#page-9-16) [2025\\)](#page-9-16) slide instances, the largest publicly available slide dataset, to construct the slide set Y as described in [§3.2.](#page-2-1) To enhance diversity and allow comparative analysis, we additionally incorporate SLIDEBENCH samples in Y . This unified set is then used to calculate the normalized complexity scores z for all slides. KMeans algorithm is used to obtain three clusters, whose cluster centers are sorted in order of z to define the simple, medium, and complex levels. From each cluster, we randomly select 100 representative samples from Y to form the final Slide2Code benchmark.\n\nFigure [2](#page-3-1) shows that both Zenodo10k and SLIDEBENCH contain a significantly larger proportion of simple and medium slides. In contrast, Slide2Code exhibits a more balanced composition across all three levels, allowing a more equitable evaluation of slide generation models under varying structural and visual complexities.\n\n<span id=\"page-3-1\"></span>![](_page_3_Figure_0.jpeg)\n\nFigure 2: Proportion of samples across three levels in the Slide2Code, Zenodo10k, and SLIDEBENCH datasets.\n\n## 4 Methodology\n\nIn this section, we introduce SlideCoder, a unified end-to-end framework for generating Pythonexecutable slide code from reference images (RIs). We assume a scenario where a user provides a design layout (\"*Design* \") and embedded visual elements such as pictures or background images (\"*Pictures* \"). SlideCoder comprises three core modules. First, a Color Gradients-based Segmentation (CGSeg) algorithm segments the input *Design* into semantically meaningful regions. Second, a Hierarchical Retrieval-Augmented Code Generation module, consisting of three collaborative agents Describer, Coder, and Assembler, generates the slide code. Third, a Layout-aware Prompt mechanism enhances the Assembler agent to ensure spatial consistency and syntactic correctness. Finally, based on this framework, we fine-tune a 7B open-source model, named SlideMaster.\n\n#### <span id=\"page-3-0\"></span>4.1 Color Gradient-based Segmentation\n\nTo reduce the difficulty of MLLM in understanding complex slide design, we proposed CGSeg, a recursive color gradient-based segmentation algorithm to divide slide design into blocks. As shown in Algorithm [1,](#page-3-2) CGSeg starts by dividing the input image (Figure [4a\\)](#page-4-0) into a grid and computing the Sobel magnitude for each block to measure the intensity of the color gradient (lines 4–5). Blocks with gradient magnitudes significantly higher than the median are marked as activated block (lines 6–14), as visualized in Figure [4b.](#page-4-0) To group visually coherent regions, CGSeg applies a flood-fill [\\(Burtsev](#page-8-13) [and Kuzmin,](#page-8-13) [1993\\)](#page-8-13) operation to the binary activation mask (line 15), identifying connected regions corresponding to sub-images (line 16), as shown in Figure [4c.](#page-4-0) These sub-images are further segmented recursively to ensure a hierarchical decomposition of the image Im, along with the corresponding positional information p<sup>m</sup> (lines 1–3 and 17–23),\n\n<span id=\"page-3-2\"></span>Algorithm 1 Color Gradient-based Segmentation (CGSeg)\n\n|     |                                        | Require: Image I, Grid size g, Depth D, Max depth Dmax, |\n|-----|----------------------------------------|---------------------------------------------------------|\n|     | Threshold T                            |                                                         |\n|     | Ensure: List of segmented sub-images   |                                                         |\n|     | 1: if D = Dmax then                    |                                                         |\n| 2:  | return ∅                               |                                                         |\n|     | 3: end if                              |                                                         |\n|     | 4: G ← SPLIT(I, g)                     | // g × g grid blocks                                    |\n|     | 5: C ← GRADMAG(G)                      | // gradient magnitudes                                  |\n|     | 6: Cmid ← MEDIAN(C)                    |                                                         |\n|     | g×g<br>7: M ← 0                        | // binary mask                                          |\n|     | 8: for each cij in C do                |                                                         |\n| 9:  | if cij > T · Cmid then                 |                                                         |\n| 10: | Mij ← 1                                | // activate the block                                   |\n| 11: | else                                   |                                                         |\n| 12: | Mij ← 0                                |                                                         |\n| 13: | end if                                 |                                                         |\n|     | 14: end for                            |                                                         |\n|     | 15: M ← FILL(M)                        | // flood-fill                                           |\n|     | 16: Ms ← REGIONS(M)                    | // split connected regions                              |\n|     | 17: R ← ∅                              |                                                         |\n|     | 18: for each m in Ms do                |                                                         |\n| 19: | Im, pm ← CROP(I, m)                    | // get sub-image                                        |\n| 20: | add Im and pm to R                     |                                                         |\n| 21: | ′ ←<br>R<br>CGSEG(Im, g, D+1, Dmax, T) |                                                         |\n| 22: | ′<br>add all in R<br>to R              |                                                         |\n|     | 23: end for                            |                                                         |\n|     | 24: return R                           |                                                         |\n|     |                                        |                                                         |\n\nwith the final segmentation result shown in Figure [4d.](#page-4-0) This recursive structure allows CGSeg to adaptively refine segment granularity based on local visual complexity, which is crucial for handling slides with heterogeneous layout densities.\n\n# 4.2 Hierarchical Retrieval-Augmented Code Generation Module\n\n#### 4.2.1 Generation Process\n\nWe design three collaborative MLLM agents whose code generation processes are augmented by H-RAG. Describer is responsible for generating a global *Design* description (Overall Description) as well as block descriptions (Block Description) for each segmented blocks. Based on block and their associated block description, Coder produces corresponding code snippets. Subsequently, Assembler generates the complete slide code by layoutaware prompt, which will be elaborated in [§4.3,](#page-4-1) along with the *Pictures* provided. Executing this code produces a slide that structurally and visually aligns with the Reference Image(RI). If the generated code is not executable Assembler applies a self-refinement mechanism to correct syntax errors, where errors serves as the feedback to prompt the MLLM to re-generate the code.\n\nBeyond the above inputs, each agent draws knowledge from distinct bases according to its role.\n\n![](_page_4_Figure_0.jpeg)\n\n![](_page_4_Figure_1.jpeg)\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_2.jpeg)\n\nFigure 4: An example of CGSeg applied to a slide reference image. The algorithm begins by computing color gradients (a-b), fills them (c), and recursively segments sub-regions (d).\n\nThe form and origin of the knowledge used in each agent's prompt are detailed in [§4.2.2.](#page-4-2)\n\n# <span id=\"page-4-2\"></span>4.2.2 Hierarchical Retrieval-Augmented Generation\n\nHierarchical Retrieval-Augmented Generation(H-RAG) comprises a Shape Type Knowledge Base and an Operation Function Knowledge Base. The former contains descriptions of objects from the python-pptx documentation, used in Describer to guide standardized description generation. For example, in \"This *autoshape* includes a *textbox*...\", both terms are object names from the documentation. The latter includes full syntax specifications (e.g., parameters, return values, etc.). Appendix [F](#page-10-0) details their structure.\n\nWe employ BGE M3-Embedding [\\(Chen et al.,](#page-8-14) [2024\\)](#page-8-14) to embed entries and build a vector-based retrieval database. For a prompt p, its vector q<sup>p</sup> is computed, and cosine similarity cos(qp, ki) is used to match k<sup>i</sup> . The top-k relevant entries are inserted into p. Given the size of the Shape Type Knowledge Base, all entries are included in Describer to ensure complete type coverage.\n\nIn the hierarchical pipeline, agents collaborate progressively. Describer retrieves object types from the Shape Type Knowledge Base to identify elements in block images and output standardized descriptions. Coder uses these to query the Operation Function Knowledge Base and generate code\n\nsnippets. Assembler uses these snippets to retrieve full syntax patterns and generate executable code.\n\n# <span id=\"page-4-1\"></span>4.3 Layout-aware Prompt\n\nAfter Coder completes the generation of code snippets for blocks, Assembler is applied to assemble these code snippets for generating the final slide in an accurate manner. The assembly prompt needs to meet the following requirements: (1) ensure that each block appears in the correct position in the final slide; (2) avoid syntax errors in the merged code and ensure code context consistency.\n\nTo achieve above goals, layout-aware prompt injects the layout position using python-pptx standard positioning units (inches) to ensure the position correctness and retrieve the grammar <Grammar> from Knowledge Base to avoid syntax errors and code conflicts. Since the resolution of the *Design* differs from the actual slide layout size, we apply proportional scaling to the Position (<x, y, w, h>) extracted from Color Gradients-based Segmentation (CGSeg) algorithm to map it onto the slide coordinates, denoted as <Position\\*>. Subsequently, the reference image design <Design>, global body description <Overall description.>, partial codes <Code Snippets> from Coder, layout representation <Position\\*>, and syntactic patterns <Grammar> retrieved from the Hierarchical Retrieval-Augmented Generation(H-RAG) knowledge base are integrated into a predefined prompt template\n\nto construct the final layout-aware prompt (see Appendix [E](#page-10-1) for details).\n\n#### 4.4 SlideMaster\n\nUsing the SLIDESBENCH training set, we construct a dataset of (RI, instruction, program) triplets. The reverse-engineering tool proposed by [\\(Ge et al.,](#page-8-2) [2025\\)](#page-8-2) produces labels (Python code) for only a limited set of slide styles, resulting in suboptimal training data quality. To mitigate this, we develop a new reverse-engineering tool capable of handling a broader spectrum of slide styles, thereby enhancing label quality. The effectiveness of this tool is analyzed in [§5.3.](#page-5-0) We fine-tune our model, Slide-Master, based on Qwen2.5-VL-7B-Instruct [\\(Bai](#page-8-7) [et al.,](#page-8-7) [2025\\)](#page-8-7), using LoRA [\\(Hu et al.,](#page-8-15) [2022\\)](#page-8-15). Full configuration details are provided in Appendix [C.](#page-10-2)\n\n# 5 Experiments and Results\n\n#### 5.1 Experimental Setup\n\nModel. To evaluate the performance of the Slide-Coder, we employ state-of-the-art (SOTA) models, including GPT-4o [\\(Achiam et al.,](#page-8-8) [2023\\)](#page-8-8), Gemini-2.0-flash [\\(Google,](#page-8-16) [2025\\)](#page-8-16), and SlideMaster, which is a fine-tuned model based on the open-source Qwen2.5-VL-7B-Instruct [\\(Bai et al.,](#page-8-7) [2025\\)](#page-8-7). The SOTA models are accessed via their official APIs, with GPT-4o using version 20241120 and Gemini-2.0-flash accessed in May 2025. For both models, the maximum token limit and temperature are set to 4096 and 0, respectively. Same as [\\(Ge et al.,](#page-8-2) [2025\\)](#page-8-2), we allow both Coder and Assembler agents up to three self-refinement attempt. The first successful attempt is taken as the output. If Coder fails to generate executable code after the maximum number of attempts, the corresponding block is discarded. If Assembler fails, the corresponding sample is marked as execution failure.\n\nMetric. To comprehensively assess generation quality, we adopt four metrics, using the notations defined in [§3.1.](#page-2-0) (1) Global Visual Metrics, including CLIP [\\(Hessel et al.,](#page-8-17) [2021\\)](#page-8-17) and SSIM [\\(Nils](#page-9-17)[son and Akenine-Möller,](#page-9-17) [2020\\)](#page-9-17) scores computed between the original image I<sup>0</sup> and the generated image Ig; (2) Local Structural Metrics, which compare the original and generated slide files F<sup>0</sup> and F<sup>g</sup> in terms of content similarity and position similarity, following [\\(Ge et al.,](#page-8-2) [2025\\)](#page-8-2); (3) Execution, defined as the success rate of executing C<sup>g</sup> without errors; and (4) Overall Score, calculated as the average of all metric values across all samples, with failed executions assigned a score of zero.\n\n#### 5.2 Quantitative Results and Analysis\n\nThe upper part of Table [1](#page-6-1) presents the performance of different frameworks on our proposed benchmark, evaluated using the metrics introduced in Section [3.1.](#page-2-0) The results show that SlideCoder consistently achieves the best performance across all difficulty levels. Specifically, its overall score surpasses the best baseline by 40.5, 34.0, and 29.9 points on the simple, medium, and complex levels, respectively, demonstrating the overall superiority of our framework. For execution success rate, SlideCoder outperforms the best baseline by 38%, 32%, and 27% across the three difficulty levels, indicating that the proposed H-RAG and CGSeg mechanisms significantly enhance model performance and reduce task difficulty.\n\nMoreover, SlideCoder outperforms all baselines in both Local Structural Metrics and Global Visual Metrics, confirming its strong fidelity in preserving both the structural layout and visual appearance of the original slides. The stepwise decline in Slide-Coder's overall score across increasing difficulty levels further indicates its ability to leverage visual and structural cues from the input slides. In contrast, baseline models relying solely on natural language descriptions exhibit weak sensitivity to slide complexity, failing to reflect the difficulty hierarchy in their overall scores.\n\nOn the SLIDESBENCH dataset (as shown in the lower part of Table [1\\)](#page-6-1), SlideCoder also surpasses all baselines across all metrics, with an overall score of 78.8 when using GPT-4o as the backbone, representing a 11.9 improvement over the best-performing baseline. Notably, the opensource fine-tuned model SlideMaster also demonstrates competitive performance, outperforming the best GPT-4o-based baseline on both datasets.\n\n#### <span id=\"page-5-0\"></span>5.3 Reverse Tool Analysis\n\nTable [2](#page-6-2) summarizes the supported object types and corresponding styles in our proposed reverse engineering tool. Our tool supports 10 commonly used object types and 44 distinct object styles, whereas Autopresent [\\(Ge et al.,](#page-8-2) [2025\\)](#page-8-2) only supports 5 object types and 16 styles. Detailed comparisons can be found in Appendix [B.](#page-10-3) To quantitatively assess the reverse engineering capabilities of both tools, we adopt two evaluation metrics:\n\nReconstruction Ratio: This metric calculates the ratio between the number of shapes in the slide <span id=\"page-6-1\"></span>Table 1: Results on Slide2Code (top) and SLIDESBENCH (bottom) using SlideCoder and AutoPresent with different MLLMs. Green , yellow , and red indicate simple, medium, and complex levels in SlideCoder. Bolded values mark the best result per level.\n\n| Framework   | Backbone        | Execution% | Local Structural Metrics |          | Global Visual Metrics |      | Overall |\n|-------------|-----------------|------------|--------------------------|----------|-----------------------|------|---------|\n|             |                 |            | Content                  | Position | Clip                  | SSIM |         |\n| Slide2Code  |                 |            |                          |          |                       |      |         |\n|             |                 | 61.0       | 92.7                     | 78.9     | 70.8                  | 80.3 | 48.6    |\n|             | AutoPresent     | 53.0       | 89.6                     | 77.3     | 69.2                  | 79.1 | 41.4    |\n|             |                 | 67.0       | 87.2                     | 71.4     | 65.9                  | 73.4 | 48.5    |\n|             |                 | 57.0       | 91.4                     | 78.3     | 69.7                  | 79.0 | 44.8    |\n| AutoPresent | Gemini2.0-flash | 68.0       | 88.7                     | 79.9     | 66.3                  | 71.6 | 51.5    |\n|             |                 | 66.0       | 89.3                     | 72.2     | 63.1                  | 64.7 | 45.2    |\n|             |                 | 58.0       | 92.7                     | 80.9     | 68.8                  | 75.6 | 45.4    |\n|             | GPT-4o          | 50.0       | 92.3                     | 74.6     | 67.6                  | 72.6 | 36.8    |\n|             |                 | 69.0       | 90.3                     | 73.3     | 62.3                  | 63.3 | 47.1    |\n|             | SlideMaster     | 86.0       | 92.4                     | 87.4     | 77.6                  | 91.1 | 76.7    |\n|             |                 | 75.0       | 84.7                     | 79.8     | 75.4                  | 86.4 | 61.7    |\n|             |                 | 73.0       | 76.1                     | 70.5     | 72.4                  | 82.8 | 54.2    |\n|             | Gemini2.0-flash | 97.0       | 94.5                     | 88.6     | 81.3                  | 90.7 | 87.0    |\n| SlideCoder  |                 | 90.0       | 90.9                     | 84.6     | 82.3                  | 85.5 | 76.6    |\n|             |                 | 88.0       | 92.7                     | 80.9     | 81.7                  | 81.2 | 71.6    |\n|             | GPT-4o          | 99.0       | 96.3                     | 88.1     | 79.8                  | 91.8 | 89.1    |\n|             |                 | 100.0      | 92.5                     | 84.7     | 81.5                  | 86.2 | 85.5    |\n|             |                 | 96.0       | 94.3                     | 80.0     | 80.7                  | 82.6 | 78.4    |\n|             | SLIDESBENCH     |            |                          |          |                       |      |         |\n|             | AutoPresent     | 84.1       | 92.2                     | 67.2     | 81.6                  | 73.7 | 65.3    |\n| AutoPresent | Gemini2.0-flash | 56.4       | 91.7                     | 62.9     | 77.1                  | 66.0 | 40.4    |\n|             | GPT-4o          | 86.7       | 92.5                     | 76.3     | 78.0                  | 70.8 | 66.9    |\n|             | SlideMaster     | 87.2       | 91.5                     | 76.9     | 73.4                  | 80.0 | 68.4    |\n| SlideCoder  | Gemini2.0-flash | 89.7       | 90.0                     | 85.4     | 81.8                  | 80.0 | 75.0    |\n|             | GPT-4o          | 94.9       | 94.8                     | 83.9     | 82.1                  | 80.9 | 78.8    |\n\n<span id=\"page-6-2\"></span>Table 2: Object Types and Corresponding Style count\n\n| Type Name        | Ours | AutoPresent's |\n|------------------|------|---------------|\n| title            | 10   | 3             |\n| textbox          | 10   | 5             |\n| bullet points    | 8    | 5             |\n| background color | 1    | 1             |\n| image            | 2    | 2             |\n| placeholder      | 4    | –             |\n| freeform         | 2    | –             |\n| connector        | 5    | –             |\n| table            | 4    | –             |\n| triangle         | 5    | –             |\n\nreconstructed from the reverse-engineered code and the original slide. Our tool achieves a reconstruction ratio of 90.38%, significantly outperforming [\\(Ge et al.,](#page-8-2) [2025\\)](#page-8-2), which only reaches 65.67%. This demonstrates the broader object type coverage enabled by our tool.\n\nCLIP Score: Our method achieves a CLIP score [\\(Hessel et al.,](#page-8-17) [2021\\)](#page-8-17) of 88.66%, whereas Autopresent [\\(Ge et al.,](#page-8-2) [2025\\)](#page-8-2) only achieves 69.87%. The higher score indicates that our reverseengineered slides more accurately preserve the visual and stylistic details of the original, owing to the broader support for object types and styles.\n\n#### <span id=\"page-6-0\"></span>5.4 Slide Complexity Metric Analysis\n\nTo evaluate the effectiveness of the proposed Slide Complexity Metric (SCM), we conducted a human subject study. A total of 100 samples were randomly selected from the Slide2Code benchmark for evaluation. Four doctoral students were recruited as annotators, each assigned 50 slides to assess. The annotators were instructed to score each slide from the perspective of three dimensions: the number of shapes, the diversity of shape types, and the level of element coverage. The scoring range was 0–100, following the protocol in Appendix [D.](#page-10-4) Each slide was rated independently by two annotators, and the final score was their average.\n\nTo assess the alignment between SCM and human perception, we first compute the Pearson correlation coefficient [\\(Cohen et al.,](#page-8-18) [2009\\)](#page-8-18) between the SCM complexity scores and the averaged human scores. The result is r = 0.873 with a p-value of 2.776 × 10−32, indicating a strong and statistically significant correlation. Additionally, we calculated the intraclass correlation coefficient [\\(Koo and Li,](#page-9-18) [2016\\)](#page-9-18) between the SCM scores and each individual annotator's score to assess consistency. The ICC result is 0.726 with a p-value of 1.186 × 10−<sup>31</sup> ,\n\n<span id=\"page-7-1\"></span>\n\n|           |  |        | SlideCoder      |                 |        | AutoPresent     |                 |\n|-----------|--|--------|-----------------|-----------------|--------|-----------------|-----------------|\n| Reference |  | GPT-4o | Gemini2.0-flash | SlideMaster(7B) | GPT-4o | Gemini2.0-flash | AutoPresent(8B) |\n| Simple    |  |        |                 |                 | Error  |                 |                 |\n| Median    |  |        |                 |                 | Error  | Error           |                 |\n| Complex   |  |        |                 |                 |        | Error           | Error           |\n\nFigure 5: Examples of slides generated by different methods in three difficulty levels.\n\ndemonstrating substantial agreement between SCM and human evaluations. These results confirm that SCM is a reliable and objective metric aligned with human judgment of slide complexity.\n\n### 5.5 Ablation Study\n\n<span id=\"page-7-0\"></span>Table 3: Overall performance of ablation study.\n\n| Setting        | Execution% | Overall |\n|----------------|------------|---------|\n|                | 100.0      | 89.9    |\n| SlideCoder     | 100.0      | 85.8    |\n|                | 100.0      | 82.2    |\n|                | 100.0      | 81.2    |\n| w/o Layout     | 93.9       | 73.6    |\n|                | 93.9       | 71.8    |\n|                | 75.8       | 55.4    |\n| w/o CGSeg      | 51.5       | 39.6    |\n|                | 69.7       | 48.4    |\n|                | 90.9       | 80.4    |\n| w/o H-RAG      | 81.8       | 69.3    |\n|                | 84.8       | 70.7    |\n|                | 75.8       | 53.9    |\n| Native Setting | 48.5       | 37.4    |\n|                | 66.7       | 46.9    |\n\nWe design three ablation settings to validate the effectiveness of different components in our framework: (1) w/o Layout, removes the layout-aware prompt; (2) w/o CGSeg, disables both the CGSeg mechanism and the layout-aware prompt; (3) w/o H-RAG, removes the <Grammar> content from all prompts.(4) Native setting, which removes H-RAG on top of the w/o CGSeg setting. Detailed descriptions are provided in Appendix [A.1.](#page-9-19) We randomly sample 33 instances from each difficulty level, resulting in a total of 99 samples, and perform inference using GPT-4o. The overall results are reported in Table [3,](#page-7-0) with detailed metric result provided in Appendix [A.2.](#page-10-5) After removing each component, both execution rate and overall score\n\nexhibit varying degrees of decline, which demonstrates the contribution of each component to the overall framework. Notably, the w/o CGSeg setting shows significant performance drops across all metrics. Although slightly better than the Native setting due to the presence of H-RAG.\n\n#### 5.6 Case Study\n\nFigure [5](#page-7-1) presents slides generated by different models under three levels of difficulty. It can be observed that models based on natural language often fail to satisfy the detailed and layout-specific requirements of reference images. These models frequently produce slides with overlapping elements or content that extends beyond canvas boundaries. In medium and complex samples, the generated code often fails to compile. In contrast, Slide-Coder's CGSeg mechanism enables the MLLM to focus more effectively on fine-grained details. Moreover, the layout-aware prompt helps ensure that the spatial arrangement of elements aligns more closely with reference image.\n\n## 6 Conclusion\n\nWe introduce a new Reference Image to Slide Generation task and a novel Slide Complexity Metric for evaluating slide complexity. Based on this metric, we build the Slide2Code benchmark with different levels of difficulty. We also propose Slide-Coder enhanced by a Color Gradients-based Segmentation algorithm, a Layout-aware Prompt and a Hierarchical Retrieval-Augmented Code Generation for accurate slide generation. A high-quality training set is curated to fine-tune a 7B open-source model. Experimental results show that SlideCoder outperforms the strongest baselines.\n\n# Limitations\n\nIn this work, we take the first step toward visionbased slide generation. While our method achieves substantial improvements across multiple evaluation metrics, several limitations remain unaddressed. First, the current framework focuses on generating a single slide from one reference image and does not explore the multi-slide generation scenario. Second, we assume that user input contains separate design and image components, and do not handle the case where a complete slide with embedded pictures is provided as input. Third, due to budget and time constraints, our segmentation algorithm adopts a fixed-rule paradigm. Future work may investigate more flexible model-based detection approaches to enable adaptive and accurate block partitioning.\n\n# References\n\n- <span id=\"page-8-8\"></span>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.\n- <span id=\"page-8-0\"></span>Shaikh Mostafa Al Masum, Mitsuru Ishizuka, and Md Tawhidul Islam. 2005. 'auto-presentation': a multi-agent system for building automatic multimodal presentation of a topic from world wide web information. In *IEEE/WIC/ACM International Conference on Intelligent Agent Technology*, pages 246– 249. IEEE.\n- <span id=\"page-8-7\"></span>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. *arXiv preprint arXiv:2502.13923*.\n- <span id=\"page-8-13\"></span>SV Burtsev and Ye P Kuzmin. 1993. An efficient floodfilling algorithm. *Computers & graphics*, 17(5):549– 561.\n\n<span id=\"page-8-4\"></span>Steve Canny. 2023. Python-ptx documentation.\n\n- <span id=\"page-8-14\"></span>Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. M3 embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through selfknowledge distillation. In *Findings of the Association for Computational Linguistics ACL 2024*, pages 2318–2335.\n- <span id=\"page-8-18\"></span>Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson correlation coefficient. *Noise reduction in speech processing*, pages 1–4.\n- <span id=\"page-8-5\"></span>Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The power of noise: Redefining retrieval for rag systems. In *Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval*, pages 719–729.\n- <span id=\"page-8-6\"></span>Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A survey on rag meeting llms: Towards retrieval-augmented large language models. In *Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, pages 6491– 6501.\n- <span id=\"page-8-9\"></span>Tsu-Jui Fu, William Yang Wang, Daniel McDuff, and Yale Song. 2022. Doc2ppt: Automatic presentation slides generation from scientific documents. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36, pages 634–642.\n- <span id=\"page-8-2\"></span>Jiaxin Ge, Zora Zhiruo Wang, Xuhui Zhou, Yi-Hao Peng, Sanjay Subramanian, Qinyue Tan, Maarten Sap, Alane Suhr, Daniel Fried, Graham Neubig, and 1 others. 2025. Autopresent: Designing structured visuals from scratch. *arXiv preprint arXiv:2501.00912*.\n- <span id=\"page-8-1\"></span>Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang, and 1 others. 2023. Openagi: When llm meets domain experts. *Advances in Neural Information Processing Systems*, 36:5539–5568.\n- <span id=\"page-8-16\"></span>Google. 2025. Gemini API. [https://ai.google.](https://ai.google.dev/gemini-api) [dev/gemini-api](https://ai.google.dev/gemini-api). Accessed: 2025-05-19.\n- <span id=\"page-8-3\"></span>Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*.\n- <span id=\"page-8-12\"></span>Jun Han and Claudio Moraga. 1995. The influence of the sigmoid function parameters on the speed of backpropagation learning. In *International workshop on artificial neural networks*, pages 195–201. Springer.\n- <span id=\"page-8-17\"></span>Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: A referencefree evaluation metric for image captioning. *arXiv preprint arXiv:2104.08718*.\n- <span id=\"page-8-15\"></span>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. *ICLR*, 1(2):3.\n- <span id=\"page-8-10\"></span>Yue Hu and Xiaojun Wan. 2014. Ppsgen: Learningbased presentation slides generation for academic papers. *IEEE transactions on knowledge and data engineering*, 27(4):1085–1097.\n- <span id=\"page-8-11\"></span>Min-Yen Kan. 2007. Slideseer: A digital library of aligned document and presentation pairs. In *Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries*, pages 81–90.\n- <span id=\"page-9-18\"></span>Terry K Koo and Mae Y Li. 2016. A guideline of selecting and reporting intraclass correlation coefficients for reliability research. *Journal of chiropractic medicine*, 15(2):155–163.\n- <span id=\"page-9-9\"></span>Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, Zhiyong Huang, and Jing Ma. 2024. Mmcode: Benchmarking multimodal large language models for code generation with visually rich programming problems. *arXiv preprint arXiv:2404.09486*.\n- <span id=\"page-9-11\"></span>Jenny GuangZhen Ma, Karthik Sreedhar, Vivian Liu, Pedro A Perez, Sitong Wang, Riya Sahni, and Lydia B Chilton. 2025. Dynex: Dynamic code synthesis with structured design exploration for accelerated exploratory programming. In *Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems*, pages 1–27.\n- <span id=\"page-9-0\"></span>Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an llm to help with code understanding. In *Proceedings of the IEEE/ACM 46th International Conference on Software Engineering*, pages 1–13.\n- <span id=\"page-9-17\"></span>Jim Nilsson and Tomas Akenine-Möller. 2020. Understanding ssim. *arXiv preprint arXiv:2006.13846*.\n- <span id=\"page-9-6\"></span>Kunato Nishina and Yusuke Matsui. 2024. Svgeditbench: A benchmark dataset for quantitative assessment of llm's svg editing capabilities. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 8142–8147.\n- <span id=\"page-9-5\"></span>Juan A Rodriguez, Abhay Puri, Shubham Agarwal, Issam H Laradji, Sai Rajeswar, David Vazquez, Christopher Pal, and Marco Pedersoli. 2025. Starvector: Generating scalable vector graphics code from images and text. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 39, pages 29691– 29693.\n- <span id=\"page-9-15\"></span>Athar Sefid and Jian Wu. 2019. Automatic slide generation for scientific papers. In *Third International Workshop on Capturing Scientific Knowledge colocated with the 10th International Conference on Knowledge Capture (K-CAP 2019), SciKnow@ K-CAP 2019*.\n- <span id=\"page-9-4\"></span>Yuxuan Wan, Yi Dong, Jingyu Xiao, Yintong Huo, Wenxuan Wang, and Michael R Lyu. 2024. Mrweb: An exploration of generating multi-page resourceaware web code from ui designs. *arXiv preprint arXiv:2412.15310*.\n- <span id=\"page-9-7\"></span>Ronghuan Wu, Wanchao Su, and Jing Liao. 2024. Chat2svg: Vector graphics generation with large language models and image diffusion models. *arXiv preprint arXiv:2411.16602*.\n- <span id=\"page-9-1\"></span>Jingyu Xiao, Yuxuan Wan, Yintong Huo, Zhiyao Xu, and Michael R Lyu. 2024. Interaction2code: How far are we from automatic interactive webpage generation? *arXiv preprint arXiv:2411.03292*.\n- <span id=\"page-9-2\"></span>Jingyu Xiao, Ming Wang, Man Ho Lam, Yuxuan Wan, Junliang Liu, Yintong Huo, and Michael R. Lyu. 2025. [Designbench: A comprehensive benchmark](https://arxiv.org/abs/2506.06251) [for mllm-based front-end code generation.](https://arxiv.org/abs/2506.06251) *Preprint*, arXiv:2506.06251.\n- <span id=\"page-9-8\"></span>Ximing Xing, Juncheng Hu, Guotao Liang, Jing Zhang, Dong Xu, and Qian Yu. 2024. Empowering llms to understand and generate complex vector graphics. *arXiv preprint arXiv:2412.11102*.\n- <span id=\"page-9-12\"></span>John Yang, Carlos E Jimenez, Alex L Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R Narasimhan, and 1 others. 2024. Swe-bench multimodal: Do ai systems generalize to visual software domains? *arXiv preprint arXiv:2410.03859*.\n- <span id=\"page-9-13\"></span>Mingyue Yuan, Jieshan Chen, Zhenchang Xing, Aaron Quigley, Yuyu Luo, Tianqi Luo, Gelareh Mohammadi, Qinghua Lu, and Liming Zhu. 2024. Designrepair: Dual-stream design guideline-aware frontend repair with large language models. *arXiv preprint arXiv:2411.01606*.\n- <span id=\"page-9-3\"></span>Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, and 1 others. 2024. Web2code: A large-scale webpage-to-code dataset and evaluation framework for multimodal llms. *arXiv preprint arXiv:2406.20098*.\n- <span id=\"page-9-10\"></span>Fengji Zhang, Linquan Wu, Huiyu Bai, Guancheng Lin, Xiao Li, Xiao Yu, Yue Wang, Bei Chen, and Jacky Keung. 2024a. Humaneval-v: Evaluating visual understanding and reasoning abilities of large multimodal models through coding tasks. *arXiv preprint arXiv:2410.12381*.\n- <span id=\"page-9-14\"></span>Linhao Zhang, Daoguang Zan, Quanshun Yang, Zhirong Huang, Dong Chen, Bo Shen, Tianyu Liu, Yongshun Gong, Pengjie Huang, Xudong Lu, and 1 others. 2024b. Codev: Issue resolving with visual data. *arXiv preprint arXiv:2412.17315*.\n- <span id=\"page-9-16\"></span>Hao Zheng, Xinyan Guan, Hao Kong, Jia Zheng, Weixiang Zhou, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun. 2025. Pptagent: Generating and evaluating presentations beyond text-to-slides. *arXiv preprint arXiv:2501.03936*.\n\n# A Detail ablation analysis\n\n### <span id=\"page-9-19\"></span>A.1 Details of Ablation Settings\n\n- w/o Layout: Removes only the layout-aware prompt, meaning that the input to Assembler does not contain the positional coordinates of each block.\n- w/o CGSeg: Disables the CGSeg mechanism. Since the goal of Coder is to generate partial code and Assembler is responsible for\n\ncode assembly, the removal of CGSeg renders Assembler unnecessary. Consequently, both Assembler and its layout-aware prompt are removed in this setting, and the output code generated by Coder is directly treated as the final output of the framework.\n\n- w/o H-RAG: Disables the retrieval of knowledge base content for all agents.\n- Native setting: Disables both H-RAG and CSeg components. Specifically, we input ordinary prompts that do not incorporate H-RAG, allowing the MLLMs to generate complete slide code directly from the reference image. This setup is used to evaluate the baseline capability of native MLLMs in handling the reference image to slide code generation task.\n\n## <span id=\"page-10-5\"></span>A.2 Detailed Analysis of Ablation Results\n\nTable [4](#page-11-0) provides a detailed evaluation metrics under different ablation settings.\n\nIn the w/o Layout setting, the Position score under the complex level drops significantly from 81.35 to 72.16. This is primarily because, in complex cases, the CGSeg algorithm typically divides the Reference Image(RI) into more blocks, and without layout information, the Agent struggles to model spatial relationships among multiple elements. This often leads to overlapping or outof-bound content, causing a sharp decline in the Position metric and slightly affecting other metrics as well.\n\nIn the w/o CGSeg setting, both the CGSeg mechanism and the layout-aware prompt are removed. As a result, a single Describer Agent is required to handle the entire complex slide, which exceeds its processing capacity, often leading to code generation failures and a sharp drop in execution success rate. Its performance is slightly better than the Native setting due to the additional knowledge provided by H-RAG.\n\nIn the w/o H-RAG setting, the <Grammar> component is removed from each Agent. Excluding this component from Describer reduces its ability to accurately identify the corresponding python-pptx object. Similarly, removing it from Coder and Assembler deprives the Agents of essential syntactic guidance, often resulting in version-related errors caused by inconsistencies between the model's training data and the current version of the python-pptx library. These combined factors lead to overall performance degradation.\n\nIn the Native setting, both the CGSeg mechanism and H-RAG are removed, leaving a single Coder Agent to handle the entire slide without any auxiliary support. This reduces the framework to a plain MLLM-based inference process, severely limiting its ability to generate structured and executable code, and resulting in the lowest execution rate and overall performance.\n\n# <span id=\"page-10-3\"></span>B Detailed comparisons of Reverse Tool\n\nTable [5](#page-11-1) lists the object types and their styles supported by our reverse engineering tool.\n\nTable [6](#page-11-2) lists the object types and their styles supported by AutoPresent's reverse engineering tool.\n\n# <span id=\"page-10-2\"></span>C LoRA fine-tuning parameters\n\nThe LoRA fine-tuning parameters are listed in Table [7.](#page-18-0)\n\n# <span id=\"page-10-4\"></span>D Evaluation Dimensions and Scoring Criteria\n\nThe evaluation guidelines for the four doctoral student annotators are provided in Figure [6.](#page-12-0)\n\n## <span id=\"page-10-1\"></span>E Prompt Templates\n\nThe prompt templates for the Describer and Coder are shown in Figure [7](#page-13-0) and Figure [8,](#page-14-0) respectively. Layout-aware prompt is shown in Figure [9.](#page-15-0)\n\n# <span id=\"page-10-0\"></span>F Details of the Knowledge Base Construction\n\nFigure [10](#page-16-0) presents several examples from the Shape Type Knowledge Base, which consists of object types defined in the python-pptx library along with their corresponding descriptions. Figure [11](#page-17-0) shows an example from the Operation Function Knowledge Base, which includes the function name, parameters, return value, usage example, and a textual explanation of the function.\n\n|                |            | Global Visual Metrics |          | Local Structural Metrics |      |         |\n|----------------|------------|-----------------------|----------|--------------------------|------|---------|\n| Setting        | Execution% | Content               | Position | Clip                     | SSIM | Overall |\n|                | 100.0      | 97.1                  | 89.9     | 80.8                     | 92.9 | 89.9    |\n| SlideCoder     | 100.0      | 92.7                  | 86.5     | 82.7                     | 85.8 | 85.8    |\n|                | 100.0      | 95.0                  | 81.3     | 82.2                     | 82.3 | 82.2    |\n|                | 100.0      | 88.8                  | 86.4     | 81.2                     | 79.2 | 81.2    |\n| w/o Layout     | 93.9       | 90.4                  | 75.2     | 80.9                     | 78.4 | 73.6    |\n|                | 93.9       | 93.6                  | 72.2     | 80.3                     | 76.4 | 71.8    |\n|                | 75.8       | 90.4                  | 86.5     | 69.4                     | 73.1 | 55.4    |\n| w/o CGSeg      | 51.5       | 91.7                  | 81.4     | 68.5                     | 71.4 | 39.6    |\n|                | 69.7       | 93.0                  | 83.2     | 68.1                     | 69.0 | 48.4    |\n|                | 90.9       | 98.6                  | 88.4     | 79.7                     | 91.8 | 80.4    |\n| w/o H-RAG      | 81.8       | 91.6                  | 84.7     | 81.7                     | 87.8 | 69.3    |\n|                | 84.8       | 94.0                  | 87.9     | 81.3                     | 83.4 | 70.7    |\n|                | 75.8       | 90.0                  | 87.9     | 71.1                     | 71.2 | 53.9    |\n| Native Setting | 48.5       | 92.9                  | 83.3     | 66.7                     | 69.5 | 37.4    |\n|                | 66.7       | 92.6                  | 85.7     | 66.5                     | 70.4 | 46.9    |\n\n<span id=\"page-11-0\"></span>Table 4: Detailed performance analysis under several ablation settings. Green , yellow , and red indicate simple, medium, and complex levels in SlideCoder. Bolded values mark the best result per level.\n\nTable 5: The object types and their styles supported by our reverse engineering tool.\n\n<span id=\"page-11-1\"></span>\n\n| Object Type        | Styles                                                                             |  |  |\n|--------------------|------------------------------------------------------------------------------------|--|--|\n| textbox            | Position, Text frame margin, Alignment, Paragraph spacing, Font style, Fill        |  |  |\n|                    | color, Font size, Bold, Italic, Underline                                          |  |  |\n| rectangle          | Position, Line color, Line width, Fill color                                       |  |  |\n| object_placeholder | Position, Fill color, Object position                                              |  |  |\n| freeform           | Position, Fill color                                                               |  |  |\n| bullet_points      | Position, Item content, Font size, Font color, Fill color, Bold, Italic, Underline |  |  |\n| image              | Position, Image path                                                               |  |  |\n| background_color   | Color                                                                              |  |  |\n| connector          | Start position, End position, Arrow color, Arrow width, Arrow style                |  |  |\n| table              | Position, Cell height, Cell fill color, Text inside cell                           |  |  |\n| triangle           | Position, Type, Line color, Line width, Fill color                                 |  |  |\n\nTable 6: The object types and their styles supported by AutoPresent's reverse engineering tool.\n\n<span id=\"page-11-2\"></span>\n\n| Object Type      | Styles                                                    |\n|------------------|-----------------------------------------------------------|\n| title            | Font size, Font color, Fill color                         |\n| textbox          | Position, Font size, Bold, Font color, Fill color         |\n| bullet_points    | Position, Item content, Font size, Font color, Fill color |\n| image            | Position, Image path                                      |\n| background color | Color                                                     |\n\n# <span id=\"page-12-0\"></span>**Slide Complexity Evaluation Guide**\n\n#### **Purpose of Evaluation**\n\nThis guideline is intended to assist you in subjectively evaluating the complexity of slide samples based on the following three dimensions:\n\n- 1. **Number of Shapes**\n- 2. **Diversity of Shape Types**\n- 3. **Visual Complexity**\n\nEach dimension should be scored on a scale from **0 to 100**. You are expected to assess each slide independently and provide a **final overall score** reflecting your holistic judgment of the slide's complexity.\n\n#### **Evaluation Procedure**\n\nFor each slide, please follow these steps:\n\n- 1. Review the slide thoroughly to understand its structure and element layout.\n- 2. Evaluate each of the three dimensions separately (see detailed criteria below).\n- 3. Based on your judgment, assign a comprehensive **overall score** (0–100).Record your scores (three dimensions + overall) clearly in the scoring table.\n\n#### **Scoring Dimensions and Criteria**\n\n#### **1. Number of Shapes**\n\nRefers to the total count of visual elements on the slide, including but not limited to: text boxes, diagrams, arrows, lines, images, geometric shapes, etc.\n\n- **0–20**: Very few elements (e.g., only a title and 1–3 text boxes).\n- **21–50**: Moderate amount of shapes (e.g., 4–10 elements, such as text + one chart).\n- **51–80**: High density of shapes (e.g., 11–20 elements, visually filled slide).\n- **81–100**: Extremely dense, cluttered with over 20 elements.\n\n#### **2. Diversity of Shape Types**\n\nMeasures how varied the types of visual components are. Common types include text boxes, images, tables, flowcharts, icons, arrows, geometric shapes (e.g., rectangles, circles, lines), and more.\n\n- **0–20**: Only one type used (e.g., all text).\n- **21–50**: Two or three different types, basic variety.\n- **51–80**: Four to six types, indicating notable diversity.\n- **81–100**: Rich variety with more than six distinct shape types.\n\n#### **3. Visual Complexity**\n\nRefers to how complex the slide appears in terms of visual density, layout structure, information layering, and cognitive load. It captures the subjective perception of how \"complicated\" the slide looks.\n\n- **0–20**: Very clean and minimalist, with generous whitespace.\n- **21–50**: Well-structured, moderately filled, visually comfortable.\n- **51–80**: Noticeably dense, some clutter, yet still readable.\n- **81–100**: Overwhelming amount of information, chaotic layout, hard to scan quickly.\n\n#### **Overall Score Guidelines**\n\nAfter rating the three dimensions above, you are asked to provide a **final overall score** (0–100) that reflects your subjective judgment of the slide's overall complexity.\n\n⚠ Note: This **does not need to be a simple average** of the three scores. Instead, consider how each factor influences the overall perception of complexity.\n\nFigure 6: Evaluation guidelines provided to the four doctoral student annotators.\n\n<span id=\"page-13-0\"></span>\n\nFigure 7: Prompt of Describer.\n\n#### <span id=\"page-14-0\"></span>**Prompt of Coder**\n\n#### **Code generation process**\n\nPlease write Python code to create a PowerPoint slide that matches the following description: *{block\\_description}*\n\nThe following is an introduction in python-pptx API Documentation: *{<Grammar> }*\n\nPlease generate Python code using the python-pptx library to create a PowerPoint slide based on the provided codes. The code should:\n\n1. Create a new PowerPoint presentation.\n\n2. Add a slide using the slide layout with index 6 (typically a Blank Layout) to ensure a clean slate for custom content placement.\n\n3. Include all text elements and shapes as specified in the slide, with properties such as font, size, color, and alignment accurately applied.\n\n4. Use inches (in) units for all size and position measurements, directly converting them using python-pptx's Inches() function for shapes and positions, and Pt for font sizes.\n\n5. Save the presentation in the output/generated\\_ppts directory with a descriptive filename (e.g., generated\\_slide.pptx).\n\n6. Ensure the code is well-commented and handles any necessary imports. *{block\\_image}*\n\n#### **Fix code process**\n\nYou are a python-pptx expert. The previous code generated an error. Please fix the code. Error message: *{error\\_message}* Previous code: *{code}*\n\nIntroduction in python-pptx API Documentation: *{<Grammar> }* Please provide the complete corrected code that will create the PowerPoint slide successfully.\n\nFigure 8: Prompt of Coder.\n\n#### <span id=\"page-15-0\"></span>**Layout-aware prompt**\n\nYou are a python-pptx expert. Based on the information and code snippets I provide, please assemble a complete python-pptx script: *<Design>* refers to the reference image for this slide.\n\nIts global description is *<Overall Description>*.\n\nThe code snippets and their layout positions are given as *<Code Snippets1>, <Position1\\*>. <Code Snippets1>, <Position1\\*>.*  …\n\nHere are some syntax rules that might be useful: *<Grammar>***.** \n\nThe background and images path is ...\n\nBackground path: *{background\\_image\\_path}* Image1 Path: *{image\\_path\\_1}* Image1 Coordinates: Left: *{x1}* inches Top: *{y1}* inches Width: *{w1}* inches Height: *{h1}* inches\n\nPlease provide the complete corrected code that will create the PowerPoint slide successfully. Please generate Python code using the python-pptx library to create a PowerPoint slide based on the provided codes. The code should:\n\n- 1. Create a new PowerPoint presentation.\n- 2. Add a slide using the slide layout with index 6 (typically a Blank Layout) to ensure a clean slate for custom content placement.\n- 3. Include all text elements and shapes as specified in the slide, with properties such as font, size, color, and alignment accurately applied.\n- 4. Use inches (in) units for all size and position measurements, directly converting them using python-pptx's Inches() function for shapes and positions, and Pt for font sizes.\n- 5. Save the presentation in the output/generated\\_ppts directory with a descriptive filename (e.g., generated\\_slide.pptx).\n- 6. Ensure the code is well-commented and handles any necessary imports.\n\nFigure 9: Layout-aware prompt.\n\n#### <span id=\"page-16-0\"></span>Auto Shape\n\nAn auto shape is a predefined, customizable shape in PowerPoint, such as a rectangle, ellipse, or block arrow, with approximately 180 variations. Auto shapes can have a fill, outline, and contain text. Some include adjustable features, indicated by yellow diamond handles (e.g., to modify the corner radius of a rounded rectangle). A text box is a specific type of auto shape, typically rectangular, with no default fill or outline.\n\n#### ########\n\n#### Picture\n\nA picture in PowerPoint refers to a raster image, such as a photograph or clip art, treated as a distinct shape type with unique behaviors compared to auto shapes. Note that an auto shape can have a picture fill, where an image serves as the shape's background instead of a color or gradient, but this is a separate feature.\n\n#### ########\n\n#### Graphic Frame\n\nA graphic frame is a container that automatically appears in a PowerPoint file when adding graphical objects like tables, charts, SmartArt diagrams, or media clips. It cannot be inserted independently and typically requires no direct interaction from the user.\n\n#### ########\n\n#### Group Shape\n\nA group shape is created when multiple shapes in PowerPoint are grouped, enabling them to be selected, moved, resized, or filled as a single unit. The group shape is only visible through its bounding box when selected, containing the individual member shapes.\n\n#### ########\n\n#### Line/Connector\n\nLines are linear shapes distinct from auto shapes. Some lines, known as connectors, can attach to other shapes and remain connected when those shapes are moved. Connectors are not yet fully supported in some contexts, but they are valuable for creating dynamic diagrams.\n\n#### ########\n\n#### Content Part\n\nA content part involves embedding external XML data, such as SVG, within a PowerPoint presentation. PowerPoint itself does not actively utilize content parts, and they can generally be ignored without impacting functionality.\n\n……\n\nFigure 10: Examples from the Shape Type knowledge base.\n\n<span id=\"page-17-0\"></span># Function: `pptx.Presentation`\n\n## Function Name\n\n`pptx.Presentation`\n\n## Function Parameters\n\n- \\*\\*pptx\\*\\* (`Union[str, IO[bytes], None]`, optional, default: `None`)\n- Description: Specifies the source of the presentation.\n- If a `str`, it represents the file path to a `.pptx` file.\n- If an `IO[bytes]`, it represents a file-like object containing the `.pptx` file data.\n- If `None`, loads the built-in default presentation template.\n- Constraints: The file or stream must be a valid `.pptx` file if provided.\n\n## Function Return Value\n\n- \\*\\*Type\\*\\*: `presentation.Presentation`\n\n- \\*\\*Description\\*\\*: A `Presentation` object representing the loaded or newly created PowerPoint presentation.\n\n## Function Python Example\n\n```python from pptx import Presentation\n\n# Create a new presentation using the default template prs = Presentation()\n\n# Load an existing presentation from a file prs = Presentation(\"existing\\_presentation.pptx\")\n\n# Load a presentation from a file-like object from io import BytesIO with open(\"existing\\_presentation.pptx\", \"rb\") as f: prs = Presentation(BytesIO(f.read())) ```\n\n## Function Purpose\n\nThe `pptx.Presentation` function is the primary entry point for creating or loading a PowerPoint presentation. It initializes a `Presentation` object, which provides access to slides, slide masters, layouts, and other presentation components, enabling programmatic manipulation of presentation content.\n\nFigure 11: An example from the Operation Function knowledge base.\n\n<span id=\"page-18-0\"></span>Table 7: LoRA fine-tuning configuration used in our experiments.\n\n| Parameter                   | Value |\n|-----------------------------|-------|\n| Rank                        | 8     |\n| Max Sequence Length         | 4096  |\n| Batch Size                  | 4     |\n| Gradient Accumulation Steps | 8     |\n| Learning rate               | 1e-4  |\n| Epochs                      | 10    |\n| Warmup Ratio                | 0.1   |\n| Mixed Precision             | bf16  |，分析其研究动机、核心方法与公式推导细节。请结合摘要与正文信息，提取论文背景、问题定义、方法核心流程与理论基础。\n",
        "agent": "论文解读专家\n",
        "status": "started"
    }
]
