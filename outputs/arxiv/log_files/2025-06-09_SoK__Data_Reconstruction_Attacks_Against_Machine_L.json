[
    {
        "timestamp": "2025-06-10 16:49:34",
        "task_name": "research_task",
        "task": "阅读论文《SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark》，论文内容如下：# <span id=\"page-0-0\"></span>SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark\n\nRui Wen 1 \\* Yiyong Liu 2 <sup>∗</sup> Michael Backes <sup>2</sup> Yang Zhang 2 † 1 *Institute of Science Tokyo* <sup>2</sup>*CISPA Helmholtz Center for Information Security*\n\n## Abstract\n\nData reconstruction attacks, which aim to recover the training dataset of a target model with limited access, have gained increasing attention in recent years. However, there is currently no consensus on a formal definition of data reconstruction attacks or appropriate evaluation metrics for measuring their quality. This lack of rigorous definitions and universal metrics has hindered further advancement in this field. In this paper, we address this issue in the vision domain by proposing a unified attack taxonomy and formal definitions of data reconstruction attacks. We first propose a set of quantitative evaluation metrics that consider important criteria such as quantifiability, consistency, precision, and diversity. Additionally, we leverage large language models (LLMs) as a substitute for human judgment, enabling visual evaluation with an emphasis on high-quality reconstructions. Using our proposed taxonomy and metrics, we present a unified framework for systematically evaluating the strengths and limitations of existing attacks and establishing a benchmark for future research. Empirical results, primarily from a memorization perspective, not only validate the effectiveness of our metrics but also offer valuable insights for designing new attacks.\n\n## 1 Introduction\n\nThe prosperous development of machine learning techniques has been witnessed in the past decade, which fertilizes its application in real-world scenarios. However, training a machine learning model for privacy-crucial tasks, such as person identification [\\[7,](#page-14-0) [72\\]](#page-17-0), disease prediction [\\[32\\]](#page-15-0), and financial risk prediction [\\[12\\]](#page-14-1), demands a large volume of data which is not only valuable but also sensitive. As a result, model owners tend to release the model only, taking it for granted that the training data will not be leaked.\n\nHowever, a series of works demonstrate that with limited access to a target model, the adversary is capable of inferring partial/complete information about the model's training samples. As a representative, membership inference attacks (MIA) [\\[27](#page-15-1) [,28](#page-15-2) [,33](#page-15-3) [–36](#page-15-4) [,49](#page-16-0) ,[52](#page-16-1) [,67](#page-16-2) [,70](#page-17-1) [,76\\]](#page-17-2) denote a line of works that aim to infer whether a specific data sample is in the target model's training dataset. The disclosure of the membership status is a severe privacy breach as this information could indicate certain sensitive properties of the target sample.\n\nA more challenging privacy attack is data reconstruction, which aims to recover the entire training dataset of a target model. This attack is considered the ultimate privacy breach, as it exposes all information about every sample. While membership inference (MIA) shares some similarities with data reconstruction, several key differences make data reconstruction a stronger attack. MIA is a sample-level attack, determining the membership status of individual samples, while data reconstruction is a dataset-level attack aimed at extracting the entire training dataset. From another angle, MIA is a decision problem, whereas data reconstruction is a search problem. In theory, MIA could aid in data reconstruction if the adversary has a large candidate dataset containing all the target samples and can perfectly predict membership. However, these assumptions are too strong, and to our knowledge, no one has attempted to use MIA for data reconstruction.\n\nOwing to the crucial role of data reconstruction in the privacy domain, numerous attacks have been proposed in recent years. For example, Fredrikson et al. [\\[20\\]](#page-14-2) propose the first data reconstruction attack, namely model inversion, which requires white-box access to the target model. Later, Yang et al. [\\[71\\]](#page-17-3) relax this assumption by leveraging a training-based approach. Zhang et al. [\\[78\\]](#page-17-4) adopt a Generative Adversarial Network (GAN) [\\[22\\]](#page-14-3) to enhance the reconstruction quality. However, some researchers [\\[44](#page-15-5) ,[52\\]](#page-16-1) have claimed that model inversion can only recover a representative sample for each class of the target model, thus not an ideal data reconstruction attack. On the other hand, for certain training paradigms like federated learning [\\[79\\]](#page-17-5) and online learning [\\[47\\]](#page-16-3), some researchers have shown that they are able to reconstruct individual training samples. Moreover, all these works have used different types of evaluation metrics (see [Section 4.1](#page-4-0) for more details).\n\nDespite all the efforts, one of the major problems in the field of data reconstruction is that there does not exist a rigorous and unified definition for the attack. Moreover, the community has no consensus on what are the proper metrics for attack evaluation. This predicament roots in the diversity of the attack scenarios, e.g., various threat models and different training paradigms. At the same time, the lack of universal metrics exacerbates this problem since no existing metric is able to reflect all aspects of the reconstructions. We argue that without a clear-stated definition and metrics, it is hard to conduct further investigation in this direction.\n\n<sup>\\*</sup>The first two authors made equal contributions.\n\n<sup>†</sup>Corresponding author\n\n<span id=\"page-1-0\"></span>In this paper, we take the first step in tackling this problem by 1) providing a definition of data reconstruction, 2) proposing a set of evaluation metrics, and 3) performing a large scale of experiments to establish a benchmark for further study. Specifically, our contributions can be summarized as follows:\n\n- Definition: We recapitulate the reconstruction goal and attack information in existing work and formally provide an attack taxonomy. Based on this, we quantitatively and rigorously define the data reconstruction attack.\n- Metrics: We formalize the desiderata for evaluation metrics and, based on this framework, propose two sets of metrics that address both macro and micro aspects of reconstruction, while emphasizing the importance of diversity in reconstruction quality. Additionally, we mitigate the limitations of using visualization as an evaluation tool by incorporating large language models (LLMs), with this metric specifically designed to assess high-quality reconstructions.\n- Evaluation: We present a unified framework for data reconstruction attacks. Especially from the perspective of memorization, we conduct a thorough evaluation of ten reconstruction attacks under various attack scenarios. Our experiments demonstrate the effectiveness of the proposed metrics and provide a comprehensive analysis of existing attacks.\n\nImplications: In this work, we propose the first rigorous definition of data reconstruction and develop a set of metrics. Our evaluation can serve as a benchmark for the current state-of-the-art approaches. We believe our results pave the way for further investigation into data reconstruction. We will share our code to facilitate research in the field in the future.\n\n## 2 Background\n\n## 2.1 Machine Learning Models\n\nMachine learning algorithms aim to construct models that can accurately predict given inputs. These models are typically represented by a parameterized function *f*<sup>θ</sup> : *X* → *Y* , where *X* denotes the input space and *Y* denotes the output space containing all possible predictions. To determine parameters θ that lead to optimal performance, a common approach is to minimize the following objective function using backpropagation:\n\n$$\n\\min_{\\theta} L(f_{\\theta}(x), y)\n$$\n\nwhere *L* denotes the classification loss, (*x*, *y*) ∈ *X* × *Y* are samples used to train the target model, constituting the training dataset.\n\nRecent research shows that given access to a trained target model *f*θ, the adversary might exploit it through techniques such as membership inference [\\[9,](#page-14-4)[28,](#page-15-2)[35,](#page-15-6)[36](#page-15-4)[,38,](#page-15-7)[39,](#page-15-8)[49,](#page-16-0)[52\\]](#page-16-1) and data reconstruction attacks [\\[1,](#page-14-5)[20,](#page-14-2)[23,](#page-14-6)[47,](#page-16-3)[71,](#page-17-3)[73,](#page-17-6)[78,](#page-17-4)[79\\]](#page-17-5). These attacks focus on inferring micro-level information about individual training samples, potentially leading to a direct privacy breach.\n\n## <span id=\"page-1-1\"></span>2.2 Data Reconstruction Attacks\n\nThe data reconstruction attack aims to recover the target dataset with limited access to the target model, with the aid of additional knowledge possessed by the adversary.\n\nExisting attacks broadly fall into three categories: optimization-based, training-based, and analysis-based. In this paper, we thoroughly investigate ten representative reconstruction attacks, analyzing their performance and limitations. These attacks are briefly introduced below:\n\n## 2.2.1 Optimization-based Attack\n\nMost existing reconstruction attacks can be classified into this particular attack type. Such attacks aim to reconstruct the training dataset by iteratively optimizing the input until the desired class achieves a high likelihood score.\n\nRecent attacks have incorporated generative models to enhance the quality of reconstruction, employing different architectural choices [\\[14,](#page-14-7)[64\\]](#page-16-4) and loss functions [\\[58\\]](#page-16-5). We select seven representative attacks in our paper.\n\nMI-Face [\\[20\\]](#page-14-2): Fredrikson et al. take the first step in reconstructing training samples from a trained model. Given a class *y*, the method first initializes a sample, and then updates *x* to maximize the likelihood/probability of belonging to that class, i.e.,\n\n$$\n\\min_{x} \\mathcal{L}(f_{\\theta}(x), y)\n$$\n\nwhere *L* is the classification loss. Similar to training a machine learning model, MI-Face also uses backpropagating. But the difference is MI-Face focuses on optimizing *x* rather than the parameters θ in normal training. The backpropagation process demands white-box access to the target model, and the reconstructed sample always converges to the most confident *x* near the initial point. Because *x* is highdimensional, the reconstruction quality highly depends on the initialization.\n\nDeepDream [\\[1\\]](#page-14-5): DeepDream was originally proposed to interpret machine learning models. However, this approach can also be utilized to improve MI-Face to acquire better results. The key idea is introducing regularization terms that force reconstructed samples to share similar statistics to natural images by penalizing each sample's total variance and ℓ<sup>2</sup> norm:\n\n$$\n\\min_{x} L(f_{\\theta}(x), y) + \\alpha_{\\text{tv}} \\mathcal{R}_{\\text{tv}}(x) + \\alpha_{\\ell_2} \\mathcal{R}_{\\ell_2}(x)\n$$\n\nwhere *R*tv and *R*<sup>ℓ</sup><sup>2</sup> denote total variance and ℓ<sup>2</sup> norm, respectively.\n\nDeepInversion [\\[73\\]](#page-17-6): DeepInversion further improves Deep-Dream by adding another loss. Its intuition is that the batch normalization layers encoded statistical information about the training samples. Thus, minimizing the distance between reconstructed statistics and those stored in the target model helps.\n\n<span id=\"page-2-0\"></span>Revealer [\\[78\\]](#page-17-4): Contrary to exploiting information encoded in the target model, Revealer leverages an auxiliary dataset to train a Generative Adversarial Network (GAN) *G* that generates samples *x*. Now instead of optimizing *x*, Revealer optimizes the input random seed *z* to *G* (and the reconstructed sample would be *x* = *G*(*z*)):\n\n$$\n\\min_{z} \\mathcal{L}(f_{\\theta}(G(z)), y)\n$$\n\nThe intuition is to utilize GAN to force the output *G*(*z*) to always look 'real' on any optimized *z*, at the same time, guarantee high confidence in the prediction.\n\nKEDMI [\\[14\\]](#page-14-7): KEDMI enhances the GAN-based approach in two primary ways. First, it optimizes the process of extracting knowledge from the auxiliary dataset by modifying the GAN's training objective. Specifically, it leverages labels assigned by the target model to the auxiliary dataset. The discriminator is trained to not only distinguish between real and fake samples but also to differentiate among the labels, enabling more nuanced learning. Second, instead of focusing on reconstructing single data points, KEDMI targets the reconstruction of the entire data distribution. To achieve this, it explicitly parameterizes the training data distribution and approximates the reconstructed distribution using its distributional parameters, such as the mean (*µ*) and standard deviation (σ).\n\nPLGMI [\\[74\\]](#page-17-7): PLGMI decouples the search space by training a conditional GAN (cGAN). It utilizes pseudo-labels generated by a top-n selection strategy to steer the training process, combined with a max-margin loss, which collectively improves the effectiveness of the attack.\n\nDeep-Leakage [\\[79\\]](#page-17-5): Deep-Leakage allows the adversary to access gradients (originally designed for federated learning systems) and aims to reconstruct training samples corresponding to the gradients. Specifically, the adversary randomly creates \"dummy input\" and \"dummy label\" and computes gradients based on this input-label pair. By optimizing this input-label pair to approximate true gradients, the \"dummy input\" and \"dummy label\" converges to target samples.\n\n## 2.2.2 Training-based Attack\n\nInv-Alignment [\\[71\\]](#page-17-3): To overcome the limitation that data reconstruction requires white-box access to the target model, Yang et al. opt for a training-based approach that works with black-box access. Briefly, they construct an autoencoder with the target model as the encoder part. Once the autoencoder is well-trained, the decoder part can be leveraged to reconstruct inputs given corresponding posteriors.\n\nUpdates-Leak [\\[47\\]](#page-16-3): Updates-Leak considers the onlinelearning scenario where the adversary has access to different versions of the target model and tries to reconstruct samples used to update the model. The adversary trains numerous shadow models to mimic the updating procedure and leverage the posterior difference to reconstruct target samples.\n\n## 2.2.3 Analysis-based Attack\n\nBias-Rec [\\[23\\]](#page-14-6): Haim et al. theoretically prove that the training data can be fully recovered given certain assumptions. In detail, if the target model is a homogeneous ReLU network and trained on a binary dataset using gradient flow, then *the parameters are linear combinations of the derivatives of the network at the training data points* [\\[23\\]](#page-14-6). According to this assertion, the adversary can derive training samples via optimization.\n\nIt is worth mentioning that the recent attack proposed by Balle et al. [\\[6\\]](#page-14-8) has shown promising results in reconstructing the missing sample in a dataset. However, their attack assumes the adversary has the whole dataset except for the reconstructed one (in order to verify differential privacy properties), whose attack scenario significantly differs from common settings. Additionally, certain attacks [\\[8,](#page-14-9) [31,](#page-15-9) [57\\]](#page-16-6) rely on invertible network architectures to execute their malicious actions. However, as these attack scenarios do not align with the scope of our investigation, we do not consider their attacks in this paper.\n\n## 2.3 Data Reconstruction vs. Membership Inference\n\nMembership inference attack (MIA) is another representative attack that aims to expose information about a training dataset, specifically by determining whether a target sample is part of it. There are two primary differences between data reconstruction and MIA, which necessitate the use of distinct attack methods. First, MIA takes a sample as input to decide its membership status, whereas data reconstruction only has a target model. This difference impels data reconstruction to employ an incompatible attack approach, as the state-of-the-art MIA regularly involves training samples in the attack process. For example, Carlini et al. [\\[9\\]](#page-14-4) train two sets of shadow models with datasets with/without the candidate sample. Such a training discrepancy is a crucial factor in enabling successful attacks.\n\nFrom a different view, MIA can be framed as a decision problem while data reconstruction is a search problem. This distinction implies that data reconstruction is a more challenging and sophisticated attack. In an ideal scenario, perfect data reconstruction would reveal all membership statuses, as the entire training dataset would be exposed. MIA could serve as the foundation for data reconstruction, given the assumption that the adversary has a dataset that includes all training samples. However, in reality, this assumption is not feasible, and the adversary must try all possible pixel combinations iteratively, which is impractical due to the enormity of the search space. Additionally, no existing MIA model can provide perfect accuracy in determining membership status, necessitating the development of alternative approaches for conducting data reconstruction attacks.\n\n## 2.4 Memorization\n\nIn the realm of attacks targeting the disclosure of information from training datasets, memorization serves as a key concept closely linked to attack performance. Within machine learn<span id=\"page-3-1\"></span>ing, memorization refers to a model's unintentional retention of intricate details from its training data, which is particularly evident in high-capacity models. Feldman [\\[19\\]](#page-14-10) provides a succinct definition of memorization for a target sample (*x<sup>i</sup>* , *yi*) with index *i*, describing it as the impact of removing a data point on the model's prediction for that particular point:\n\n<span id=\"page-3-0\"></span>\n$$\nmem(\\mathcal{A}, \\mathcal{D}, i) = \\Pr_{f_{\\theta} \\sim \\mathcal{A}(\\mathcal{D})} [f_{\\theta}(x_i) = y_i] - \\Pr_{f_{\\theta} \\sim \\mathcal{A}(\\mathcal{D}^{\\backslash i})} [f_{\\theta}(x_i) = y_i]\n$$\n(1)\n\nwhere *D*\\*<sup>i</sup>* denotes the dataset with the sample (*x<sup>i</sup>* , *yi*) removed.\n\nWhile memorization can be beneficial, and even indispensable [\\[19\\]](#page-14-10), for achieving high model performance, it simultaneously poses risks that adversaries can exploit to expose sensitive information.\n\nTramèr et al. [\\[61\\]](#page-16-7) demonstrates that enhancing a model's memorization through data poisoning attacks significantly increases the effectiveness of various privacy attacks, including membership inference, attribute inference, and data extraction. Further investigations by Carlini et al. [\\[10\\]](#page-14-11) illustrate the \"privacy onion effect\", highlighting that vulnerabilities arising from memorization cannot be easily mitigated by merely removing outliers.\n\nConversely, reducing a model's memorization of the training data can mitigate its vulnerability to privacy attacks, as seen in approaches like differential privacy [\\[4,](#page-14-12) [18,](#page-14-13) [46\\]](#page-15-10). However, this often comes at the expense of model performance. Despite its importance, the relationship between memorization and data reconstruction remains surprisingly underexplored. We aim to address this gap by benchmarking existing data reconstruction attacks from the perspective of memorization.\n\n## 3 Defining Data Reconstruction\n\n## 3.1 Reconstruction Taxonomy\n\nTo establish a rigorous definition of data reconstruction attacks, it is necessary to take into account various aspects such as training type, model access, and dataset access. To this end, we present a taxonomy that captures these aspects and use it to formulate a formal definition of data reconstruction. We are motivated by two key questions that arise in this context.\n\n1) What data does the adversary aim to reconstruct? The diversity of attack scenarios poses challenges to developing a unified definition of data reconstruction attacks. Some attacks focus on the standard setting where the model is trained on one fixed dataset, while others focus on settings that entail dataset change during the training process, e.g., online learning. Additionally, some attacks only consider data that contributes to certain updates. This diversity makes it hard to incorporate all possibilities into a unified definition.\n\n2) What information does the adversary have? This problem also stems from the diverse attack scenarios where the adversary has varying levels of information. For example, when users release their models, the adversary has white-box access to the model, while if models only provide a query interface, the adversary may lack detailed information about the model's architecture and parameters. Furthermore, the adversary may or may not have access to the same distribution of the target dataset. Thus, it is crucial to consider all possible situations to provide a comprehensive definition of data reconstruction attacks.\n\nIn the following, we categorize reconstruction attacks from three dimensions: training type, model access, and dataset access:\n\nTraining Type: We categorize training into two types: *static* and *dynamic*, based on the target dataset's role. In static, the target dataset remains fixed during training, and the attack aims to recover it. In dynamic, the target dataset is introduced during training, causing model changes, as the updating dataset in online learning.\n\nModel Access: The attacker may have one of the following access to the target model: *Black-box Access*, which means the attacker could only query the model in an API manner. *White-box Access*, which means the attacker could get full information about the target model, including the model architecture, parameters, and even gradients of the target sample calculated on the target model. We also acknowledge that real-world scenarios may involve intermediate access, representing a hybrid of black-box and white-box approaches. For example, an encoder might only be queryable via an API (black-box), while subsequent classification layers are directly accessible (white-box). Our framework's analysis of these two extremes, pure black-box and full white-box, effectively bounds the attack performance for all such intermediate cases.\n\nDataset Access: The attacker may have one of the following types of knowledge about the target dataset: *No Data*, meaning no access to any dataset information; *Same Distribution*, meaning the attacker can sample data from the same distribution as the target dataset; *Similar Distribution*, meaning the attacker can sample from distributions similar to the target. The key difference between \"same\" and \"similar\" distributions lies in the level of specificity of the distribution information that the adversary possesses: \"same\" refers to detailed information, e.g., images of a specific person, while \"similar\" refers to more general knowledge, e.g., knowing the dataset contains human faces.\n\nIn the following, we denote such information as extra knowledge (*K* ), defined as below:\n\nDefinition 3.1 (Extra Knowledge *K* ). Extra knowledge *K* provides the necessary information required for the reconstruction attack. A standard extra knowledge should contain information from three dimensions:\n\n- 1. Training Type: static or dynamic\n- 2. Model Access: black-box access or white-box access\n- 3. Dataset Access: no data or similar distribution or same distribution\n\nIt should be emphasized that while our present investigation concerns the realm of vision, the attack taxonomy we <span id=\"page-4-2\"></span><span id=\"page-4-1\"></span>Table 1: We group ten reconstruction attacks from three dimensions, which indicate the necessary information for the attack, including training type, model access, and dataset access. Note that attacks requiring more information about the model or dataset can be extended from attacks that require less information.\n\n| Training Type | Model Access |                                                   | Dataset Access             |                            |\n|---------------|--------------|---------------------------------------------------|----------------------------|----------------------------|\n|               |              | No Data                                           | Similar Distribution       | Same Distribution          |\n|               | Black-Box    |                                                   | Inv-Alignment              | Inv-Alignment              |\n| Static        | White-Box    | MI-Face<br>DeepDream<br>DeepInversion<br>Bias-Rec | Revealer<br>KEDMI<br>PLGMI | Revealer<br>KEDMI<br>PLGMI |\n| Dynamic       | Black-Box    |                                                   |                            | Updates-Leak               |\n|               | White-Box    | Deep-Leakage                                      |                            |                            |\n\nhave formulated has broader applicability to reconstruction attacks in diverse domains.\n\nWe categorize the ten attacks in [Table 1.](#page-4-1) Current reconstruction attacks most focus on the scenario where the adversary has white-box access to the target model, as this access provides crucial information that aids in dataset reconstruction. For attacks without model information, they tend to rely on information from a dataset that shares similar characteristics with the target dataset. For example, Inv-Alignment and Updates-Leak do not require white-box access but require data from the same distribution as a substitute. Furthermore, attacks with model information can be further improved with the help of dataset information. For instance, PLGMI leverages information from both the model and the dataset, resulting in improved performance compared to attacks that rely solely on model or dataset information, as demonstrated in the evaluation part.\n\nTheoretically, attacks with black-box access can be easily extended to white-box attacks, while the inverse transition is not straightforward. One potential solution to transfer whitebox attacks to black-box attacks is through the use of model stealing to extract their internal parameters. Additionally, for attacks that leverage information about the same distribution, we also investigate the feasibility of using similar distribution. These aspects are examined in [Section 7.](#page-11-0)\n\n## <span id=\"page-4-3\"></span>3.2 Reconstruction Definition\n\nGiven the extra information, another remaining question is the number of samples to reconstruct. Existing attacks generate a surplus of samples and designate those that best align with the target dataset as the reconstruction outcome. However, this approach is deemed unsuitable as it imparts specific information about the target dataset that is unavailable to the adversary. For the same reason, permitting the adversary to generate an infinite number of samples is inappropriate as generating every conceivable pixel combination can subsume the target dataset, yet such a reconstruction does not furnish any useful information.\n\nTherefore, we explicitly stipulate that the reconstructed dataset has the same size as the target model. Generating a greater number of samples than the target samples and selecting high-quality reconstructed samples is allowed. But it is crucial that the selection procedure does not involve any information about the target dataset.\n\nIt should be emphasized that the reconstruction size is a requisite for evaluation and not for the adversary. Furthermore, our definition encompasses scenarios where the focus is on reconstructing a subset of the training dataset. A detailed description of this is presented in [Section 4.3.](#page-5-0)\n\nProvided the reconstruction size, the target model, and the necessary information indicated in extra knowledge, we formally define the reconstruction algorithm as follows:\n\nDefinition 3.2 (Reconstruction Algorithm). Given a target model *m* ∈ *M* and extra knowledge *k* ∈ *K* , reconstruction algorithm *A* could reconstruct a dataset *Drec* = *A k* (*m*) ∈ D, i.e., *A*: *M* × *K* → D, with the same size as the target dataset *Dtar*.\n\nIn the static setting, the target dataset *Dtar* is the whole training dataset of the target model; in the dynamic setting, the target dataset refers to the subset of data that directly contributes to the model change.\n\n## 4 Evaluation Metric for Data Reconstruction\n\nA robust evaluation metric is crucial for the development of data reconstruction attacks, analogous to the role of loss functions in guiding model optimization. This section reviews existing metrics, outlines the desired properties of ideal metrics, and introduces our proposed metrics in [Sec](#page-5-0)[tion 4.3.](#page-5-0)\n\n## <span id=\"page-4-0\"></span>4.1 Existing Common Metrics\n\nVisualization: Visualization is a useful tool for understanding the quality of reconstructions and has been widely used in previous work [\\[1,](#page-14-5) [20,](#page-14-2) [47,](#page-16-3) [58,](#page-16-5) [71,](#page-17-3) [73,](#page-17-6) [75,](#page-17-8) [78,](#page-17-4) [79\\]](#page-17-5). While visualization provides the most direct and intuitive impression of reconstruction quality, its non-quantitative nature and reliance on subjective human judgment limit its effectiveness as an evaluation metric. Consequently, it is crucial to also utilize quantitative metrics in order to accurately assess reconstruction quality.\n\nMSE/PSNR/SSIM: Mean Squared Error (MSE) and other similarity measures, such as Peak Signal-to-Noise Ratio <span id=\"page-5-2\"></span>(PSNR) and Structural Similarity Index (SSIM), are commonly utilized to provide quantitative evaluation results [\\[47,](#page-16-3) [71,](#page-17-3) [78,](#page-17-4) [79\\]](#page-17-5). However, such metrics only measure the similarity between individual samples rather than the overall dataset, which poses two issues. First, the choice of sample pairs for comparison is not fixed, causing evaluation results to vary based on the selected pairs. Second, samplelevel metrics can't capture the diversity of the reconstructed dataset. For example, if all reconstructed samples are similar to *one* target sample, the measured distance may be small, but the reconstruction may still be unsatisfactory.\n\nFeature Distance: Feature distance measures the similarity in the feature space and has been used in previous works [\\[14,](#page-14-7) [78\\]](#page-17-4). Concretely, for each reconstructed sample, the distance to the centroid of its class in the feature space is calculated. However, as the feature space is determined by an evaluation network, the evaluation results can be inconsistent because different evaluation networks use different feature spaces with varying class centroids. Additionally, like other sample-level metrics such as MSE, this measure doesn't capture the diversity of the reconstruction.\n\nAccuracy (Train): To incorporate the macro similarity of the reconstructed dataset, one method uses reconstructed samples to train a model for the same task [\\[73,](#page-17-6) [75,](#page-17-8) [78\\]](#page-17-4). The model's testing accuracy reflects the reconstruction quality, with higher accuracy indicating better reconstruction. We point out that this metric overlooks the precision of the reconstruction, as demonstrated by a counterexample in [Fig](#page-18-0)[ure 7a.](#page-18-0) Concretely, we utilize the data-free model extraction method [\\[63\\]](#page-16-8) to generate a dataset, which is then used to steal the target model. Although the resulting stolen model exhibits high accuracy, the generated dataset, which serves as the training dataset of the stolen model, is vastly dissimilar from the target dataset.\n\nAccuracy (Test): Instead of training a model on the reconstructed dataset, one can train an evaluation model on a dataset from the same distribution as the target model and assess whether the evaluation model can accurately classify each reconstructed sample [\\[14,](#page-14-7) [58,](#page-16-5) [64,](#page-16-4) [75,](#page-17-8) [78\\]](#page-17-4). The idea is that high-quality reconstructions should contain recognizable patterns that the evaluation model can capture. However, we argue that this metric is ineffective, as shown by examples that resemble random noise but are still classified with high confidence by the evaluation model, as illustrated in [Figure 7b.](#page-18-0) These counterexamples, generated using MI-Face [\\[20\\]](#page-14-2), achieve prediction confidences above 0.9 for their corresponding classes.\n\n## 4.2 Design Desiderata\n\nTo propose metrics that can reflect the quality of a reconstruction, we need to consider four questions: 1) can such metrics *quantitatively* measure the quality? 2) can such metrics provide a *consistent* result for a fixed reconstruction? 3) can such metrics embody the quality in a *micro* aspect? 4) can such metrics incorporate the quality in a *macro* aspect?\n\nIn response to the drawbacks of existing metrics, we propose a set of properties that suitable evaluation metrics should possess, including quantifiability, consistency, precision, and diversity:\n\n- Quantifiability. The evaluation metric should provide quantitative results and eliminate the influence of subjective factors.\n- Consistency. The evaluation metric should be consistent, that is, given a pair of the reconstructed dataset and the target dataset, the evaluated result should be determined, regardless of the testing time, place, and order.\n- Precision. The evaluation metric should be aware of the reconstruction precision. Specifically, it should capture the sample-level similarity of reconstructed samples to target samples.\n- Diversity. The evaluation metric should be aware of the reconstruction diversity. Concretely, the metric should reflect the percentage of data being reconstructed. An attack that can only recover a few samples accurately is not regarded as a successful reconstruction attack.\n\n## <span id=\"page-5-0\"></span>4.3 Definition of Our Quantitative Metrics\n\nTo capture both precision and diversity aspects, we introduce two metrics from different perspectives. The first is a datasetlevel metric that measures the distribution similarity between the reconstructed and target datasets.\n\nDefinition 4.1 (Dataset-level Metric). The dataset-level metric *µ* : D × D → R is defined as a mapping that takes two datasets and produces a single real number D-Dis, i.e., D-Dis = *µ*(*A k* (*m*),*Dtar*).\n\nWe leverage Fréchet Inception Distance (FID) to incorporate the macro quality of the reconstructed dataset, which measures the distance between different data distributions. Specifically, we approximate the two dataset distributions by Gaussian distributions as follows:\n\n$$\n\\mathcal{D}_{tar} \\sim \\mathcal{N}(v_{tar}, \\Sigma_{ori}),\n$$\n and  $\\mathcal{A}^k(m) \\sim \\mathcal{N}(v_{rec}, \\Sigma_{rec})$ \n\nWe compute the D-Dis value as:\n\n$$\n||\\mathbf{v}_{tar} - \\mathbf{v}_{rec}||_2^2 + tr\\left(\\Sigma_{tar} + \\Sigma_{rec} - 2(\\Sigma_{tar}^{\\frac{1}{2}} \\cdot \\Sigma_{rec} \\cdot \\Sigma_{tar}^{\\frac{1}{2}})^{\\frac{1}{2}}\\right)\n$$\n\nwhere ||·||<sup>2</sup> denotes the Euclidean distance, and *tr*(·) denotes the trace of a matrix. ν and variance Σ denote the parameters of the corresponding Gaussian distributions.\n\nWe choose Fréchet Inception Distance (FID) to measure the dataset-level distance due to its established use as a metric for evaluating the quality of generated distributions. Furthermore, FID is model-agnostic and can be calculated using any feature extractor, but common practice involves using the pre-trained InceptionV3 regardless of the dataset[1](#page-5-1) . Using FID to measure dataset-level distance also resolves concerns related to consistency, as feature distances are calculated using the same model across different tasks. We further introduce sample-level metrics to improve our evaluation by taking precision into account. Specifically, we concentrate on\n\n<span id=\"page-5-1\"></span><sup>1</sup>https://github.com/mseitzer/pytorch-fid\n\n<span id=\"page-6-2\"></span>how similar the reconstructed sample is to the target sample in the original data space.\n\n<span id=\"page-6-3\"></span>Definition 4.2 (Sample-level Metric). The sample-level metric *µ* : D × D → R <sup>2</sup> maps two datasets into two real numbers S-Dis and α, i.e., (S-Dis,α) = *µ*(*A k* (*m*),*Dtar*), where the first value S-Dis calculates the averaged minimal distance between reconstructed dataset and the target dataset, and the second value α denotes the coverage of reconstructed part. Formally:\n\n$$\n\\text{S-Dis} = \\frac{1}{|\\mathcal{A}^k(m)|} \\sum_{x_i \\in \\mathcal{A}^k(m)} d(x_i, f(x_i))\n$$\n\nwhere *f* : *A k* (*m*) → *Dtar*, such that, ∀*x<sup>i</sup>* ∈ *A k* (*m*), *f* maps *x<sup>i</sup>* to the sample *f*(*xi*) ∈ *Dtar* with the minimal distance to *x<sup>i</sup>* .\n\nFor coverage α, it indicates the reconstructed diversity:\n\n$$\n\\alpha = \\frac{|f(\\mathcal{A}^k(m))|}{|\\mathcal{D}_{tar}|}\n$$\n\nHere, *f*(*A k* (*m*)) denotes the image of *f* , and α ∈ (0,1], larger coverage indicates better diversity.\n\nNote that the choice of *d* may vary depending on the aspect of reconstruction quality to be assessed. Both metrics proposed above provide deterministic evaluation results, only based on the content of the reconstructed dataset, which satisfies the property of quantifiability and consistency. Furthermore, the coverage reflects the diversity of the reconstruction. The precision of the metrics is partially illustrated in [Section 5.1,](#page-6-0) and we discuss it in more detail in [Section 6.1.](#page-9-0)\n\nWe can see that the reconstruction of a subset is encompassed by our reconstruction definition. To clarify, we can replicate such samples, and the outcome will remain unaffected as the corresponding image in the target dataset is fixed, resulting in an unchanged coverage and distance. In the event that accurately reconstructing a subset, it is conceivable that it will be characterized by a small distance, yet it may have limited coverage. A small distance indicates an accurate reconstruction, while the small coverage indicates that the reconstructed samples constitute only a minor fraction.\n\nProvided the evaluation metric, we can parameterize the attack to characterize the ability of data reconstruction attack. We first formulate the ideal case where the attack exactly reconstructs the target dataset under the measurement of *µ*.\n\nDefinition 4.3 (*µ*-Exact Reconstruction). We say a reconstruction algorithm *A* achieves *µ*-exact reconstruction if *µ*(*A k* (*m*),*Dtar*) = 0.\n\n*µ* denotes the distance measure that specifies the quality of the reconstruction, where the output could be a tuple if the measure has more than one evaluation metric. Note that *µ*-Exact Reconstruction is a necessary and insufficient condition for the perfect reconstruction, depending on the choice of measure *µ*, which further highlights the importance of selecting appropriate metrics to evaluate the reconstruction performance.\n\nThe relaxed version allows the reconstructed dataset to have a small distance ε with the target dataset, we refer to this version as (ε,*µ*)-Approximate Reconstruction:\n\nDefinition 4.4 ((ε,*µ*)-Approximate Reconstruction). We say a reconstruction algorithm *A* achieves (ε,*µ*)-approximate reconstruction if *µ*(*A k* (*m*),*Dtar*) ≤ ε.\n\n*Remark.* (ε,*µ*)-Approximate Reconstruction corresponds to the specific case of *µ*-Exact Reconstruction when ε = 0.\n\nOur framework's core metrics effectively address a broad spectrum of attack goals. For instance, reconstructing a single missing sample [\\[48\\]](#page-16-9) translates within our definition to a low coverage score, indicating its narrow scope, combined with a reconstruction-distance metric to assess fidelity. In contrast, an attack targeting an entire class aims for high coverage [\\[20\\]](#page-14-2), signifying the retrieval of numerous distinct dataset items. Overall, the two metrics work together: distance measures reconstruction accuracy, while coverage scales with the breadth of the attacker's objective, from single-sample inference to full-dataset recovery.\n\n## 5 Evaluation\n\nDue to space constraints, we defer the description of the experimental setup to [Appendix C,](#page-17-9) where we provide details on the dataset, attack configurations, and evaluation metrics.\n\n## <span id=\"page-6-0\"></span>5.1 Results under Quantitative Metrics\n\nWe divide ten reconstruction attacks into two groups based on the training type. To provide an overview of the attack performance, we visualize the reconstruction results of all ten attacks in [Figure 1.](#page-7-0) Overall, none of the attacks can achieve exact reconstruction, and their quality varies significantly. To better analyze and compare these attacks, we apply the proposed metrics to evaluate the reconstruction datasets, and report the results in [Table 2.](#page-8-0)\n\nDataset-level Metric: For the CelebA dataset with the largest size, we visually confirm that three GAN-based attacks (Revealer, KEDMI, and PLGMI) outperform other methods, which is consistent with their lower FID scores. For dynamic training-type attacks, Updates-Leak shows a lower FID score than Deep-Leakage, indicating better reconstruction quality. This pattern holds for the other two datasets. We also observe that DeepInversion's performance worsens as dataset complexity[2](#page-6-1) increases, with FID scores rising from 64.678 (MNIST) to 131.545 (CIFAR10) to 234.672 (CelebA). In contrast, GAN-based attacks, such as PLGMI, maintain clear semantic reconstruction, reflected in their low FID scores: 82.940 (MNIST), 123.018 (CIFAR10), and 85.143 (CelebA).\n\nIn general, the FID score reflects reconstruction quality, with lower values indicating better reconstructions. However, we argue that relying solely on FID may not fully capture reconstruction quality, as shown with the CelebA dataset. As seen in [Figure 1,](#page-7-0) Inv-Alignment generates reconstructions that clearly show the semantic meaning of the\n\n<span id=\"page-6-1\"></span><sup>2</sup>We follow the criteria used in [\\[39\\]](#page-15-8) to determine the complexity of the dataset. Briefly, gray-scale datasets are simpler than colored datasets.\n\n<span id=\"page-7-0\"></span>![](_page_7_Figure_0.jpeg)\n\nFigure 1: Visualization of existing reconstruction attacks. For each attack, the left two images are reconstructed from the target model with a smaller training size (1,000 for CelebA and 100 for CIFAR10 and MNIST), and the right two images are from the larger one (20,000).\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_2.jpeg)\n\nFigure 2: Relationship between sample-level metrics and the dataset-level metric, we plot the correlation heatmap using the absolute value of correlation between different metrics.\n\ntarget dataset, i.e., human faces, whereas some DeepInversion reconstructions lack sufficient information, despite having a lower FID score (234.672) compared to Inv-Alignment (357.610). This highlights the need to incorporate samplelevel metrics like SSIM, PSNR, and MSE to more comprehensively evaluate reconstruction quality.\n\nSample-level Metric: Sample-level metrics complement dataset-level metrics by providing a micro-scale evaluation of reconstructions, which, in certain contexts, may better align with human perception. As previously noted, Inv-Alignment surpasses DeepInversion on the CelebA dataset; however, this superiority is not reflected by the FID score. In contrast, all three sample-level metrics deliver results consistent with our expectations, as shown in [Figure 1.](#page-7-0) Additionally, sample-level metrics and dataset-level metrics generally provide consistent outcomes, as demonstrated in [Fig](#page-7-1)[ure 2.](#page-7-1) Specifically, attacks achieving higher performance at the dataset level also tend to perform well on sample-level metrics. For instance, [Figure 2a](#page-7-1) shows that reconstructions with higher FID scores typically exhibit lower SSIM values, with similar trends observed across other metrics.\n\nSample-level metrics also play a vital role in assessing the diversity of reconstructed samples (coverage), a key indicator of a reconstruction attack's success. Diversity is challenging to evaluate using dataset-level metrics, as these rely on a few representative samples to approximate the distribution. Diversity is crucial for determining reconstruction quality, as generating identical data similar to a few samples in the training dataset might achieve high performance but does not constitute an effective reconstruction attack, as it limits the exposed information to only a few data points. To measure coverage, we identify the nearest pair for each reconstructed sample in the target dataset within the same class. The proportion of such pairs in the target dataset indicates the reconstruction's diversity. Given that coverage is influenced by both the number of classes and reconstructed samples, we present the results for CIFAR10 and MNIST together in [Figure 11,](#page-20-0) and separately for CelebA. The findings reveal an intriguing observation: high-quality reconstructions do not necessarily correspond to greater diversity in the reconstructed samples. Furthermore, results in [Table 2](#page-8-0) show a decline in coverage as the size of the training dataset increases. This trend aligns with our hypothesis that current reconstruction attacks tend to focus on capturing general information about the training dataset rather than individual sample details. Consequently, the ability to reconstruct diverse, unique samples diminishes as the training dataset grows, leading to lower coverage metrics for larger datasets.\n\nSummary: While there is a relationship between datasetlevel and sample-level metrics, their correlation is relatively weak, especially when evaluating low-quality reconstructions, as illustrated in [Figure 2d.](#page-7-1) For example, both MI-Face and DeepDream perform poorly on the CelebA task, which is reflected in their FID scores (336.906 vs. 370.396). How-\n\n<span id=\"page-8-0\"></span>Table 2: Evaluation results of existing reconstruction attacks. The target model is VGG16 trained on CelebA with 6 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance. Experimental results for other model architectures and datasets can be found from [Table 17](#page-27-0) to [Table 24](#page-34-0) in the appendix.\n\n| Attack        |               | Metrics |                 |                |                | Target Data Size |                |                |\n|---------------|---------------|---------|-----------------|----------------|----------------|------------------|----------------|----------------|\n|               |               |         | 1,000           | 2,000          | 5,000          | 10,000           | 15,000         | 20,000         |\n|               | Memorization  |         | 1.000           | 0.981          | 0.862          | 0.539            | 0.386          | 0.301          |\n|               | Dataset-level | FID ↓   | 362.361         | 365.570        | 361.517        | 340.959          | 334.520        | 336.906        |\n| MI-Face       |               | SSIM ↑  | 0.014(100.00%)  | 0.019(72.15%)  | 0.028(57.88%)  | 0.024(57.75%)    | 0.024(57.28%)  | 0.023(54.80%)  |\n|               | Sample-level  | PSNR ↑  | 7.978(100.00%)  | 8.453(53.80%)  | 8.872(30.74%)  | 8.851(17.63%)    | 9.005(13.61%)  | 9.037(10.08%)  |\n|               |               | MSE ↓   | 0.163(100.00%)  | 0.145(53.80%)  | 0.132(30.74%)  | 0.131(17.63%)    | 0.127(13.61%)  | 0.126(10.08%)  |\n|               | Dataset-level | FID ↓   | 337.139         | 306.974        | 297.590        | 315.676          | 345.472        | 370.396        |\n| DeepDream     |               | SSIM ↑  | 0.079(100.00%)  | 0.140(57.65%)  | 0.234(26.44%)  | 0.290(14.38%)    | 0.321(8.95%)   | 0.346(6.81%)   |\n|               | Sample-level  | PSNR ↑  | 9.162(100.00%)  | 10.304(53.70%) | 11.957(26.60%) | 13.006(14.43%)   | 13.980(9.29%)  | 14.027(7.63%)  |\n|               |               | MSE ↓   | 0.128(100.00%)  | 0.100(53.70%)  | 0.069(26.60%)  | 0.053(14.43%)    | 0.042(9.29%)   | 0.041(7.63%)   |\n|               | Dataset-level | FID ↓   | 287.497         | 283.429        | 273.183        | 273.415          | 245.736        | 234.672        |\n| DeepInversion |               | SSIM ↑  | 0.100(100.00%)  | 0.110(61.70%)  | 0.118(42.44%)  | 0.140(28.97%)    | 0.150(27.07%)  | 0.153(22.47%)  |\n|               | Sample-level  | PSNR ↑  | 9.676(100.00%)  | 10.094(56.10%) | 10.550(31.68%) | 11.143(21.07%)   | 11.388(19.09%) | 11.343(15.73%) |\n|               |               | MSE ↓   | 0.119(100.00%)  | 0.105(56.10%)  | 0.094(31.68%)  | 0.081(21.07%)    | 0.076(19.09%)  | 0.077(15.73%)  |\n|               | Dataset-level | FID ↓   | 116.712         | 103.428        | 94.899         | 93.961           | 93.781         | 92.982         |\n| Revealer      |               | SSIM ↑  | 0.101(100.00%)  | 0.116(66.75%)  | 0.135(50.50%)  | 0.150(44.05%)    | 0.157(40.47%)  | 0.162(38.33%)  |\n|               | Sample-level  | PSNR ↑  | 9.144(100.00%)  | 9.720(60.90%)  | 10.087(43.68%) | 10.449(37.20%)   | 10.622(33.66%) | 10.733(32.17%) |\n|               |               | MSE ↓   | 0.123(100.00%)  | 0.113(60.90%)  | 0.103(43.68%)  | 0.094(37.20%)    | 0.091(33.66%)  | 0.088(32.17%)  |\n|               | Dataset-level | FID ↓   | 344.049         | 243.849        | 359.419        | 229.609          | 361.569        | 357.910        |\n| Inv-Alignment |               | SSIM ↑  | 0.255(100.00%)  | 0.312(51.45%)  | 0.285(22.56%)  | 0.353(13.09%)    | 0.336(9.46%)   | 0.328(7.89%)   |\n|               | Sample-level  | PSNR ↑  | 11.292(100.00%) | 12.463(52.40%) | 13.023(23.10%) | 13.858(13.93%)   | 14.007(9.74%)  | 14.253(8.02%)  |\n|               |               | MSE ↓   | 0.081(100.00%)  | 0.061(52.40%)  | 0.052(23.10%)  | 0.043(13.93%)    | 0.041(9.74%)   | 0.039(8.02%)   |\n|               | Dataset-level | FID ↓   | 327.883         | 323.892        | 316.452        | 319.774          | 318.895        | 315.227        |\n| Bias-Rec      |               | SSIM ↑  | 0.039(38.20%)   | 0.040(35.55%)  | 0.043(30.88%)  | 0.044(27.50%)    | 0.045(25.44%)  | 0.047(24.28%)  |\n|               | Sample-level  | PSNR ↑  | 9.783(3.20%)    | 10.211(0.75%)  | 10.249(0.80%)  | 10.311(0.53%)    | 10.222(0.60%)  | 10.354(0.57%)  |\n|               |               | MSE ↓   | 0.105(3.20%)    | 0.096(0.75%)   | 0.095(0.80%)   | 0.093(0.53%)     | 0.095(0.60%)   | 0.093(0.57%)   |\n|               | Dataset-level | FID ↓   | 121.689         | 110.088        | 101.730        | 94.341           | 95.852         | 97.667         |\n| KEDMI         |               | SSIM ↑  | 0.111(100.00%)  | 0.124(57.30%)  | 0.144(32.60%)  | 0.157(24.29%)    | 0.160(21.10%)  | 0.167(18.93%)  |\n|               | Sample-level  | PSNR ↑  | 9.167(100.00%)  | 9.715(54.50%)  | 10.605(30.28%) | 11.126(20.60%)   | 11.428(17.12%) | 11.560(15.36%) |\n|               |               | MSE ↓   | 0.133(100.00%)  | 0.115(54.50%)  | 0.092(30.28%)  | 0.081(20.60%)    | 0.075(17.12%)  | 0.073(15.36%)  |\n|               | Dataset-level | FID ↓   | 127.722         | 107.883        | 104.842        | 97.730           | 84.836         | 85.143         |\n| PLGMI         |               | SSIM ↑  | 0.110(100.00%)  | 0.132(60.40%)  | 0.136(37.02%)  | 0.149(26.15%)    | 0.154(21.83%)  | 0.161(18.54%)  |\n|               | Sample-level  | PSNR ↑  | 9.448(100.00%)  | 10.164(56.10%) | 10.588(31.58%) | 10.921(20.55%)   | 11.299(16.44%) | 11.513(13.54%) |\n|               |               | MSE ↓   | 0.122(100.00%)  | 0.102(56.10%)  | 0.092(31.58%)  | 0.085(20.55%)    | 0.078(16.44%)  | 0.074(13.54%)  |\n|               | Dataset-level | FID ↓   | 192.446         | 259.231        | 263.899        | 275.845          | 312.011        | 260.100        |\n| Updates-Leak  |               | SSIM ↑  | 0.203(18.00%)   | 0.184(11.00%)  | 0.194(9.00%)   | 0.155(22.00%)    | 0.170(18.00%)  | 0.187(5.00%)   |\n|               | Sample-level  | PSNR ↑  | 13.173(14.00%)  | 13.165(4.00%)  | 12.693(6.00%)  | 12.554(14.00%)   | 12.537(10.00%) | 12.509(6.00%)  |\n|               |               | MSE ↓   | 0.049(14.00%)   | 0.050(4.00%)   | 0.056(6.00%)   | 0.058(14.00%)    | 0.058(10.00%)  | 0.058(6.00%)   |\n|               | Dataset-level | FID ↓   | 376.852         | 383.272        | 382.227        | 384.841          | 384.023        | 383.338        |\n| Deep-Leakage  |               | SSIM ↑  | 0.031(49.00%)   | 0.030(48.00%)  | 0.034(43.00%)  | 0.033(45.00%)    | 0.037(43.00%)  | 0.040(52.00%)  |\n|               | Sample-level  | PSNR ↑  | 10.957(3.00%)   | 10.965(2.00%)  | 11.014(4.00%)  | 11.076(2.00%)    | 11.152(2.00%)  | 11.199(2.00%)  |\n|               |               | MSE ↓   | 0.080(3.00%)    | 0.080(2.00%)   | 0.079(4.00%)   | 0.078(2.00%)     | 0.077(2.00%)   | 0.076(2.00%)   |\n\never, their SSIM scores vary significantly (0.023 vs. 0.346), highlighting the inconsistency between metrics.\n\nTo examine the relationship between metrics in the context of high-quality reconstructions, we focus on three GANbased attacks that generate higher-quality outputs. In these cases, the correlation coefficients improve: the correlation between FID and SSIM increases from 0.465 to 0.597, FID and PSNR from 0.117 to 0.721, and FID and MSE from 0.289 to 0.511.\n\nFurthermore, low-quality reconstructions introduce discrepancies in coverage, even within the same attack. For instance, in the case of Bias-Rec on CelebA, coverage varies significantly between SSIM (24.28%) and PSNR (0.57%). When low-quality reconstructions are excluded, coverage across sample-level metrics aligns more closely, with the correlation coefficient between SSIM and PSNR, as well as between SSIM and MSE, increasing dramatically from 0.429 to 0.993.\n\nDespite these improvements, the correlation between dataset-level and sample-level metrics remains insufficiently strong to ignore their distinctions. Dataset-level metrics provide a broad perspective on reconstruction quality, whereas sample-level metrics offer detailed insights into the fidelity of individual samples. Therefore, it is essential to utilize mul<span id=\"page-9-3\"></span>tiple metrics during evaluation. Although a single score can be derived using the minimum of normalized metrics to represent reconstruction quality, it is generally advisable to consider all available metrics. Ultimately, the decision between prioritizing usability and ensuring accuracy or effectiveness involves a careful trade-off.\n\n## 6 Memorization in Data Reconstruction\n\nWe observe that existing attacks show varying performance, and even the same attack may exploit different levels of vulnerabilities when target models are trained with datasets of varying sizes. From the perspective of model owners, understanding which models are more vulnerable to data reconstruction attacks is essential. In this section, we examine model vulnerability to data reconstruction attacks through the lens of memorization. This choice is motivated by the close connection between memorization and membership inference attacks. Previous work [\\[10,](#page-14-11)[61\\]](#page-16-7) suggests that strongly memorized samples are more vulnerable to membership inference. Given that data reconstruction can be framed as a search problem within the context of membership inference, we investigate whether models with higher memorization scores are likewise more prone to data reconstruction attacks.\n\nAs shown in [Table 2,](#page-8-0) we trained models using six datasets of varying sizes, which resulted in different levels of memorization for individual samples. To quantify memorization at the model level, we extend the sample-based memorization definition [\\(Equation 1\\)](#page-3-0) to cover the entire model. Specifically, we use the average memorization score of the first 1,000 samples in the training dataset as a proxy for the model's overall memorization score:\n\nmodel-mem\n$$\n(A, D) = \\mathop{\\mathbb{E}}_{x_i \\in D} \\left[ \\Pr_{f_{\\theta} \\sim A(D)} [f_{\\theta}(x_i) = y_i] - \\Pr_{f_{\\theta} \\sim A(D^{(i)})} [f_{\\theta}(x_i) = y_i] \\right]\n$$\n (2)\n\nAs expected, we observe that the model memorization score decreases from 1.000 to 0.301 as the training size increases from 1,000 to 20,000. However, the performance of different attacks is inconsistent. For instance, with the DeepDream attack, the FID distance to the training dataset is 337.139 when the memorization score is 1.000, and it decreases as the memorization score drops to 0.301. In contrast, for attacks such as Revealer, reconstruction quality (as measured by FID) improves as the memorization score decreases.\n\nWe attribute this discrepancy to two potential factors. First, the attack methods may lack the necessary capacity to accurately capture underlying vulnerabilities, particularly in the case of non-generative model-based attacks. This further raises the question of whether current methods are genuinely extracting private information from the model or merely imputing plausible samples, we provide some initial discussion in [Section 7.](#page-11-0) Second, the evaluation metrics used may not effectively capture the true performance of these attacks. As discussed in [Section 5.1,](#page-6-0) while existing metrics provide a rough estimate of reconstruction quality, there remains a gap between the numerical results and human perception. In\n\nTable 3: Evaluation with GPT-4o.\n\n<span id=\"page-9-1\"></span>\n\n| Attack      | Metrics                        |     |                   |     | Target Data Size                       |       |       |\n|-------------|--------------------------------|-----|-------------------|-----|----------------------------------------|-------|-------|\n|             |                                |     |                   |     | 1,000 2,000 5,000 10,000 15,000 20,000 |       |       |\n|             | Memorization 1.000 0.981 0.862 |     |                   |     | 0.539                                  | 0.386 | 0.301 |\n|             | # of Major                     | 349 | 182               | 188 | 133                                    | 94    | 54    |\n| Revealer    | # of All                       | 166 | 35                | 45  | 46                                     | 26    | 15    |\n|             | Pred Rate                      |     | 0.335 0.185 0.194 |     | 0.137                                  | 0.096 | 0.053 |\n|             | # of Major                     | 303 | 157               | 227 | 188                                    | 90    | 35    |\n| KEDMI       | # of All                       | 115 | 14                | 37  | 37                                     | 14    | 6     |\n|             | Pred Rate                      |     | 0.281 0.169 0.217 |     | 0.189                                  | 0.097 | 0.047 |\n|             | # of Major                     | 325 | 133               | 162 | 192                                    | 120   | 68    |\n| PLGMI       | # of All                       | 116 | 12                | 12  | 44                                     | 15    | 6     |\n|             | Pred Rate                      |     | 0.283 0.142 0.174 |     | 0.200                                  | 0.125 | 0.076 |\n|             | # of Major                     | 484 | 146               | 186 | 126                                    | 38    | 20    |\n| PLGMI (Pre) | # of All                       | 246 | 15                | 29  | 21                                     | 8     | 0     |\n|             | Pred Rate                      |     | 0.446 0.160 0.188 |     | 0.125                                  | 0.054 | 0.026 |\n\n<span id=\"page-9-2\"></span>Table 4: Evaluation with InternVL 2.5 and Claude 3.7 for PLGMI (pre) on VGG16 trained on CelebA.\n\n| LLMs         | Metrics                                               |            | Target Data Size |         |                                        |                    |                  |  |  |\n|--------------|-------------------------------------------------------|------------|------------------|---------|----------------------------------------|--------------------|------------------|--|--|\n|              |                                                       |            |                  |         | 1,000 2,000 5,000 10,000 15,000 20,000 |                    |                  |  |  |\n| InternVL 2.5 | # of Major<br># of All<br>Pred Rate 0.410 0.157 0.130 | 531<br>69  | 134<br>0         | 92<br>0 | 149<br>2<br>0.165                      | 65<br>0<br>0.096   | 29<br>0<br>0.042 |  |  |\n| Claude 3.7   | # of Major<br># of All<br>Pred Rate 0.434 0.179 0.056 | 479<br>143 | 188<br>31        | 41<br>0 | 125<br>23<br>0.134                     | 130<br>17<br>0.158 | 37<br>0<br>0.039 |  |  |\n\nsome cases, measurements do not align with visual evaluations. We discuss this challenge in the next section.\n\n## <span id=\"page-9-0\"></span>6.1 Linkage between Model Memorization and Dataset Leakage\n\nTo evaluate the effectiveness of data reconstruction, we have introduced a set of metrics designed to quantitatively assess reconstruction quality. While these metrics offer a coarsegrained view of reconstruction success, previous findings suggest that high quantitative scores do not necessarily correlate with high-quality reconstructions. Although human inspection remains the most direct and intuitive method for quality assessment, it is often prohibitive in terms of cost and scale for extensive datasets. Therefore, to achieve an efficient and scalable evaluation, we propose utilizing GPT-4o. This model is recognized for its robust performance and has been further refined during the Reinforcement Learning from Human Feedback (RLHF) phase to align with human preferences.\n\nWe evaluate the attack performance using 1,000 randomly chosen CelebA images as targets. Each target appears in six training sets of increasing size. For every set, we run the reconstruction algorithm and keep the 1,000 results with the highest PSNR score relative to their targets. Thus, each target image has six candidate reconstructions—one from each training size. To decide which reconstruction looks closest to the ground truth, we prompt GPT-4o: \"*From the second to the seventh image, which image is more similar to the first one? Please make sure your response must be the index of that image and don't say any other words.*\" We repeat this query five times and tally how often each training size is cho-\n\n<span id=\"page-10-3\"></span><span id=\"page-10-0\"></span>![](_page_10_Figure_0.jpeg)\n\nFigure 3: Visualization of PLGMI on different target models.\n\nsen. We report three summary measures: \"# of major\" (how often a setting wins the majority vote), \"# of all\" (how often the vote is unanimous), and \"pred rate\" (its overall selection frequency across all queries).\n\nOur analysis concentrates on three GAN-based approaches, which consistently yield high-quality reconstructions. As indicated in [Table 3,](#page-9-1) despite some variability, all three metrics across the 1,000 identities generally exhibit a trend where reconstruction quality diminishes as the memorization score decreases. These results can also be generalized to other leading LLMs, such as InternVL 2.5 and Claude 3.7, as demonstrated in [Table 4.](#page-9-2) Visual evidence in [Fig](#page-10-0)[ure 3](#page-10-0) further supports the observation that reconstructions from the smallest target dataset size (1,000 samples) resemble the target images more closely than those from the largest size (20,000 samples). This observation aligns with our expectations but contrasts with results from previous metrics.\n\nWe interpret these findings in two ways. From the perspective of GAN-based attacks, these methods reconstruct data at a class level and aim to closely approximate the distribution of the targets. Consequently, when a target dataset includes multiple samples within a class, achieving a close match for any specific target sample becomes unlikely. Conversely, smaller target datasets result in less generalized feature learning, manifesting as blurred areas in reconstructions, like the eyes and cheeks. While this blurriness negatively impacts performance as measured by both dataset-level and sample-level metrics, its effect on visual assessments is relatively modest, provided the blurring is not extensive.\n\nThis second interpretation brings forth another noteworthy observation. For each attack method, the optimal performance is achieved at a dataset size of 1000. Interestingly, the second-best performance does not occur at a slightly larger size, such as 2000, but frequently occurs at medium dataset sizes, such as 5,000 or 10,000 samples. This suggests that the reconstruction process benefits from the model's ability to learn additional features. However, this advantage is counterbalanced by the challenges posed by larger datasets, as a more extensive training dataset complicates the accurate recovery of individual samples, consistent with our earlier findings.\n\nTo further explore the benefits of feature learning, we trained a model pre-trained on disjoint datasets and finetuned it with the target data. This approach enables the model\n\nTable 5: PLGMI vs. PLGMI (Pre).\n\n<span id=\"page-10-1\"></span>\n\n| Metrics    | Target Data Size |          |         |         |         |         |  |  |  |  |\n|------------|------------------|----------|---------|---------|---------|---------|--|--|--|--|\n|            | 1,000            | 2,000    | 5,000   | 10,000  | 15,000  | 20,000  |  |  |  |  |\n| # of Major | 11 : 989         | 17 : 983 | 6 : 994 | 6 : 994 | 3 : 997 | 6 : 994 |  |  |  |  |\n| # of All   | 1 : 956          | 2 : 939  | 1 : 964 | 0 : 961 | 0 : 979 | 0 : 952 |  |  |  |  |\n|            | 0.015            | 0.023    | 0.012   | 0.013   | 0.006   | 0.014   |  |  |  |  |\n| Pred Rate  | 0.985            | 0.977    | 0.988   | 0.987   | 0.994   | 0.986   |  |  |  |  |\n\n<span id=\"page-10-2\"></span>Table 6: GPT-4o evaluation with different sizes of feature embeddings on CelebA with data size of 1,000 and 20,000.\n\n| Attack | Metrics    | Feature size (1000) |     |             | Feature size (20000) |     |             |  |\n|--------|------------|---------------------|-----|-------------|----------------------|-----|-------------|--|\n|        |            | 2,048 (Ori)         | 512 | 128         | 2,048 (Ori)          | 512 | 128         |  |\n|        | # of Major | 917                 | 75  | 8           | 742                  | 251 | 7           |  |\n| PLGMI  | # of All   | 719                 | 12  | 0           | 465                  | 39  | 0           |  |\n|        | Pred Rate  | 0.884               |     | 0.105 0.011 | 0.725                |     | 0.263 0.012 |  |\n\nto learn additional features, as evidenced by the improved testing accuracy. We evaluate the attack on this fine-tuned model in [Table 3.](#page-9-1) The results indicate that pre-training enhances the advantage of using a smaller dataset size, as the pre-trained model has already developed a degree of generalizability and is able to learn sample-specific features more efficiently.\n\nTo better understand the effect of pre-training, we compare attack performance between models trained from scratch and those fine-tuned from pre-trained weights, as presented in [Ta](#page-10-1)[ble 5.](#page-10-1) Fine-tuning a pre-trained model enables it to learn additional features beyond those required for generalization, leading to improved reconstruction performance.\n\nTogether, these findings demonstrate that learning additional features enhances reconstruction quality. Notably, reconstructions from pre-trained models exhibit finer structural details and less blurring, as shown in [Figure 3.](#page-10-0)\n\nWe further validate the role of learning additional features from the opposite perspective by reducing the bottleneck size (i.e., the width of the final layer after convolutions), thereby limiting the model's capacity to retain features [\\[60\\]](#page-16-10). As shown in [Table 6,](#page-10-2) this reduction in learned feature information leads to a degradation in reconstruction performance.\n\nFrom this study, we derive two key insights. First, from an attack perspective, for GAN-based methods that produce high-quality reconstructions, we observe a trade-off between the generalizability of the target model and the quality of the reconstructions. This insight could inspire further advancements in data reconstruction attacks. Second, from an evaluation perspective, the quantitative metrics employed reliably assess attack performance when the differences between methods are pronounced (e.g., comparing GAN-based methods to techniques producing less semantically meaningful reconstructions, such as MI-Face). However, since current reconstruction attacks are far from perfectly recovering target datasets, existing dataset-level and sample-level metrics often fail to align precisely with human perception, particularly when differences are subtle.\n\nWith the advancements in large language models (LLMs), some limitations of visualization as an evaluation metric can be mitigated. We encourage future research to explore inte-\n\n<span id=\"page-11-5\"></span><span id=\"page-11-1\"></span>![](_page_11_Figure_0.jpeg)\n\n(a) Influence of data access (b) Influence of model access\n\nFigure 4: Influence of auxiliary information for PLGMI. Experimental results for other attacks can be found in [Figure 14.](#page-22-0)\n\n<span id=\"page-11-2\"></span>![](_page_11_Picture_3.jpeg)\n\nFigure 5: Effect of additional data to the attack performance of Inv-Alignment.\n\n<span id=\"page-11-0\"></span>grating LLMs with quantitative metrics for a more comprehensive evaluation of data reconstruction attacks.\n\n## 7 Utilization of Attack Knowledge\n\nThis section investigates whether data reconstruction reveals sensitive information or just replicates attack knowledge, and how different levels of attack information affect performance.\n\nInfluence of Data Access: We first assess how current methods use auxiliary datasets to enhance attacks, focusing on whether reconstruction merely involves imputing data from a similarly distributed dataset. By replacing auxiliary datasets with similar but distinct ones, that is, Kuzushiji-MNIST [\\[16\\]](#page-14-14) for MNIST, CIFAR100 [\\[2\\]](#page-14-15) for CIFAR10, and FFHQ [\\[30\\]](#page-15-11) for CelebA, we observe moderate performance drops in GANbased attacks (see [Figure 4a](#page-11-1) and [Figure 14\\)](#page-22-0). This suggests reconstruction extends beyond mere imputation from auxiliary data.\n\nThe picture changes for low-fidelity methods such as Inv-Alignment. Supplying auxiliary data drawn from a different distribution can even yield better results than providing data from the original distribution. For instance, providing CI-FAR100 as the auxiliary set yields a FID of 330.31, whereas supplying data from the same distribution gives 357.91.\n\nTo further investigate this limitation, we conducted a backdoor experiment. A model was trained on a mixed dataset that included both clean and backdoored images, where the backdoored images contained a 16 × 16 black square in the bottom-right corner. We then reconstructed the training set while giving Inv-Alignment both clean and backdoored samples. Although the trigger should be a strong cue, it never appears in the reconstructions (see [Figure 5\\)](#page-11-2). These findings suggest that low-quality reconstruction methods fail to exploit critical attack information.\n\nInfluence of Model Access: We next investigate whether access to model parameters increases information leakage by\n\n#### <span id=\"page-11-3\"></span>Table 7: Effect of batch normalization for DeepInversion on VGG16 trained on CelebA.\n\n| BatchNorm | Target Data Size |       |       |        |                                                 |        |  |  |  |\n|-----------|------------------|-------|-------|--------|-------------------------------------------------|--------|--|--|--|\n|           | 1,000            | 2,000 | 5,000 | 10,000 | 15,000                                          | 20,000 |  |  |  |\n| updated   |                  |       |       |        | 287.497 283.429 273.183 273.415 245.736 234.672 |        |  |  |  |\n| fixed     |                  |       |       |        | 372.803 348.387 346.099 346.149 325.639 311.791 |        |  |  |  |\n|           |                  |       |       |        |                                                 |        |  |  |  |\n\n<span id=\"page-11-4\"></span>![](_page_11_Picture_14.jpeg)\n\nFigure 6: Visualization of attacks on VGG16 trained on backdoored CelebA with size 20,000.\n\ncomparing attack performance under black-box and whitebox settings.\n\nFor black-box evaluations, we first apply state-of-the-art model stealing techniques [\\[63\\]](#page-16-8) to create a white-box surrogate model from the black-box target model. We then execute standard attacks on this surrogate model.\n\nThe results presented in [Figure 4b](#page-11-1) and [Figure 14](#page-22-0) demonstrate that several attack methods—particularly those with stronger baseline performance—benefit from white-box access, as expected. The performance gap widens for more complex reconstruction tasks, indicating that these methods effectively exploit model parameters to improve their performance. To illustrate this point, we examine two specific attacks: DeepDream and DeepInversion. The key distinction between them is that DeepInversion explicitly leverages the statistical information encoded in BatchNorm layers. As shown in [Table 2,](#page-8-0) DeepInversion consistently outperforms DeepDream, highlighting the benefit of incorporating internal model statistics.\n\nTo further investigate the role of statistical information stored in model parameters, we conduct an experiment where a target model is trained with BatchNorm parameters fixed, allowing only the remaining parameters to be updated. We then apply DeepInversion to both the standard and modified models. As shown in [Table 7,](#page-11-3) the reconstruction performance significantly degrades in the modified setting, despite using the same attack method. This performance drop underscores the importance of access to BatchNorm statistics, which appear critical for high-quality reconstruction.\n\nHowever, it is important to note that not all attack methods exhibit improved performance under white-box conditions. In some cases, the lack of performance gain suggests that these methods fail to effectively utilize the information memorized by the target model. To further validate this, we evaluate attacks on a backdoored model, where a predefined trigger, known to be strongly memorized, is embedded during training. As shown in [Figure 6,](#page-11-4) among the selected methods, DeepInversion and Revealer are able to clearly reconstruct the square trigger, and they also produce the highest-quality reconstructions overall. This finding aligns with earlier observations and further emphasizes the limitations of certain\n\n<span id=\"page-12-3\"></span>Table 8: Evaluation on Swin with GPT-4o.\n\n<span id=\"page-12-0\"></span>\n\n| Attack | Metrics    | Target Data Size |                   |     |                                        |       |       |  |\n|--------|------------|------------------|-------------------|-----|----------------------------------------|-------|-------|--|\n|        |            |                  |                   |     | 1,000 2,000 5,000 10,000 15,000 20,000 |       |       |  |\n|        | # of Major | 101              | 435               | 229 | 185                                    | 46    | 4     |  |\n| PLGMI  | # of All   | 10               | 43                | 11  | 10                                     | 3     | 0     |  |\n|        | Pred Rate  |                  | 0.120 0.369 0.251 |     | 0.197                                  | 0.056 | 0.007 |  |\n\n<span id=\"page-12-1\"></span>Table 9: Evaluation of the Vec2Text [\\[45\\]](#page-15-12) attack performance across varying target data sizes on different datasets, measured by BLEU, Sim. (Similarity), and R-L (ROUGE-L).\n\n| Dataset | Metrics | Target Data Size  |  |                                        |       |       |  |  |\n|---------|---------|-------------------|--|----------------------------------------|-------|-------|--|--|\n|         |         |                   |  | 1,000 2,000 5,000 10,000 15,000 20,000 |       |       |  |  |\n|         | BLEU    | 0.169 0.172 0.152 |  | 0.083                                  | 0.066 | 0.058 |  |  |\n| SST2    | Sim.    | 0.707 0.706 0.663 |  | 0.514                                  | 0.460 | 0.440 |  |  |\n|         | R-L     | 0.457 0.463 0.432 |  | 0.298                                  | 0.251 | 0.231 |  |  |\n|         | BLEU    | 0.039 0.042 0.036 |  | 0.028                                  | 0.026 | 0.025 |  |  |\n| AGNews  | Sim.    | 0.658 0.657 0.585 |  | 0.503                                  | 0.470 | 0.453 |  |  |\n|         | R-L     | 0.203 0.210 0.195 |  | 0.175                                  | 0.166 | 0.163 |  |  |\n|         | BLEU    | 0.010 0.010 0.009 |  | 0.009                                  | 0.008 | 0.008 |  |  |\n| IMDB    | Sim.    | 0.531 0.501 0.477 |  | 0.468                                  | 0.463 | 0.465 |  |  |\n|         | R-L     | 0.136 0.135 0.135 |  | 0.132                                  | 0.132 | 0.132 |  |  |\n\nattack methods in fully leveraging model-internal information.\n\n## 8 Discussion and Conclusion\n\nOur findings in [Table 3](#page-9-1) reveal a clear trend: model memorization is strongly correlated with reconstruction performance. Specifically, models that exhibit higher levels of memorization toward individual training samples tend to be more vulnerable to reconstruction attacks.\n\nWe extend this analysis to larger, contemporary model architectures—particularly transformer-based models, which are widely adopted in current practice. In our experiments with Swin [\\[40\\]](#page-15-13) and MAE [\\[25\\]](#page-15-14) architectures, results presented in [Table 8](#page-12-0) support similar observations: models trained on smaller datasets exhibit greater vulnerability compared to those trained on larger datasets.\n\nWe further broaden our analysis to encompass additional data modalities, with a focus on the text domain, motivated by the rapid rise of Large Language Models. In this setting, we investigate two representative reconstruction attack strategies: Vec2Text [\\[45\\]](#page-15-12), which reconstructs input text from its embeddings, and Complete [\\[11,](#page-14-16) [67\\]](#page-16-2), which attempts to recover the original prompt provided to an LLM. Detailed descriptions of these methods and the experimental setup are provided in [Appendix G.](#page-19-0) To quantitatively measure attack efficacy, we utilized three metrics. Semantic similarity served as a macro-level metric, assessing the overall likeness between the reconstructed and original texts. For micro-level evaluation, analogous to pixel-level comparisons in image reconstruction, we employed BLEU and ROUGE-L scores to capture finer-grained textual similarities.\n\nAs shown in [Table 9,](#page-12-1) despite the difference in modality, we observe a consistent pattern: training on larger datasets reduces the model's memorization of individual samples,\n\n<span id=\"page-12-2\"></span>Table 10: Effect of the features learned by the target model to the attack performance for VGG16 trained on CelebA. Lower FID indicates better attack performance.\n\n| Attack                                                        | Target Data Size |                         |                                                 |        |        |        |  |  |\n|---------------------------------------------------------------|------------------|-------------------------|-------------------------------------------------|--------|--------|--------|--|--|\n|                                                               | 1,000            | 2,000                   | 5,000                                           | 10,000 | 15,000 | 20,000 |  |  |\n| PLGMI (DP)                                                    |                  |                         | 173.735 156.141 140.480 127.090 122.218 119.856 |        |        |        |  |  |\n| PLGMI (Prune) 154.435 153.764 138.074 126.206 122.993 121.437 |                  |                         |                                                 |        |        |        |  |  |\n| PLGMI                                                         |                  | 127.722 107.833 104.842 |                                                 | 97.730 | 84.836 | 85.143 |  |  |\n| PLGMI (Pre)                                                   | 94.603           | 82.571                  | 86.860                                          | 78.429 | 82.768 | 80.814 |  |  |\n\nwhich in turn leads to poorer reconstruction quality. This suggests that memorization plays a critical role in determining vulnerability to reconstruction.\n\nBuilding on these observations, a natural defense strategy emerges: reducing memorization may decrease susceptibility to reconstruction attacks. This insight helps explain why Differential Privacy (DP) is effective—by limiting the model's exposure to individual samples, DP inherently reduces memorization. Beyond DP, we also explore model pruning as an alternative. As shown in [Table 10,](#page-12-2) pruning a substantial number of neuron connections discards stored information, which can mitigate unnecessary memorization and thus reduce reconstruction attack success. Notably, pruning results in minimal performance degradation—typically under 1%—whereas DP often incurs a much larger accuracy drop [\\[24\\]](#page-15-15). This suggests that pruning may be a more practical defense method in some cases and highlights the potential of developing defenses that target sample-specific memorization.\n\nThis intuition is further supported by the observation that datasets used for fine-tuning pre-trained models are often more susceptible to reconstruction. During fine-tuning, the model, having already learned general data distributions from pre-training, tends to develop stronger memorization of the specific features unique to the fine-tuning samples. This underscores the need for caution when fine-tuning pre-trained models, as this process can heighten data exposure risks. This finding also resonates with existing research on attacks that manipulate pre-trained models to render fine-tuned versions more vulnerable to membership inference [\\[37,](#page-15-16) [69\\]](#page-17-10). Such research indicates that adversaries might modify pre-trained models to make the fine-tuning dataset easier to reconstruct, potentially by encouraging the model to memorize more sample-specific features.\n\nFurthermore, caution should be exercised when releasing model parameters, particularly those that encapsulate statistical information about the training data, as demonstrated in [Table 7.](#page-11-3) Such parameters can be exploited in reconstruction efforts.\n\nFinally, for the advancement of reconstruction attack methodologies, researchers should aim to optimally utilize all available information. This includes strategically leveraging knowledge of the dataset distribution or statistical details embedded within model parameters. Concurrently, a complementary and important research avenue involves developing robust reconstruction techniques that can succeed even without access to such auxiliary information.\n\n## Acknowledgments\n\nWe would like to thank the anonymous reviewers for their insightful comments and constructive suggestions. We are especially grateful to Tianhao Wang for the in-depth discussions and valuable feedback throughout the development of this work. This work is partially funded by the European Health and Digital Executive Agency (HADEA) within the project \"Understanding the individual host response against Hepatitis D Virus to develop a personalized approach for the management of hepatitis D\" (DSolve, grant agreement number 101057917) and the BMBF with the project \"Repräsentative, synthetische Gesundheitsdaten mit starken Privatsph ärengarantien\" (PriSyn, 16KISAO29K).\n\n## Ethics Considerations\n\nAll experiments in this study were conducted using publicly available open-source datasets, ensuring that no private or sensitive data was involved. The target models were trained exclusively on open-source benchmark datasets, which were utilized solely for the purposes of this research. Furthermore, the reconstruction attacks were designed to reconstruct data from these open-source datasets only, and no attempt was made to access or infer any sensitive or personal information. This approach aligns with ethical research practices and ensures that the work complies with privacy standards and community guidelines.\n\n## Open Science\n\nIn compliance with the open science policy, we are committed to promoting transparency and reproducibility in our research. To this end, we share the artifacts associated with our work, including the data reconstruction attack framework and its evaluation code, available at [https://doi.](https://doi.org/10.5281/zenodo.15603060) [org/10.5281/zenodo.15603060](https://doi.org/10.5281/zenodo.15603060). These resources are provided to facilitate further research and the development of more efficient data reconstruction attack methods, enabling researchers to better evaluate privacy leakage in machine learning models. By making these tools publicly available, we aim to contribute meaningfully to the broader scientific community and uphold the principles of open science.\n\n## References\n\n- <span id=\"page-14-5\"></span>[1] [https://ai.googleblog.com/2015/06/](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) [inceptionism-going-deeper-into-neural.html](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html). [2,](#page-1-0) [5](#page-4-2)\n- <span id=\"page-14-15\"></span>[2] [https://www.cs.toronto.edu/~kriz/cifar.](https://www.cs.toronto.edu/~kriz/cifar.html) [html](https://www.cs.toronto.edu/~kriz/cifar.html). [12,](#page-11-5) [18](#page-17-11)\n- <span id=\"page-14-17\"></span>[3] <http://yann.lecun.com/exdb/mnist/>. [18](#page-17-11)\n- <span id=\"page-14-12\"></span>[4] Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep Learning with Differential Privacy. In *ACM SIGSAC Conference on Computer and Communications Security (CCS)*, pages 308–318. ACM, 2016. [4,](#page-3-1) [20](#page-19-1)\n- <span id=\"page-14-20\"></span>[5] Giuseppe Ateniese, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and Giovanni Felici. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers. *Int. J. Secur. Networks*, 2015. [22](#page-21-0)\n- <span id=\"page-14-8\"></span>[6] Borja Balle, Giovanni Cherubin, and Jamie Hayes. Reconstructing Training Data with Informed Adversaries. In *IEEE Symposium on Security and Privacy (S&P)*, pages 1138–1156. IEEE, 2022. [3](#page-2-0)\n- <span id=\"page-14-0\"></span>[7] Martin Bäuml, Makarand Tapaswi, and Rainer Stiefelhagen. Semi-supervised Learning with Constraints for Person Identification in Multimedia Data. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 3602–3609. IEEE, 2013. [1](#page-0-0)\n- <span id=\"page-14-9\"></span>[8] Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David Duvenaud, and Jörn-Henrik Jacobsen. Invertible Residual Networks. In *International Conference on Machine Learning (ICML)*, pages 573–582. PMLR, 2019. [3](#page-2-0)\n- <span id=\"page-14-4\"></span>[9] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramèr. Membership Inference Attacks From First Principles. In *IEEE Symposium on Security and Privacy (S&P)*, pages 1897– 1914. IEEE, 2022. [2,](#page-1-0) [3,](#page-2-0) [22](#page-21-0)\n- <span id=\"page-14-11\"></span>[10] Nicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, and Florian Tramèr. The Privacy Onion Effect: Memorization is Relative. In *Annual Conference on Neural Information Processing Systems (NeurIPS)*. NeurIPS, 2022. [4,](#page-3-1) [10](#page-9-3)\n- <span id=\"page-14-16\"></span>[11] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting Training Data from Large Language Models. In *USENIX Security Symposium (USENIX Security)*, pages 2633–2650. USENIX, 2021. [13,](#page-12-3) [20,](#page-19-1) [21](#page-20-1)\n- <span id=\"page-14-1\"></span>[12] Paola Cerchiello and Paolo Giudici. Big Data Analysis for Financial Risk Management. *Journal of Big Data*, 2016. [1](#page-0-0)\n- <span id=\"page-14-22\"></span>[13] Harsh Chaudhari, John Abascal, Alina Oprea, Matthew Jagielski, Florian Tramèr, and Jonathan R. Ullman. SNAP: Efficient Extraction of Private Properties with Poisoning. In *IEEE Symposium on Security and Privacy (S&P)*, pages 1935–1952. IEEE, 2023. [22](#page-21-0)\n- <span id=\"page-14-7\"></span>[14] Si Chen, Mostafa Kahla, Ruoxi Jia, and Guo-Jun Qi. Knowledge-Enriched Distributional Model Inversion Attacks. In *IEEE International Conference on Computer Vision (ICCV)*, pages 16158–16167. IEEE, 2021. [2,](#page-1-0) [3,](#page-2-0) [6](#page-5-2)\n- <span id=\"page-14-19\"></span>[15] Christopher A. Choquette Choo, Florian Tramèr, Nicholas Carlini, and Nicolas Papernot. Label-Only Membership Inference Attacks. In *International Conference on Machine Learning (ICML)*, pages 1964– 1974. PMLR, 2021. [22](#page-21-0)\n- <span id=\"page-14-14\"></span>[16] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep Learning for Classical Japanese Literature. *CoRR abs/1812.01718*, 2018. [12](#page-11-5)\n- <span id=\"page-14-18\"></span>[17] Haonan Duan, Adam Dziedzic, Mohammad Yaghini, Nicolas Papernot, and Franziska Boenisch. On the Privacy Risk of In-context Learning. In *Workshop on Trustworthy Natural Language Processing (TrustNLP)*, 2023. [20](#page-19-1)\n- <span id=\"page-14-13\"></span>[18] Cynthia Dwork and Aaron Roth. *The Algorithmic Foundations of Differential Privacy*. Now Publishers Inc., 2014. [4](#page-3-1)\n- <span id=\"page-14-10\"></span>[19] Vitaly Feldman. Does Learning Require Memorization? A Short Tale about a Long Tail. In *Annual ACM Symposium on Theory of Computing (STOC)*, pages 954–959. ACM, 2020. [4](#page-3-1)\n- <span id=\"page-14-2\"></span>[20] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures. In *ACM SIGSAC Conference on Computer and Communications Security (CCS)*, pages 1322–1333. ACM, 2015. [1,](#page-0-0) [2,](#page-1-0) [5,](#page-4-2) [6,](#page-5-2) [7,](#page-6-2) [18](#page-17-11)\n- <span id=\"page-14-21\"></span>[21] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and Nikita Borisov. Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations. In *ACM SIGSAC Conference on Computer and Communications Security (CCS)*, pages 619–633. ACM, 2018. [22](#page-21-0)\n- <span id=\"page-14-3\"></span>[22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In *Annual Conference on Neural Information Processing Systems (NIPS)*, pages 2672–2680. NIPS, 2014. [1](#page-0-0)\n- <span id=\"page-14-6\"></span>[23] Niv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir, and Michal Irani. Reconstructing Training Data from Trained Neural Networks. In *Annual Conference on Neural Information Processing Systems (NeurIPS)*. NeurIPS, 2022. [2,](#page-1-0) [3](#page-2-0)\n- <span id=\"page-15-15\"></span>[24] Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both Weights and Connections for Efficient Neural Networks. *CoRR abs/1506.02626*, 2015. [13](#page-12-3)\n- <span id=\"page-15-14\"></span>[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked Autoencoders Are Scalable Vision Learners. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 15979–15988. IEEE, 2022. [13,](#page-12-3) [21](#page-20-1)\n- <span id=\"page-15-18\"></span>[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 770–778. IEEE, 2016. [19](#page-18-1)\n- <span id=\"page-15-1\"></span>[27] Xinlei He, Zheng Li, Weilin Xu, Cory Cornelius, and Yang Zhang. Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models. *CoRR abs/2208.10445*, 2022. [1](#page-0-0)\n- <span id=\"page-15-2\"></span>[28] Xinlei He, Rui Wen, Yixin Wu, Michael Backes, Yun Shen, and Yang Zhang. Node-Level Membership Inference Attacks Against Graph Neural Networks. *CoRR abs/2102.05429*, 2021. [1,](#page-0-0) [2](#page-1-0)\n- <span id=\"page-15-21\"></span>[29] Sanjay Kariyappa, Atul Prakash, and Moinuddin K. Qureshi. MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 13814–13823. IEEE, 2021. [22](#page-21-0)\n- <span id=\"page-15-11\"></span>[30] Tero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Generative Adversarial Networks. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 4401–4410. IEEE, 2019. [12](#page-11-5)\n- <span id=\"page-15-9\"></span>[31] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. In *Annual Conference on Neural Information Processing Systems (NeurIPS)*, pages 10236–10245. NeurIPS, 2018. [3](#page-2-0)\n- <span id=\"page-15-0\"></span>[32] Pahulpreet Singh Kohli and Shriya Arora. Application of Machine Learning in Disease Prediction. In *International Conference on Computing Communication and Automation (ICCCA)*, pages 1–4. IEEE, 2018. [1](#page-0-0)\n- <span id=\"page-15-3\"></span>[33] Hao Li, Zheng Li, Siyuan Wu, Chengrui Hu, Yutong Ye, Min Zhang, Dengguo Feng, and Yang Zhang. SeqMIA: Sequential-Metric Based Membership Inference Attack. *CoRR abs/2407.15098*, 2024. [1](#page-0-0)\n- [34] Zheng Li, Xinlei He, Ning Yu, and Yang Zhang. Membership Inference Attack Against Masked Image Modeling. *CoRR abs/2408.06825*, 2024. [1](#page-0-0)\n- <span id=\"page-15-6\"></span>[35] Zheng Li, Yiyong Liu, Xinlei He, Ning Yu, Michael Backes, and Yang Zhang. Auditing Membership Leakages of Multi-Exit Networks. In *ACM SIGSAC Conference on Computer and Communications Security (CCS)*, pages 1917–1931. ACM, 2022. [1,](#page-0-0) [2](#page-1-0)\n- <span id=\"page-15-4\"></span>[36] Zheng Li and Yang Zhang. Membership Leakage in Label-Only Exposures. In *ACM SIGSAC Conference on*\n\n*Computer and Communications Security (CCS)*, pages 880–895. ACM, 2021. [1,](#page-0-0) [2,](#page-1-0) [22](#page-21-0)\n\n- <span id=\"page-15-16\"></span>[37] Ruixuan Liu, Tianhao Wang, Yang Cao, and Li Xiong. PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps. In *ACM SIGSAC Conference on Computer and Communications Security (CCS)*, pages 3511–3524. ACM, 2024. [13](#page-12-3)\n- <span id=\"page-15-7\"></span>[38] Yiyong Liu, Zhengyu Zhao, Michael Backes, and Yang Zhang. Membership Inference Attacks by Exploiting Loss Trajectory. In *ACM SIGSAC Conference on Computer and Communications Security (CCS)*, pages 2085–2098. ACM, 2022. [2,](#page-1-0) [22](#page-21-0)\n- <span id=\"page-15-8\"></span>[39] Yugeng Liu, Rui Wen, Xinlei He, Ahmed Salem, Zhikun Zhang, Michael Backes, Emiliano De Cristofaro, Mario Fritz, and Yang Zhang. ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models. In *USENIX Security Symposium (USENIX Security)*, pages 4525–4542. USENIX, 2022. [2,](#page-1-0) [7](#page-6-2)\n- <span id=\"page-15-13\"></span>[40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In *IEEE International Conference on Computer Vision (ICCV)*, pages 9992–10002. IEEE, 2021. [13,](#page-12-3) [21](#page-20-1)\n- <span id=\"page-15-17\"></span>[41] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. In *IEEE International Conference on Computer Vision (ICCV)*, pages 3730–3738. IEEE, 2015. [18](#page-17-11)\n- <span id=\"page-15-19\"></span>[42] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning Word Vectors for Sentiment Analysis. In *Annual Meeting of the Association for Computational Linguistics (ACL)*, pages 142–150. ACL, 2011. [20](#page-19-1)\n- <span id=\"page-15-20\"></span>[43] Saeed Mahloujifar, Esha Ghosh, and Melissa Chase. Property Inference from Poisoning. In *IEEE Symposium on Security and Privacy (S&P)*, pages 1120–1137. IEEE, 2022. [22](#page-21-0)\n- <span id=\"page-15-5\"></span>[44] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting Unintended Feature Leakage in Collaborative Learning. In *IEEE Symposium on Security and Privacy (S&P)*, pages 497–512. IEEE, 2019. [1](#page-0-0)\n- <span id=\"page-15-12\"></span>[45] John X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander M. Rush. Text Embeddings Reveal (Almost) As Much As Text. In *Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 12448–12460. Association for Computational Linguistics, 2023. [13,](#page-12-3) [20](#page-19-1)\n- <span id=\"page-15-10\"></span>[46] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Úlfar Erlingsson. Scalable Private Learning with PATE. In *International Conference on Learning Representations (ICLR)*, 2018. [4](#page-3-1)\n- <span id=\"page-16-3\"></span>[47] Ahmed Salem, Apratim Bhattacharya, Michael Backes, Mario Fritz, and Yang Zhang. Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning. In *USENIX Security Symposium (USENIX Security)*, pages 1291–1308. USENIX, 2020. [1,](#page-0-0) [2,](#page-1-0) [3,](#page-2-0) [5,](#page-4-2) [6](#page-5-2)\n- <span id=\"page-16-9\"></span>[48] Ahmed Salem, Giovanni Cherubin, David Evans, Boris Köpf, Andrew Paverd, Anshuman Suri, Shruti Tople, and Santiago Zanella Béguelin. SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning. In *IEEE Symposium on Security and Privacy (S&P)*, pages 327–345. IEEE, 2023. [7](#page-6-2)\n- <span id=\"page-16-0\"></span>[49] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes. ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models. In *Network and Distributed System Security Symposium (NDSS)*. Internet Society, 2019. [1,](#page-0-0) [2,](#page-1-0) [22](#page-21-0)\n- <span id=\"page-16-12\"></span>[50] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 4510–4520. IEEE, 2018. [19](#page-18-1)\n- <span id=\"page-16-18\"></span>[51] Sunandini Sanyal, Sravanti Addepalli, and R. Venkatesh Babu. Towards Data-Free Model Stealing in a Hard Label Setting. *CoRR abs/2204.11022*, 2022. [22](#page-21-0)\n- <span id=\"page-16-1\"></span>[52] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership Inference Attacks Against Machine Learning Models. In *IEEE Symposium on Security and Privacy (S&P)*, pages 3–18. IEEE, 2017. [1,](#page-0-0) [2,](#page-1-0) [22](#page-21-0)\n- <span id=\"page-16-11\"></span>[53] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In *International Conference on Learning Representations (ICLR)*, 2015. [19](#page-18-1)\n- <span id=\"page-16-13\"></span>[54] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In *Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 1631–1642. ACL, 2013. [20](#page-19-1)\n- <span id=\"page-16-19\"></span>[55] Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. Machine Learning Models that Remember Too Much. In *ACM SIGSAC Conference on Computer and Communications Security (CCS)*, pages 587–601. ACM, 2017. [22](#page-21-0)\n- <span id=\"page-16-20\"></span>[56] Congzheng Song and Vitaly Shmatikov. Overlearning Reveals Sensitive Attributes. In *International Conference on Learning Representations (ICLR)*, 2020. [22](#page-21-0)\n- <span id=\"page-16-6\"></span>[57] Yang Song, Chenlin Meng, and Stefano Ermon. Mint-Net: Building Invertible Neural Networks with Masked\n\nConvolutions. In *Annual Conference on Neural Information Processing Systems (NeurIPS)*, pages 11002– 11012. NeurIPS, 2019. [3](#page-2-0)\n\n- <span id=\"page-16-5\"></span>[58] Lukas Struppek, Dominik Hintersdorf, Antonio De Almeida Correia, Antonia Adler, and Kristian Kersting. Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks. In *International Conference on Machine Learning (ICML)*, pages 20522–20545. PMLR, 2022. [2,](#page-1-0) [5,](#page-4-2) [6](#page-5-2)\n- <span id=\"page-16-16\"></span>[59] Anshuman Suri and David Evans. Formalizing and Estimating Distribution Inference Risks. *CoRR abs/2109.06024*, 2021. [22](#page-21-0)\n- <span id=\"page-16-10\"></span>[60] Naftali Tishby and Noga Zaslavsky. Deep Learning and the Information Bottleneck Principle. *CoRR abs/1503.02406*, 2015. [11](#page-10-3)\n- <span id=\"page-16-7\"></span>[61] Florian Tramèr, Reza Shokri, Ayrton San Joaquin, Hoang Le, Matthew Jagielski, Sanghyun Hong, and Nicholas Carlini. Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets. In *ACM SIGSAC Conference on Computer and Communications Security (CCS)*. ACM, 2022. [4,](#page-3-1) [10,](#page-9-3) [22](#page-21-0)\n- <span id=\"page-16-17\"></span>[62] Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. Stealing Machine Learning Models via Prediction APIs. In *USENIX Security Symposium (USENIX Security)*, pages 601–618. USENIX, 2016. [22](#page-21-0)\n- <span id=\"page-16-8\"></span>[63] Jean-Baptiste Truong, Pratyush Maini, Robert J. Walls, and Nicolas Papernot. Data-Free Model Extraction. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 4771–4780. IEEE, 2021. [6,](#page-5-2) [12,](#page-11-5) [22](#page-21-0)\n- <span id=\"page-16-4\"></span>[64] Kuan-Chieh Wang, Yan Fu, Ke Li, Ashish Khisti, Richard S. Zemel, and Alireza Makhzani. Variational Model Inversion Attacks. In *Annual Conference on Neural Information Processing Systems (NeurIPS)*, pages 9706–9719. NeurIPS, 2021. [2,](#page-1-0) [6](#page-5-2)\n- <span id=\"page-16-15\"></span>[65] Tianhao Wang, Yuheng Zhang, and Ruoxi Jia. Improving Robustness to Model Inversion Attacks via Mutual Information Regularization. In *AAAI Conference on Artificial Intelligence (AAAI)*, pages 11666–11673. AAAI, 2021. [20](#page-19-1)\n- <span id=\"page-16-21\"></span>[66] Rui Wen, Michael Backes, and Yang Zhang. Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm? In *Network and Distributed System Security Symposium (NDSS)*. Internet Society, 2025. [22](#page-21-0)\n- <span id=\"page-16-2\"></span>[67] Rui Wen, Zheng Li, Michael Backes, and Yang Zhang. Membership Inference Attacks Against In-Context Learning. In *ACM SIGSAC Conference on Computer and Communications Security (CCS)*. ACM, 2024. [1,](#page-0-0) [13,](#page-12-3) [20,](#page-19-1) [21](#page-20-1)\n- <span id=\"page-16-14\"></span>[68] Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, and Ahmed Salem. Last One Standing: A Comparative Analysis of Security and Privacy of Soft\n\n<span id=\"page-17-11\"></span>Prompt Tuning, LoRA, and In-Context Learning. *CoRR abs/2310.11397*, 2023. [20](#page-19-1)\n\n- <span id=\"page-17-10\"></span>[69] Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, and Nicholas Carlini. Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models. In *Annual Conference on Neural Information Processing Systems (NeurIPS)*. NeurIPS, 2024. [13](#page-12-3)\n- <span id=\"page-17-1\"></span>[70] Yixin Wu, Rui Wen, Michael Backes, Pascal Berrang, Mathias Humbert, Yun Shen, and Yang Zhang. Quantifying Privacy Risks of Prompts in Visual Prompt Learning. In *USENIX Security Symposium (USENIX Security)*. USENIX, 2024. [1](#page-0-0)\n- <span id=\"page-17-3\"></span>[71] Ziqi Yang, Jiyi Zhang, Ee-Chien Chang, and Zhenkai Liang. Neural Network Inversion in Adversarial Setting via Background Knowledge Alignment. In *ACM SIGSAC Conference on Computer and Communications Security (CCS)*, page 225–240. ACM, 2019. [1,](#page-0-0) [2,](#page-1-0) [3,](#page-2-0) [5,](#page-4-2) [6](#page-5-2)\n- <span id=\"page-17-0\"></span>[72] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven C. H. Hoi. Deep Learning for Person Re-Identification: A Survey and Outlook. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2022. [1](#page-0-0)\n- <span id=\"page-17-6\"></span>[73] Hongxu Yin, Pavlo Molchanov, Jose M. Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K. Jha, and Jan Kautz. Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 8712–8721. IEEE, 2020. [2,](#page-1-0) [5,](#page-4-2) [6](#page-5-2)\n- <span id=\"page-17-7\"></span>[74] Xiaojian Yuan, Kejiang Chen, Jie Zhang, Weiming Zhang, Nenghai Yu, and Yang Zhang. Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network. In *AAAI Conference on Artificial Intelligence (AAAI)*. AAAI, 2023. [3](#page-2-0)\n- <span id=\"page-17-8\"></span>[75] Zhuowen Yuan, Fan Wu, Yunhui Long, Chaowei Xiao, and Bo Li. SecretGen: Privacy Recovery on Pre-trained Models via Distribution Discrimination. In *European Conference on Computer Vision (ECCV)*, pages 139– 155. Springer, 2022. [5,](#page-4-2) [6](#page-5-2)\n- <span id=\"page-17-2\"></span>[76] Minxing Zhang, Ning Yu, Rui Wen, Michael Backes, and Yang Zhang. Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models. In *Winter Conference on Applications of Computer Vision (WACV)*, pages 4827–4837. IEEE, 2024. [1](#page-0-0)\n- <span id=\"page-17-12\"></span>[77] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level Convolutional Networks for Text Classification. In *Annual Conference on Neural Information Processing Systems (NIPS)*, pages 649–657. NIPS, 2015. [20](#page-19-1)\n- <span id=\"page-17-4\"></span>[78] Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song. The Secret Revealer: Generative Model-Inversion Attacks Against\n\nDeep Neural Networks. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 250–258. IEEE, 2020. [1,](#page-0-0) [2,](#page-1-0) [3,](#page-2-0) [5,](#page-4-2) [6](#page-5-2)\n\n<span id=\"page-17-5\"></span>[79] Ligeng Zhu, Zhijian Liu, and Song Han. Deep Leakage from Gradients. In *Annual Conference on Neural Information Processing Systems (NeurIPS)*, pages 14747–14756. NeurIPS, 2019. [1,](#page-0-0) [2,](#page-1-0) [3,](#page-2-0) [5,](#page-4-2) [6](#page-5-2)\n\n## A Counterexamples\n\nWe generate counterexamples in [Figure 7a](#page-18-0) which contribute a lot to the model training, but are vastly dissimilar from the target dataset.\n\nWe generate counterexamples that resemble random noise but are still classified with high confidence by the evaluation model in [Figure 7b.](#page-18-0) These counterexamples, generated using MI-Face [\\[20\\]](#page-14-2), achieve prediction confidences above 0.9 for their corresponding classes.\n\n## B t-SNE visualization\n\nTo provide an overview of the relationship between the reconstructed dataset and the target dataset, we leverage tdistributed stochastic neighbor embedding (t-SNE) to reduce these datasets to two dimensions and exhibit their distributions in [Figure 8,](#page-18-2) [Figure 9,](#page-18-3) and [Figure 10.](#page-19-2)\n\n## <span id=\"page-17-9\"></span>C Setup For Main Experiments\n\nDatasets: We conduct our evaluation on three benchmark datasets, including CelebA [\\[41\\]](#page-15-17), CIFAR10 [\\[2\\]](#page-14-15), and MNIST [\\[3\\]](#page-14-17). CelebA is a large-scale face dataset with 10,177 identities and 202,599 images. CIFAR10 consists of 60,000 colored images belonging to 10 classes equally; these classes include common objects in daily life like airplanes, birds, and dogs. MNIST is a gray-scale dataset containing 10 classes of digital handwritten numbers.\n\nAttack Details: We conduct an evaluation of ten representative reconstruction attacks that were introduced in [Sec](#page-1-1)[tion 2.2.](#page-1-1) In order to conform to our definition, some of the attacks required modifications. Specifically, the MI-Face attack was originally designed to recover one image for each class due to the fixed initialization, but we enable this attack to generate multiple samples for a single class by using random initialization. Similarly, Inv-Alignment assumes that the adversary had information about the posterior corresponding to the target samples, but we generalize this assumption by randomly creating pseudo-posteriors and using them to reconstruct the target dataset.\n\nWe follow the standard setting to split the dataset into two disjoint parts to prevent data leakage. For CelebA, training and updating data come from 1,000 identities, with balanced subsets of sizes 1,000, 2,000, 5,000, 10,000, 15,000, and 20,000 used for training the target model. Smaller subsets are included in larger ones. A disjoint subset of size 30,000 is used as the auxiliary dataset, which can be further used to train the generator models. For CIFAR10 and MNIST,\n\n<span id=\"page-18-1\"></span><span id=\"page-18-0\"></span>![](_page_18_Figure_0.jpeg)\n\nFigure 7: Counterexamples of accuracy-based metrics. The left examples show that using random-like samples can train a wellgeneralized model, and the right examples demonstrate that random-like samples can be predicted with high confidence.\n\n<span id=\"page-18-2\"></span>![](_page_18_Figure_2.jpeg)\n\n<span id=\"page-18-3\"></span>(a) MI-Face (b) DeepDream (c) DeepInversion (d) Revealer (e) Inv-Alignment -20 -10 0 10 20 -10 -5 0 5 10 (f) Bias-Rec -30 -20 -10 0 10 20 30 -20 -10 0 10 20 30 (g) KEDMI -40 -20 0 20 -40 -20 0 20 40 (h) PLGMI -4 -2 0 2 4 -6 -4 -2 0 2 4 (i) Updates-Leak -2 -1 0 1 2 -3 -2 -1 0 1 2 3 (j) Deep-Leakage\n\nFigure 9: t-SNE visualization of reconstructions and corresponding target datasets for CIFAR10.\n\nwe divide each dataset into two non-overlapping parts: one containing samples from the first five classes, designated as the target dataset, and the other comprising samples from the remaining five classes. The sizes of the target datasets vary from 100 to 20,000, while the auxiliary dataset size is consistently set at 20,000. For dynamic-type attacks, which involve various versions of the target model, the models are updated with 100 samples. Static-type attacks aim to reconstruct the data used for training the target model, while dynamic-type attacks target the 100 updating samples.\n\nWe evaluate all attacks except for Bias-Rec on three different model architectures: VGG16 [\\[53\\]](#page-16-11), MobileNetV2 [\\[50\\]](#page-16-12), and ResNet-18 [\\[26\\]](#page-15-18). Bias-Rec requires the target model to follow certain architecture constraints, namely that it should not contain any skip-connections or bias terms. Therefore, we adhere to the original paper's methodology and construct a model composed of linear layers for this particular attack.\n\nMetric Details: As noted in Definition [4.2,](#page-6-3) the choice of sample-level metric *d* in S-Dis can be determined by the auditor. In this paper, we use three common metrics to measure sample similarity: Structural Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE). For SSIM and PSNR, higher values indicate greater similarity, while for MSE, the opposite is true.\n\n<span id=\"page-19-2\"></span><span id=\"page-19-1\"></span>![](_page_19_Picture_0.jpeg)\n\nFigure 10: t-SNE visualization of reconstructions and corresponding target datasets for MNIST.\n\nFor the dataset-level metric, we leverage FID to represent the distribution similarity, and a lower FID score indicates a smaller distance between two distributions. Note that these metrics can be normalized into [0,1] to incorporate with our definition, for ease of comparison, we present the original value in the following sections.\n\n## D Coverage Relationship\n\nWe present the relationship between coverages with different sample-level metrics and the dataset-level metric for CI-FAR10 and MNIST together in [Figure 11,](#page-20-0) and separately for CelebA in [Figure 12.](#page-20-2) The findings reveal an intriguing observation: high-quality reconstructions do not necessarily correspond to greater diversity in the reconstructed samples.\n\n## E Influence of Reconstruction Quantity\n\nWe explicitly stipulate that the reconstruction algorithm must produce the same number of samples as the target dataset. In [Section 3.2,](#page-4-3) we provide an intuitive justification for this requirement, and in this section, we present empirical evidence to support our decision. Specifically, we use DeepInversion with 20,000 target samples as a representative case and reconstruct a total of 40,000 samples. We evaluate the performance using different numbers of reconstructions, ranging from 5,000 to 40,000 in steps of 5,000.\n\nAs depicted in [Figure 13,](#page-22-1) the number of reconstructions has a minimal impact on reconstruction quality, both at the dataset level and sample level. This suggests that the reconstruction algorithm's ability is not affected by the number of reconstructions.\n\nOn the other hand, increasing the number of reconstructions significantly increases coverage. This supports our earlier argument in [Section 3.2](#page-4-3) that generating an infinite number of samples through random initialization is not reasonable, as it only increases coverage without improving reconstruction quality. Furthermore, without the guidance of the target dataset, the adversary has no way to determine which reconstructions are of high quality, and the increased diversity does not contribute to better performance at either the dataset level or sample level.\n\n## F Influence of Attack Knowledge\n\n<span id=\"page-19-0\"></span>We plot the influence of attack knowledge in [Figure 14.](#page-22-0)\n\n## G Experimental Setup For The Text Modality\n\nOur experimental evaluation is performed on three established text datasets: SST2 [\\[54\\]](#page-16-13), AGNews [\\[77\\]](#page-17-12), and IMDB [\\[42\\]](#page-15-19). For fine-tuning the target models, we utilize subsets of these datasets ranging from 1,000 to 20,000 samples. This focus on fine-tuning, rather than training language models from scratch, is adopted due to the substantial data requirements of the latter and aligns with common practices in recent literature [\\[17,](#page-14-18) [68\\]](#page-16-14).\n\nWe investigate two distinct attack methodologies:\n\n- 1. Vec2Text Attack [\\[45\\]](#page-15-12): In this configuration, the sentence-transformers/gtr-t5-base model is fine-tuned. The corresponding decoder is a T5-base model, which is further trained using an auxiliary dataset drawn from the same data distribution as the fine-tuning set.\n- 2. Complete Attack [\\[11,](#page-14-16) [67\\]](#page-16-2): For this attack, the GPT2 model is fine-tuned for 20 epochs. To reconstruct the original input, the fine-tuned model is queried using the first three words of the said input as a prompt.\n\nDetailed results of these attack evaluations are presented in [Table 9](#page-12-1) and [Table 11.](#page-20-3) To assess the quality of the reconstructed text, we employ several metrics. Semantic similarity is quantified by the cosine similarity between sentence embeddings; these embeddings are generated using the sentence-transformers/all-MiniLM-L6-v2 model. Additionally, we report BLEU scores to measure n-gram precision and ROUGE-L scores to evaluate the longest common subsequence, thereby providing insights into the lexical overlap and fluency of the reconstructed outputs.\n\n## H Defenses Against Reconstruction Attacks\n\nWe test Mutual Information Regularization (MID) [\\[65\\]](#page-16-15) and Differential Privacy (DP) [\\[4\\]](#page-14-12) as countermeasures against data\n\n<span id=\"page-20-1\"></span><span id=\"page-20-0\"></span>![](_page_20_Figure_0.jpeg)\n\nFigure 11: Relationship between coverages calculated with different sample-level metrics and the dataset-level metric for CIFAR-10 and MNIST, we plot the correlation heatmap using the absolute value of correlation between different metrics.\n\n<span id=\"page-20-2\"></span>![](_page_20_Figure_2.jpeg)\n\nFigure 12: Relationship between coverages calculated with different sample-level metrics and the dataset-level metric for CelebA, we plot the correlation heatmap using the absolute value of correlation between different metrics.\n\n<span id=\"page-20-3\"></span>Table 11: Evaluation of the Complete [\\[11,](#page-14-16) [67\\]](#page-16-2) attack performance across varying target data sizes on different datasets, measured by BLEU, Sim. (Similarity), and R-L (ROUGE-L). Higher scores on these metrics correspond to better attack performance.\n\n| Dataset | Metric      |                        |                                        | Target Data Size                       |                         |                         |\n|---------|-------------|------------------------|----------------------------------------|----------------------------------------|-------------------------|-------------------------|\n|         |             |                        |                                        | 1,000 2,000 5,000 10,000 15,000 20,000 |                         |                         |\n| SST2    | Sim.<br>R-L | BLEU 0.606 0.527 0.450 | 0.875 0.774 0.721<br>0.777 0.651 0.575 | 0.115<br>0.496<br>0.229                | 0.103<br>0.449<br>0.205 | 0.097<br>0.417<br>0.203 |\n| AGNews  | Sim.<br>R-L | BLEU 0.412 0.397 0.370 | 0.865 0.859 0.848<br>0.576 0.563 0.536 | 0.328<br>0.825<br>0.493                | 0.291<br>0.806<br>0.452 | 0.258<br>0.784<br>0.415 |\n| IMDB    | Sim.<br>R-L | BLEU 0.344 0.282 0.123 | 0.747 0.697 0.552<br>0.557 0.481 0.272 | 0.051<br>0.463<br>0.182                | 0.035<br>0.442<br>0.164 | 0.029<br>0.431<br>0.157 |\n\nreconstruction attacks. The experimental results, summarized in [Table 16,](#page-26-0) demonstrate that both MID and DP can mitigate model vulnerability in most cases. This is evidenced by higher Fréchet Inception Distance (FID) scores, indicating successful defense, across varied training dataset sizes.\n\nHowever, the protective capabilities of MID and DP are not uniformly significant across all evaluated conditions. This variability in performance underscores the pressing need for the research community to develop more consistently robust and broadly effective defense mechanisms. Such advancements are crucial for adequately countering the sophisticated threats posed by advanced reconstruction attacks.\n\nTable 12: Evaluation on MAE with GPT-4o.\n\n<span id=\"page-20-4\"></span>\n\n| Attack | Metrics    | Target Data Size |                   |     |                                        |       |       |  |\n|--------|------------|------------------|-------------------|-----|----------------------------------------|-------|-------|--|\n|        |            |                  |                   |     | 1,000 2,000 5,000 10,000 15,000 20,000 |       |       |  |\n|        | # of Major | 79               | 233               | 312 | 275                                    | 76    | 25    |  |\n| PLGMI  | # of All   | 7                | 12                | 24  | 24                                     | 0     | 1     |  |\n|        | Pred Rate  |                  | 0.097 0.222 0.299 |     | 0.267                                  | 0.090 | 0.025 |  |\n\n## I Transferability To Larger Architectures\n\nWe extend our analysis of the relationship between memorization and reconstruction performance to larger, contemporary model architectures—specifically, transformer-based models that are widely used in modern machine learning practice. Our experiments with Swin [\\[40\\]](#page-15-13) and MAE [\\[25\\]](#page-15-14) architectures, as presented in [Table 8](#page-12-0) and [Table 12,](#page-20-4) reveal consistent trends: models trained on smaller datasets tend to be more vulnerable, in line with our previous findings.\n\nWe also observe that the worst-case performance does not occur at the smallest dataset size. This can be attributed to the nature of transformer-based models, which typically require substantially larger datasets to train effectively. As the dataset grows, the model begins to capture sample-specific features, leading to increased unnecessary memorization. This nuanced behavior aligns with our analysis in [Section 6.1.](#page-9-0)\n\n## J Related Work\n\n## J.1 Membership Inference Attack\n\nMembership inference attack reveals the membership status of a target sample, i.e., whether the target sample is in the <span id=\"page-21-0\"></span>training dataset or not, which leads to a direct privacy breach.\n\nShokri et al. [\\[52\\]](#page-16-1) proposed the seminal work on membership inference attack against machine learning models, wherein several shadow models were trained to imitate the behavior of the target model. This attack requires access to data from the same distribution as the training dataset. Later, Salem et al. [\\[49\\]](#page-16-0) relax the assumption of the same distribution and demonstrate the effectiveness of using only one shadow model, largely reducing the computational cost required. Subsequent research [\\[15,](#page-14-19) [36\\]](#page-15-4) explores a more challenging setting where the adversary only has hard-label access to the target model. Specifically, Li and Zhang [\\[36\\]](#page-15-4) utilize adversary examples to approximate the distance between the target sample to its decision boundary in order to make decisions based on this distance. Recently, more work [\\[9,](#page-14-4) [38,](#page-15-7) [61\\]](#page-16-7) aims at enhancing the performance of membership inference attacks. For example, Carlini et al. [\\[9\\]](#page-14-4) take advantage of the discrepancy of models trained with and without the target sample. Liu et al. [\\[38\\]](#page-15-7) demonstrate the effectiveness of loss trajectory.\n\n## J.2 Other Privacy Attacks\n\nProperty inference attack differs from data reconstruction and membership inference attack as it aims to infer macrolevel information about the target dataset, such as the gender proportion. Ateniese et al. [\\[5\\]](#page-14-20) presented the first property inference attacks against Hidden Markov Models (HMMs) and Support Vector Machine (SVM), which was later extended to Fully Connected Neural Networks (FCNNs) by Ganju et al. [\\[21\\]](#page-14-21). Both attacks rely on a meta-classifier to infer the property of the training dataset using white-box access to the target model. Training the meta-classifier requires multiple shadow models, making it computationally expensive.\n\nSuri and Evans [\\[59\\]](#page-16-16) first formalized the property inference attack and provided a method for conducting the attack with black-box access to the target model. Subsequent research [\\[13](#page-14-22)[,43\\]](#page-15-20), has focused on improving the performance of property inference attacks by adding a small amount of \"poisoned\" data to the training dataset. For example, Chaudhari et al. [\\[13\\]](#page-14-22) select a limited number of samples in one class, and flip their labels to increase the discrepancy of posterior distributions for different properties.\n\nAdditionally, model stealing attacks aim to construct a local surrogate model from the target model. Tramèr et al. [\\[62\\]](#page-16-17) propose the first attack against neural networks by querying the target model to construct the training dataset. However, their attack demands high-quality data to be effective, which motivates recent research to relax this assumption through the development of data-free attack paradigms [\\[29,](#page-15-21) [51,](#page-16-18) [63\\]](#page-16-8).\n\n## J.3 Memorization and Privacy Leakage\n\nSong et al. [\\[55\\]](#page-16-19) demonstrate that malicious trainers can easily encode training samples into the model parameters, which can later be extracted. This highlights the potential threats for model leakage and emphasizes the need for researchers to consider the security implications of memorization in their models. Song and Shmatikov [\\[56\\]](#page-16-20) further develop this point by uncovering the intrinsic behavior of models, specifically their tendency to retain information that is not pertinent to the designed classification task. Despite attempts to eliminate this extraneous information, the unintentional disclosure of sensitive data remains a persistent issue.\n\nSeveral studies have leveraged memorization to enhance the attack performance. Tramèr et al. [\\[61\\]](#page-16-7) show that with access to a tiny fraction of the training dataset, the adversary can boost the performance of membership inference by poisoning data samples as the memorization of these poisoned samples increases. Wen et al. [\\[66\\]](#page-16-21) investigate the relationship between data importance (used as a proxy for memorization) and privacy attacks, observing that data samples with higher importance exhibit increased vulnerability to certain attacks.\n\n<span id=\"page-22-1\"></span>![](_page_22_Figure_0.jpeg)\n\nFigure 13: Influence of reconstruction quantity. For ease of presentation, values are normalized to fit the same y-axis.\n\n<span id=\"page-22-0\"></span>![](_page_22_Figure_2.jpeg)\n\nFigure 14: More results on the influence of auxiliary information.\n\n| Attack        | Metrics       |                           |                                                     |                                                  | Target Data Size                                 |                                                  |                                                  |                                                  |\n|---------------|---------------|---------------------------|-----------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|\n|               |               |                           | 1,000                                               | 2,000                                            | 5,000                                            | 10,000                                           | 15,000                                           | 20,000                                           |\n|               | Dataset-level | FID ↓                     | 359.896                                             | 357.629                                          | 348.142                                          | 364.073                                          | 366.656                                          | 360.530                                          |\n| MI-Face       | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.013(100.00%)<br>8.292(100.00%)<br>0.152(100.00%)  | 0.017(70.65%)<br>8.921(53.70%)<br>0.130(53.70%)  | 0.021(59.00%)<br>8.977(26.72%)<br>0.128(26.72%)  | 0.023(54.89%)<br>9.444(14.25%)<br>0.115(14.25%)  | 0.023(54.49%)<br>9.526(10.59%)<br>0.112(10.59%)  | 0.022(55.33%)<br>9.800(9.31%)<br>0.106(9.31%)    |\n|               | Dataset-level | FID ↓                     | 344.805                                             | 365.463                                          | 357.851                                          | 368.678                                          | 344.333                                          | 320.711                                          |\n| DeepDream     | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.055(100.00%)<br>9.094(100.00%)<br>0.130(100.00%)  | 0.069(68.80%)<br>9.633(60.25%)<br>0.114(60.25%)  | 0.099(53.52%)<br>10.297(35.94%)<br>0.096(35.94%) | 0.193(29.45%)<br>12.225(19.37%)<br>0.062(19.37%) | 0.191(24.78%)<br>12.523(12.92%)<br>0.058(12.92%) | 0.209(17.25%)<br>12.816(8.57%)<br>0.054(8.57%)   |\n|               | Dataset-level | FID ↓                     | 132.130                                             | 118.508                                          | 102.343                                          | 98.582                                           | 96.802                                           | 95.217                                           |\n| Revealer      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.091(100.00%)<br>8.925(100.00%)<br>0.139(100.00%)  | 0.112(66.60%)<br>9.667(61.25%)<br>0.115(61.25%)  | 0.136(52.16%)<br>10.371(45.28%)<br>0.096(45.28%) | 0.152(44.63%)<br>10.768(37.97%)<br>0.087(37.97%) | 0.159(41.72%)<br>10.883(34.36%)<br>0.085(34.36%) | 0.163(39.43%)<br>10.892(32.76%)<br>0.085(32.76%) |\n|               | Dataset-level | FID ↓                     | 309.533                                             | 299.998                                          | 335.177                                          | 269.722                                          | 318.032                                          | 335.431                                          |\n| Inv-Alignment | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.208(100.00%)<br>11.401(100.00%)<br>0.080(100.00%) | 0.319(52.15%)<br>12.460(52.00%)<br>0.062(52.00%) | 0.343(22.76%)<br>13.333(25.38%)<br>0.049(25.38%) | 0.373(16.99%)<br>14.135(21.95%)<br>0.040(21.95%) | 0.387(11.48%)<br>14.358(14.65%)<br>0.038(14.65%) | 0.362(7.32%)<br>14.379(9.44%)<br>0.038(9.44%)    |\n|               | Dataset-level | FID ↓                     | 349.122                                             | 341.570                                          | 337.992                                          | 338.623                                          | 338.630                                          | 338.034                                          |\n| Bias-Rec      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.045(32.00%)<br>10.310(2.80%)<br>0.093(2.80%)      | 0.047(25.40%)<br>10.710(0.50%)<br>0.085(0.50%)   | 0.046(27.46%)<br>10.770(0.72%)<br>0.084(0.72%)   | 0.049(24.16%)<br>10.678(0.55%)<br>0.086(0.55%)   | 0.048(22.29%)<br>10.627(0.57%)<br>0.087(0.57%)   | 0.050(21.88%)<br>10.772(0.54%)<br>0.085(0.54%)   |\n|               | Dataset-level | FID ↓                     | 119.894                                             | 104.382                                          | 97.362                                           | 97.670                                           | 95.522                                           | 96.972                                           |\n| KEDMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.107(100.00%)<br>8.992(100.00%)<br>0.138(100.00%)  | 0.128(57.55%)<br>9.889(55.20%)<br>0.110(55.20%)  | 0.145(32.30%)<br>10.678(29.00%)<br>0.090(29.00%) | 0.159(21.81%)<br>11.140(18.31%)<br>0.081(18.31%) | 0.168(16.55%)<br>11.451(14.99%)<br>0.075(14.99%) | 0.169(15.21%)<br>11.576(13.37%)<br>0.073(13.37%) |\n|               | Dataset-level | FID ↓                     | 134.842                                             | 103.134                                          | 97.194                                           | 80.272                                           | 80.102                                           | 77.295                                           |\n| PLGMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.109(100.00%)<br>9.199(100.00%)<br>0.129(100.00%)  | 0.128(63.35%)<br>9.836(59.00%)<br>0.110(59.00%)  | 0.142(43.34%)<br>10.419(35.30%)<br>0.096(35.30%) | 0.163(30.92%)<br>11.108(25.19%)<br>0.082(25.19%) | 0.168(25.47%)<br>11.452(19.48%)<br>0.075(19.48%) | 0.170(22.99%)<br>11.649(17.17%)<br>0.071(17.17%) |\n|               | Dataset-level | FID ↓                     | 172.760                                             | 195.664                                          | 169.448                                          | 186.883                                          | 245.177                                          | 282.291                                          |\n| Updates-Leak  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.289(41.00%)<br>12.878(38.00%)<br>0.054(38.00%)    | 0.298(37.00%)<br>13.278(29.00%)<br>0.050(29.00%) | 0.276(40.00%)<br>12.930(39.00%)<br>0.053(39.00%) | 0.241(36.00%)<br>11.879(33.00%)<br>0.067(33.00%) | 0.263(18.00%)<br>12.429(16.00%)<br>0.061(16.00%) | 0.267(13.00%)<br>12.902(14.00%)<br>0.052(14.00%) |\n|               | Dataset-level | FID ↓                     | 383.297                                             | 387.471                                          | 385.031                                          | 383.957                                          | 388.319                                          | 380.184                                          |\n| Deep-Leakage  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.030(46.00%)<br>10.961(2.00%)<br>0.080(2.00%)      | 0.030(42.00%)<br>10.966(2.00%)<br>0.080(2.00%)   | 0.030(47.00%)<br>10.975(2.00%)<br>0.080(2.00%)   | 0.030(47.00%)<br>10.969(2.00%)<br>0.080(2.00%)   | 0.030(46.00%)<br>10.978(2.00%)<br>0.080(2.00%)   | 0.030(44.00%)<br>10.958(2.00%)<br>0.080(2.00%)   |\n\nTable 13: Evaluation results of existing reconstruction attacks on larger models. The target model is Swin Transformer trained on CelebA with 6 different sizes.\n\n|                                                                                  | Metrics       |                           |                                                     |                                                  | Target Data Size                                 |                                                  |                                                  |                                                  |\n|----------------------------------------------------------------------------------|---------------|---------------------------|-----------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|\n| Attack<br>MI-Face<br>DeepDream<br>Revealer<br>Inv-Alignment<br>Bias-Rec<br>KEDMI |               |                           | 1,000                                               | 2,000                                            | 5,000                                            | 10,000                                           | 15,000                                           | 20,000                                           |\n|                                                                                  | Dataset-level | FID ↓                     | 325.226                                             | 304.771                                          | 311.452                                          | 322.519                                          | 337.842                                          | 351.695                                          |\n|                                                                                  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.014(100.00%)<br>8.191(100.00%)<br>0.160(100.00%)  | 0.018(70.05%)<br>8.750(55.85%)<br>0.138(55.85%)  | 0.021(57.19%)<br>9.188(29.80%)<br>0.123(29.80%)  | 0.026(49.44%)<br>9.737(16.51%)<br>0.107(16.51%)  | 0.025(49.80%)<br>9.823(13.58%)<br>0.106(13.58%)  | 0.022(49.90%)<br>9.541(9.24%)<br>0.104(9.24%)    |\n|                                                                                  | Dataset-level | FID ↓                     | 288.219                                             | 258.241                                          | 321.577                                          | 303.685                                          | 289.375                                          | 313.243                                          |\n|                                                                                  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.063(100.00%)<br>9.053(100.00%)<br>0.137(100.00%)  | 0.131(58.35%)<br>8.612(56.20%)<br>0.154(56.20%)  | 0.056(49.86%)<br>10.238(27.80%)<br>0.099(27.80%) | 0.102(35.16%)<br>10.597(15.73%)<br>0.095(15.73%) | 0.228(18.79%)<br>12.331(11.76%)<br>0.063(11.76%) | 0.199(19.16%)<br>12.684(8.35%)<br>0.059(8.35%)   |\n|                                                                                  | Dataset-level | FID ↓                     | 167.208                                             | 128.094                                          | 110.680                                          | 95.625                                           | 92.578                                           | 92.221                                           |\n|                                                                                  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.126(100.00%)<br>7.996(100.00%)<br>0.179(100.00%)  | 0.138(68.60%)<br>8.991(68.85%)<br>0.138(68.85%)  | 0.152(52.62%)<br>9.707(50.50%)<br>0.114(50.50%)  | 0.154(48.16%)<br>10.202(45.43%)<br>0.100(45.43%) | 0.160(45.10%)<br>10.373(40.76%)<br>0.096(40.76%) | 0.162(41.83%)<br>10.540(38.17%)<br>0.092(38.17%) |\n|                                                                                  | Dataset-level | FID ↓                     | 345.477                                             | 325.461                                          | 323.843                                          | 349.254                                          | 381.913                                          | 369.680                                          |\n|                                                                                  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.262(100.00%)<br>11.015(100.00%)<br>0.088(100.00%) | 0.309(51.00%)<br>12.323(51.95%)<br>0.063(51.95%) | 0.336(21.34%)<br>13.429(23.20%)<br>0.048(23.20%) | 0.355(11.13%)<br>14.220(11.43%)<br>0.039(11.43%) | 0.359(7.73%)<br>14.715(7.67%)<br>0.035(7.67%)    | 0.383(5.71%)<br>14.921(5.87%)<br>0.034(5.87%)    |\n|                                                                                  | Dataset-level | FID ↓                     | 350.907                                             | 343.026                                          | 338.143                                          | 339.478                                          | 338.239                                          | 339.468                                          |\n|                                                                                  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.045(31.70%)<br>10.306(2.70%)<br>0.093(2.70%)      | 0.045(28.65%)<br>10.706(0.55%)<br>0.086(0.55%)   | 0.048(24.14%)<br>10.708(0.58%)<br>0.085(0.58%)   | 0.047(25.41%)<br>10.748(0.59%)<br>0.084(0.59%)   | 0.049(23.52%)<br>10.691(0.54%)<br>0.086(0.54%)   | 0.050(22.90%)<br>10.787(0.43%)<br>0.083(0.43%)   |\n|                                                                                  | Dataset-level | FID ↓                     | 136.602                                             | 128.622                                          | 109.811                                          | 104.660                                          | 102.369                                          | 99.232                                           |\n|                                                                                  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.124(100.00%)<br>9.210(100.00%)<br>0.134(100.00%)  | 0.144(61.50%)<br>9.972(59.30%)<br>0.109(59.30%)  | 0.152(41.25%)<br>10.544(38.18%)<br>0.094(38.18%) | 0.163(26.99%)<br>11.215(23.12%)<br>0.080(23.12%) | 0.172(22.83%)<br>11.506(18.63%)<br>0.075(18.63%) | 0.173(19.34%)<br>11.627(16.33%)<br>0.074(16.33%) |\n|                                                                                  | Dataset-level | FID ↓                     | 196.432                                             | 153.114                                          | 137.388                                          | 96.311                                           | 90.159                                           | 85.646                                           |\n| PLGMI                                                                            | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.131(100.00%)<br>8.788(100.00%)<br>0.143(100.00%)  | 0.154(65.05%)<br>9.130(62.25%)<br>0.134(62.25%)  | 0.153(46.90%)<br>9.486(41.76%)<br>0.122(41.76%)  | 0.155(32.80%)<br>10.891(25.11%)<br>0.086(25.11%) | 0.165(26.13%)<br>11.347(18.60%)<br>0.077(18.60%) | 0.167(24.48%)<br>11.456(17.59%)<br>0.076(17.59%) |\n|                                                                                  | Dataset-level | FID ↓                     | 386.663                                             | 265.855                                          | 276.238                                          | 320.563                                          | 248.002                                          | 265.317                                          |\n| Updates-Leak                                                                     | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.227(33.00%)<br>11.932(22.00%)<br>0.066(22.00%)    | 0.253(21.00%)<br>12.349(20.00%)<br>0.060(20.00%) | 0.297(13.00%)<br>12.715(12.00%)<br>0.056(12.00%) | 0.302(12.00%)<br>12.893(11.00%)<br>0.054(11.00%) | 0.295(9.00%)<br>12.766(8.00%)<br>0.057(8.00%)    | 0.306(7.00%)<br>12.933(6.00%)<br>0.054(6.00%)    |\n|                                                                                  | Dataset-level | FID ↓                     | 381.325                                             | 383.118                                          | 385.002                                          | 380.343                                          | 383.609                                          | 390.253                                          |\n| Deep-Leakage                                                                     | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.030(42.00%)<br>10.952(2.00%)<br>0.081(2.00%)      | 0.030(43.00%)<br>10.961(2.00%)<br>0.082(2.00%)   | 0.031(45.00%)<br>10.961(2.00%)<br>0.080(2.00%)   | 0.030(41.00%)<br>10.973(2.00%)<br>0.081(2.00%)   | 0.030(44.00%)<br>10.988(2.00%)<br>0.080(2.00%)   | 0.030(43.00%)<br>10.969(2.00%)<br>0.080(2.00%)   |\n\nTable 14: Evaluation results of existing reconstruction attacks on larger models. The target model is Masked AutoEncoder (MAE) trained on CelebA with 6 different sizes.\n\n| Attack        | Metrics       |        | Target Data Size |                |                |                |                |                |  |  |\n|---------------|---------------|--------|------------------|----------------|----------------|----------------|----------------|----------------|--|--|\n|               |               |        | 100              | 500            | 1,000          | 5,000          | 10,000         | 20,000         |  |  |\n|               | Memorization  |        | 0.970            | 0.800          | 0.760          | 0.460          | 0.390          | 0.310          |  |  |\n|               | Dataset-level | FID ↓  | 391.454          | 348.294        | 334.498        | 302.592        | 315.456        | 319.278        |  |  |\n| MI-Face       |               | SSIM ↑ | 0.028(76.00%)    | 0.042(59.20%)  | 0.050(53.40%)  | 0.060(47.00%)  | 0.064(46.54%)  | 0.065(44.27%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 9.591(57.00%)    | 9.896(19.20%)  | 10.425(12.80%) | 11.414(3.44%)  | 11.887(1.94%)  | 11.947(1.34%)  |  |  |\n|               |               | MSE ↓  | 0.113(57.00%)    | 0.104(19.20%)  | 0.092(12.80%)  | 0.073(3.44%)   | 0.065(1.94%)   | 0.064(1.34%)   |  |  |\n|               | Dataset-level | FID ↓  | 295.639          | 288.325        | 265.255        | 256.581        | 250.863        | 251.972        |  |  |\n| DeepDream     |               | SSIM ↑ | 0.229(54.00%)    | 0.310(17.20%)  | 0.333(10.40%)  | 0.424(2.28%)   | 0.457(1.36%)   | 0.495(0.74%)   |  |  |\n|               | Sample-level  | PSNR ↑ | 13.172(51.00%)   | 15.515(18.80%) | 16.247(13.20%) | 17.736(2.60%)  | 18.840(1.43%)  | 19.319(0.73%)  |  |  |\n|               |               | MSE ↓  | 0.056(51.00%)    | 0.034(18.80%)  | 0.027(13.20%)  | 0.018(2.60%)   | 0.014(1.43%)   | 0.012(0.73%)   |  |  |\n|               | Dataset-level | FID ↓  | 261.342          | 204.975        | 157.732        | 123.866        | 125.015        | 122.208        |  |  |\n| DeepInversion |               | SSIM ↑ | 0.123(63.00%)    | 0.141(41.80%)  | 0.159(41.20%)  | 0.179(27.90%)  | 0.190(22.22%)  | 0.203(17.13%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 11.287(62.00%)   | 11.601(31.80%) | 11.777(26.10%) | 11.853(16.00%) | 12.021(11.75%) | 12.236(7.23%)  |  |  |\n|               |               | MSE ↓  | 0.082(62.00%)    | 0.077(31.80%)  | 0.074(26.10%)  | 0.069(16.00%)  | 0.066(11.75%)  | 0.061(7.23%)   |  |  |\n|               | Dataset-level | FID ↓  | 287.236          | 218.823        | 200.024        | 165.886        | 163.183        | 153.322        |  |  |\n| Revealer      |               | SSIM ↑ | 0.119(75.00%)    | 0.146(52.20%)  | 0.162(49.50%)  | 0.189(40.56%)  | 0.198(38.88%)  | 0.208(34.72%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 10.764(64.00%)   | 11.609(41.00%) | 12.063(37.30%) | 12.548(28.08%) | 12.808(25.50%) | 12.781(23.53%) |  |  |\n|               |               | MSE ↓  | 0.098(64.00%)    | 0.074(41.00%)  | 0.066(37.30%)  | 0.060(28.08%)  | 0.056(25.50%)  | 0.057(23.53%)  |  |  |\n|               | Dataset-level | FID ↓  | 487.186          | 411.613        | 407.791        | 523.798        | 432.829        | 342.285        |  |  |\n| Inv-Alignment |               | SSIM ↑ | 0.132(50.00%)    | 0.212(11.80%)  | 0.248(7.80%)   | 0.302(1.68%)   | 0.327(0.84%)   | 0.349(0.49%)   |  |  |\n|               | Sample-level  | PSNR ↑ | 12.676(50.00%)   | 14.852(12.00%) | 15.902(6.40%)  | 16.692(1.68%)  | 17.104(0.85%)  | 17.732(0.54%)  |  |  |\n|               |               | MSE ↓  | 0.063(50.00%)    | 0.036(12.00%)  | 0.028(6.40%)   | 0.022(1.68%)   | 0.020(0.85%)   | 0.017(0.54%)   |  |  |\n|               | Dataset-level | FID ↓  | 412.151          | 348.602        | 340.031        | 332.172        | 329.594        | 333.383        |  |  |\n| Bias-Rec      |               | SSIM ↑ | 0.054(55.00%)    | 0.080(42.00%)  | 0.084(38.40%)  | 0.094(30.64%)  | 0.100(26.89%)  | 0.105(23.32%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 12.322(2.00%)    | 11.390(3.00%)  | 11.402(3.50%)  | 11.539(2.24%)  | 11.501(1.93%)  | 11.624(1.61%)  |  |  |\n|               |               | MSE ↓  | 0.059(2.00%)     | 0.073(3.00%)   | 0.073(3.50%)   | 0.071(2.24%)   | 0.072(1.93%)   | 0.070(1.61%)   |  |  |\n|               | Dataset-level | FID ↓  | 270.267          | 193.301        | 152.543        | 103.166        | 105.810        | 97.692         |  |  |\n| KEDMI         |               | SSIM ↑ | 0.143(64.00%)    | 0.174(38.60%)  | 0.203(30.90%)  | 0.240(21.18%)  | 0.252(18.94%)  | 0.254(17.34%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 10.810(61.00%)   | 11.342(34.00%) | 11.846(27.70%) | 11.531(20.10%) | 11.975(16.75%) | 11.603(15.20%) |  |  |\n|               |               | MSE ↓  | 0.096(61.00%)    | 0.081(34.00%)  | 0.071(27.70%)  | 0.076(20.10%)  | 0.069(16.75%)  | 0.075(15.20%)  |  |  |\n|               | Dataset-level | FID ↓  | 242.909          | 144.436        | 149.024        | 145.562        | 110.711        | 67.610         |  |  |\n| PLGMI         |               | SSIM ↑ | 0.167(69.00%)    | 0.185(53.40%)  | 0.204(47.30%)  | 0.235(41.76%)  | 0.212(1.79%)   | 0.219(0.82%)   |  |  |\n|               | Sample-level  | PSNR ↑ | 12.275(70.00%)   | 12.749(45.80%) | 13.007(41.50%) | 13.385(30.86%) | 13.722(1.05%)  | 13.646(0.49%)  |  |  |\n|               |               | MSE ↓  | 0.067(70.00%)    | 0.058(45.80%)  | 0.054(41.50%)  | 0.049(30.86%)  | 0.044(1.05%)   | 0.045(0.49%)   |  |  |\n|               | Dataset-level | FID ↓  | 288.321          | 281.211        | 298.998        | 292.945        | 304.711        | 284.351        |  |  |\n| Updates-Leak  |               | SSIM ↑ | 0.152(33.00%)    | 0.168(43.00%)  | 0.156(27.00%)  | 0.160(39.00%)  | 0.151(21.00%)  | 0.177(34.00%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 11.633(27.00%)   | 12.480(27.00%) | 12.104(21.00%) | 12.115(23.00%) | 11.910(18.00%) | 12.230(30.00%) |  |  |\n|               |               | MSE ↓  | 0.073(27.00%)    | 0.060(27.00%)  | 0.068(21.00%)  | 0.065(23.00%)  | 0.067(18.00%)  | 0.065(30.00%)  |  |  |\n|               | Dataset-level | FID ↓  | 408.130          | 410.365        | 401.905        | 410.773        | 413.346        | 418.941        |  |  |\n| Deep-Leakage  |               | SSIM ↑ | 0.055(61.00%)    | 0.055(53.00%)  | 0.056(56.00%)  | 0.054(53.00%)  | 0.054(59.00%)  | 0.055(57.00%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 12.551(2.00%)    | 12.579(2.00%)  | 12.550(2.00%)  | 12.526(2.00%)  | 12.504(2.00%)  | 12.480(2.00%)  |  |  |\n|               |               | MSE ↓  | 0.056(2.00%)     | 0.055(2.00%)   | 0.056(2.00%)   | 0.056(2.00%)   | 0.056(2.00%)   | 0.057(2.00%)   |  |  |\n\nTable 15: Evaluation results of existing reconstruction attacks. The target model is VGG16 trained on CIFAR100 with 6 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance.\n\n| Attack        | Metrics       |                  | No Defense                       |                                 | MID                              |                                 | DP                               |                                 |\n|---------------|---------------|------------------|----------------------------------|---------------------------------|----------------------------------|---------------------------------|----------------------------------|---------------------------------|\n|               |               |                  | 1,000                            | 20,000                          | 1,000                            | 20,000                          | 1,000                            | 20,000                          |\n|               | Dataset-level | FID ↓            | 362.361                          | 336.906                         | 370.543                          | 363.668                         | 378.974                          | 370.767                         |\n| MI-Face       |               | SSIM ↑           | 0.014(100.00%)                   | 0.023(54.80%)                   | 0.015(100.00%)                   | 0.027(56.02%)                   | 0.010(100.00%)                   | 0.021(48.32%)                   |\n|               | Sample-level  | PSNR ↑<br>MSE ↓  | 7.978(100.00%)<br>0.163(100.00%) | 9.037(10.08%)<br>0.126(10.08%)  | 8.066(100.00%)<br>0.169(100.00%) | 9.018(9.82%)<br>0.126(9.82%)    | 7.868(100.00%)<br>0.171(100.00%) | 9.686(6.69%)<br>0.128(6.69%)    |\n|               | Dataset-level | FID ↓            | 337.139                          | 370.396                         | 373.527                          | 388.262                         | 365.830                          | 352.024                         |\n| DeepDream     |               | SSIM ↑           | 0.079(100.00%)                   | 0.346(6.81%)                    | 0.020(100.00%)                   | 0.150(27.58%)                   | 0.020(100.00%)                   | 0.149(23.12%)                   |\n|               | Sample-level  | PSNR ↑           | 9.162(100.00%)                   | 14.027(7.63%)                   | 8.486(100.00%)                   | 11.847(9.20%)                   | 9.153(100.00%)                   | 10.221(6.18%)                   |\n|               |               | MSE ↓            | 0.128(100.00%)                   | 0.041(7.63%)                    | 0.145(100.00%)                   | 0.067(9.20%)                    | 0.125(100.00%)                   | 0.096(6.18%)                    |\n|               | Dataset-level | FID ↓            | 287.497                          | 234.672                         | 347.005                          | 295.533                         | 393.327                          | 352.239                         |\n| DeepInversion |               | SSIM ↑           | 0.100(100.00%)                   | 0.153(22.47%)                   | 0.134(100.00%)                   | 0.099(31.06%)                   | 0.038(100.00%)                   | 0.031(27.93%)                   |\n|               | Sample-level  | PSNR ↑           | 9.676(100.00%)                   | 11.343(15.73%)                  | 11.602(100.00%)                  | 11.472(24.25%)                  | 10.059(100.00%)                  | 11.613(6.05%)                   |\n|               |               | MSE ↓            | 0.119(100.00%)                   | 0.077(15.73%)                   | 0.077(100.00%)                   | 0.074(24.25%)                   | 0.104(100.00%)                   | 0.070(6.05%)                    |\n|               | Dataset-level | FID ↓            | 116.712                          | 92.982                          | 123.963                          | 95.481                          | 143.723                          | 97.973                          |\n| Revealer      |               | SSIM ↑           | 0.101(100.00%)                   | 0.162(38.33%)                   | 0.100(100.00%)                   | 0.144(36.19%)                   | 0.106(100.00%)                   | 0.156(32.58%)                   |\n|               | Sample-level  | PSNR ↑           | 9.144(100.00%)                   | 10.733(32.17%)                  | 9.215(100.00%)                   | 11.146(30.42%)                  | 7.700(100.00%)                   | 11.347(27.68%)                  |\n|               |               | MSE ↓            | 0.132(100.00%)                   | 0.088(32.17%)                   | 0.131(100.00%)                   | 0.080(30.42%)                   | 0.185(100.00%)                   | 0.092(27.68%)                   |\n|               | Dataset-level | FID ↓            | 344.049                          | 357.910                         | 348.905                          | 250.018                         | 353.076                          | 296.621                         |\n| Inv-Alignment |               | SSIM ↑           | 0.255(100.00%)                   | 0.328(7.89%)                    | 0.272(100.00%)                   | 0.448(5.46%)                    | 0.272(100.00%)                   | 0.426(10.32%)                   |\n|               | Sample-level  | PSNR ↑           | 11.292(100.00%)                  | 14.253(8.02%)                   | 11.466(100.00%)                  | 15.077(5.47%)                   | 11.372(100.00%)                  | 14.732(11.51%)                  |\n|               |               | MSE ↓            | 0.081(100.00%)                   | 0.039(8.02%)                    | 0.077(100.00%)                   | 0.032(5.47%)                    | 0.079(100.00%)                   | 0.035(11.51%)                   |\n|               | Dataset-level | FID ↓            | 327.883                          | 315.227                         | 347.965                          | 336.897                         | 348.310                          | 336.834                         |\n| Bias-Rec      |               | SSIM ↑           | 0.039(38.20%)                    | 0.047(24.28%)                   | 0.046(31.10%)                    | 0.052(22.82%)                   | 0.045(31.50%)                    | 0.050(21.49%)                   |\n|               | Sample-level  | PSNR ↑           | 9.783(3.20%)                     | 10.354(0.57%)                   | 10.273(2.90%)                    | 10.672(0.51%)                   | 10.273(2.60%)                    | 10.716(0.41%)                   |\n|               |               | MSE ↓            | 0.105(3.20%)                     | 0.093(0.57%)                    | 0.094(2.90%)                     | 0.086(0.51%)                    | 0.094(2.60%)                     | 0.085(0.41%)                    |\n|               | Dataset-level | FID ↓            | 121.689                          | 97.667                          | 133.162                          | 97.577                          | 128.606                          | 94.335                          |\n| KEDMI         |               | SSIM ↑           | 0.111(100.00%)                   | 0.167(18.93%)                   | 0.099(100.00%)                   | 0.151(16.22%)                   | 0.095(100.00%)                   | 0.143(15.03%)                   |\n|               | Sample-level  | PSNR ↑           | 9.167(100.00%)                   | 11.560(15.36%)                  | 8.648(100.00%)                   | 11.430(13.19%)                  | 8.529(100.00%)                   | 11.637(12.79%)                  |\n|               |               | MSE ↓            | 0.133(100.00%)                   | 0.073(15.36%)                   | 0.154(100.00%)                   | 0.076(13.19%)                   | 0.157(100.00%)                   | 0.072(12.79%)                   |\n|               | Dataset-level | FID ↓            | 127.722                          | 85.143                          | 108.067                          | 90.279                          | 173.735                          | 119.856                         |\n| PLGMI         |               | SSIM ↑           | 0.110(100.00%)                   | 0.161(18.54%)                   | 0.107(100.00%)                   | 0.154(21.47%)                   | 0.106(100.00%)                   | 0.152(25.88%)                   |\n|               | Sample-level  | PSNR ↑<br>MSE ↓  | 9.448(100.00%)<br>0.122(100.00%) | 11.513(13.54%)<br>0.074(13.54%) | 9.055(100.00%)<br>0.131(100.00%) | 11.448(15.99%)<br>0.075(15.99%) | 6.800(100.00%)<br>0.234(100.00%) | 10.445(23.31%)<br>0.096(23.31%) |\n|               |               |                  |                                  |                                 |                                  |                                 |                                  |                                 |\n|               | Dataset-level | FID ↓            | 192.446                          | 260.100                         | 168.452                          | 177.857                         | 328.838                          | 176.782                         |\n| Updates-Leak  |               | SSIM ↑           | 0.203(18.00%)                    | 0.187(5.00%)                    | 0.282(48.00%)                    | 0.293(42.00%)                   | 0.208(23.00%)                    | 0.283(43.00%)                   |\n|               | Sample-level  | PSNR ↑<br>MSE ↓  | 13.173(14.00%)<br>0.049(14.00%)  | 12.509(6.00%)<br>0.058(6.00%)   | 12.794(38.00%)<br>0.054(38.00%)  | 12.861(39.00%)<br>0.054(39.00%) | 12.001(18.00%)<br>0.066(18.00%)  | 13.074(40.00%)<br>0.051(40.00%) |\n|               |               | FID ↓            | 376.852                          | 383.338                         | 384.783                          | 389.193                         | 375.418                          | 384.563                         |\n|               | Dataset-level |                  |                                  |                                 |                                  |                                 |                                  |                                 |\n| Deep-Leakage  | Sample-level  | SSIM ↑<br>PSNR ↑ | 0.031(49.00%)<br>10.957(3.00%)   | 0.040(52.00%)<br>11.199(2.00%)  | 0.030(45.00%)<br>10.968(2.00%)   | 0.030(45.00%)<br>10.964(2.00%)  | 0.094(42.00%)<br>11.519(3.00%)   | 0.030(44.00%)<br>10.953(2.00%)  |\n|               |               | MSE ↓            | 0.080(3.00%)                     | 0.076(2.00%)                    | 0.080(2.00%)                     | 0.080(2.00%)                    | 0.071(3.00%)                     | 0.080(2.00%)                    |\n\n<span id=\"page-26-0\"></span>Table 16: Evaluation results of existing reconstruction attacks with defenses MID and DP. The target model is VGG16 trained on CelebA with 2 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance.\n\n| Attack        | Metrics       |                           | Target Data Size                                    |                                                  |                                                  |                                                  |                                                  |                                                  |  |  |\n|---------------|---------------|---------------------------|-----------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--|--|\n|               |               |                           | 1,000                                               | 2,000                                            | 5,000                                            | 10,000                                           | 15,000                                           | 20,000                                           |  |  |\n|               | Memorization  |                           | 1.000                                               | 0.987                                            | 0.864                                            | 0.626                                            | 0.474                                            | 0.377                                            |  |  |\n|               | Dataset-level | FID ↓                     | 366.886                                             | 371.293                                          | 359.870                                          | 365.360                                          | 352.610                                          | 361.813                                          |  |  |\n| MI-Face       | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.010(100.00%)<br>8.047(100.00%)<br>0.161(100.00%)  | 0.015(72.45%)<br>8.283(53.15%)<br>0.151(53.15%)  | 0.019(63.14%)<br>8.683(26.08%)<br>0.137(26.08%)  | 0.020(60.78%)<br>9.178(15.79%)<br>0.122(15.79%)  | 0.020(58.72%)<br>9.242(11.93%)<br>0.120(11.93%)  | 0.020(57.73%)<br>9.562(8.54%)<br>0.112(8.54%)    |  |  |\n|               | Dataset-level | FID ↓                     | 343.531                                             | 332.206                                          | 352.673                                          | 351.912                                          | 356.788                                          | 349.529                                          |  |  |\n| DeepDream     | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.013(100.00%)<br>8.533(100.00%)<br>0.143(100.00%)  | 0.021(69.85%)<br>8.778(52.85%)<br>0.135(52.85%)  | 0.093(47.20%)<br>10.462(25.68%)<br>0.093(25.68%) | 0.092(40.80%)<br>11.222(14.70%)<br>0.078(14.70%) | 0.070(41.07%)<br>11.276(9.49%)<br>0.076(9.49%)   | 0.097(33.32%)<br>11.911(7.34%)<br>0.066(7.34%)   |  |  |\n|               | Dataset-level | FID ↓                     | 344.519                                             | 337.954                                          | 323.117                                          | 341.752                                          | 325.206                                          | 328.909                                          |  |  |\n| DeepInversion | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.020(100.00%)<br>8.958(100.00%)<br>0.133(100.00%)  | 0.021(70.85%)<br>9.304(55.00%)<br>0.120(55.00%)  | 0.050(47.90%)<br>9.131(30.82%)<br>0.125(30.82%)  | 0.045(39.55%)<br>9.685(19.89%)<br>0.109(19.89%)  | 0.048(40.87%)<br>9.968(14.49%)<br>0.102(14.49%)  | 0.057(34.84%)<br>10.820(8.92%)<br>0.084(8.92%)   |  |  |\n|               | Dataset-level | FID ↓                     | 121.003                                             | 108.319                                          | 98.901                                           | 97.173                                           | 93.785                                           | 93.032                                           |  |  |\n| Revealer      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.105(100.00%)<br>8.992(100.00%)<br>0.138(100.00%)  | 0.128(67.90%)<br>9.465(65.00%)<br>0.121(65.00%)  | 0.143(53.12%)<br>10.054(49.60%)<br>0.104(49.60%) | 0.157(46.55%)<br>10.324(43.63%)<br>0.097(43.63%) | 0.161(42.89%)<br>10.469(38.28%)<br>0.094(38.28%) | 0.166(40.97%)<br>10.534(36.73%)<br>0.093(36.73%) |  |  |\n|               | Dataset-level | FID ↓                     | 386.926                                             | 341.256                                          | 363.262                                          | 381.789                                          | 371.438                                          | 386.340                                          |  |  |\n| Inv-Alignment | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.271(100.00%)<br>11.367(100.00%)<br>0.082(100.00%) | 0.288(50.65%)<br>12.266(50.75%)<br>0.064(50.75%) | 0.272(20.98%)<br>12.558(20.94%)<br>0.058(20.94%) | 0.315(11.25%)<br>13.834(11.39%)<br>0.043(11.39%) | 0.339(8.31%)<br>14.337(8.49%)<br>0.038(8.49%)    | 0.361(5.90%)<br>14.417(6.40%)<br>0.038(6.40%)    |  |  |\n|               | Dataset-level | FID ↓                     | 327.883                                             | 323.892                                          | 316.452                                          | 319.774                                          | 318.895                                          | 315.227                                          |  |  |\n| Bias-Rec      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.039(38.20%)<br>9.783(3.20%)<br>0.105(3.20%)       | 0.040(35.55%)<br>10.211(0.75%)<br>0.096(0.75%)   | 0.043(30.88%)<br>10.249(0.80%)<br>0.095(0.80%)   | 0.044(27.50%)<br>10.311(0.53%)<br>0.093(0.53%)   | 0.045(25.44%)<br>10.222(0.60%)<br>0.095(0.60%)   | 0.047(24.28%)<br>10.354(0.57%)<br>0.093(0.57%)   |  |  |\n|               | Dataset-level | FID ↓                     | 122.096                                             | 107.683                                          | 101.131                                          | 96.742                                           | 96.718                                           | 97.270                                           |  |  |\n| KEDMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.103(100.00%)<br>8.946(100.00%)<br>0.140(100.00%)  | 0.121(58.31%)<br>9.645(55.75%)<br>0.117(55.75%)  | 0.147(35.68%)<br>10.650(31.44%)<br>0.091(31.44%) | 0.158(25.56%)<br>11.180(21.29%)<br>0.081(21.29%) | 0.158(21.55%)<br>11.296(17.67%)<br>0.078(17.67%) | 0.161(19.13%)<br>11.501(14.72%)<br>0.074(14.72%) |  |  |\n|               | Dataset-level | FID ↓                     | 166.503                                             | 145.897                                          | 93.929                                           | 92.886                                           | 77.547                                           | 88.646                                           |  |  |\n| PLGMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.116(100.00%)<br>9.249(100.00%)<br>0.127(100.00%)  | 0.129(63.40%)<br>9.650(60.05%)<br>0.116(60.05%)  | 0.144(40.36%)<br>10.458(34.10%)<br>0.095(34.10%) | 0.158(32.66%)<br>10.831(26.44%)<br>0.087(26.44%) | 0.159(27.13%)<br>11.064(20.74%)<br>0.082(20.74%) | 0.163(24.32%)<br>11.242(19.40%)<br>0.079(19.40%) |  |  |\n|               | Dataset-level | FID ↓                     | 178.288                                             | 269.703                                          | 263.324                                          | 306.025                                          | 212.040                                          | 294.527                                          |  |  |\n| Updates-Leak  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.181(28.00%)<br>12.543(18.00%)<br>0.057(18.00%)    | 0.166(15.00%)<br>12.811(5.00%)<br>0.053(5.00%)   | 0.207(8.00%)<br>12.609(8.00%)<br>0.059(8.00%)    | 0.175(13.00%)<br>12.668(8.00%)<br>0.056(8.00%)   | 0.199(21.00%)<br>12.374(16.00%)<br>0.062(16.00%) | 0.178(8.00%)<br>12.455(4.00%)<br>0.059(4.00%)    |  |  |\n|               | Dataset-level | FID ↓                     | 386.317                                             | 385.654                                          | 387.449                                          | 383.176                                          | 383.946                                          | 388.654                                          |  |  |\n| Deep-Leakage  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.030(40.00%)<br>10.950(2.00%)<br>0.080(2.00%)      | 0.030(47.00%)<br>10.954(2.00%)<br>0.080(2.00%)   | 0.030(42.00%)<br>10.961(2.00%)<br>0.080(2.00%)   | 0.030(48.00%)<br>10.938(2.00%)<br>0.081(2.00%)   | 0.030(48.00%)<br>10.959(2.00%)<br>0.080(2.00%)   | 0.030(50.00%)<br>10.954(2.00%)<br>0.080(2.00%)   |  |  |\n\n<span id=\"page-27-0\"></span>Table 17: Evaluation results of existing reconstruction attacks. The target model is MobileNetV2 trained on CelebA with 6 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance.\n\n| Attack        | Metrics       |        |                 | Target Data Size |                |                |                |                |  |  |  |\n|---------------|---------------|--------|-----------------|------------------|----------------|----------------|----------------|----------------|--|--|--|\n|               |               |        | 1,000           | 2,000            | 5,000          | 10,000         | 15,000         | 20,000         |  |  |  |\n|               | Memorization  |        | 0.999           | 0.984            | 0.871          | 0.560          | 0.374          | 0.317          |  |  |  |\n|               | Dataset-level | FID ↓  | 364.926         | 326.776          | 313.911        | 363.683        | 371.507        | 369.358        |  |  |  |\n| MI-Face       |               | SSIM ↑ | 0.020(100.00%)  | 0.016(69.55%)    | 0.023(56.92%)  | 0.021(58.08%)  | 0.021(57.39%)  | 0.022(56.68%)  |  |  |  |\n|               | Sample-level  | PSNR ↑ | 9.243(100.00%)  | 9.120(50.95%)    | 9.796(22.46%)  | 9.998(12.14%)  | 10.108(8.38%)  | 10.087(6.76%)  |  |  |  |\n|               |               | MSE ↓  | 0.125(100.00%)  | 0.125(50.95%)    | 0.107(22.46%)  | 0.101(12.14%)  | 0.098(8.38%)   | 0.099(6.76%)   |  |  |  |\n|               | Dataset-level | FID ↓  | 284.058         | 283.468          | 291.180        | 255.410        | 269.499        | 265.756        |  |  |  |\n| DeepDream     |               | SSIM ↑ | 0.187(100.00%)  | 0.219(53.75%)    | 0.261(24.90%)  | 0.207(20.34%)  | 0.239(13.76%)  | 0.244(11.86%)  |  |  |  |\n|               | Sample-level  | PSNR ↑ | 10.452(100.00%) | 11.288(52.50%)   | 12.172(24.84%) | 11.980(13.17%) | 12.434(8.53%)  | 12.957(6.64%)  |  |  |  |\n|               |               | MSE ↓  | 0.101(100.00%)  | 0.082(52.50%)    | 0.065(24.84%)  | 0.066(13.17%)  | 0.059(8.53%)   | 0.053(6.64%)   |  |  |  |\n|               | Dataset-level | FID ↓  | 290.997         | 273.416          | 229.688        | 243.607        | 243.291        | 262.318        |  |  |  |\n| DeepInversion |               | SSIM ↑ | 0.098(100.00%)  | 0.105(62.05%)    | 0.120(41.66%)  | 0.130(30.74%)  | 0.125(25.49%)  | 0.117(26.37%)  |  |  |  |\n|               | Sample-level  | PSNR ↑ | 9.796(100.00%)  | 10.056(54.55%)   | 10.521(30.44%) | 10.569(18.45%) | 10.855(11.89%) | 11.110(9.42%)  |  |  |  |\n|               |               | MSE ↓  | 0.114(100.00%)  | 0.105(54.55%)    | 0.093(30.44%)  | 0.091(18.45%)  | 0.085(11.89%)  | 0.079(9.42%)   |  |  |  |\n|               | Dataset-level | FID ↓  | 114.623         | 100.995          | 94.381         | 92.208         | 92.231         | 91.915         |  |  |  |\n| Revealer      |               | SSIM ↑ | 0.107(100.00%)  | 0.119(67.00%)    | 0.138(50.66%)  | 0.151(45.49%)  | 0.159(42.07%)  | 0.166(39.81%)  |  |  |  |\n|               | Sample-level  | PSNR ↑ | 9.028(100.00%)  | 9.412(61.70%)    | 9.950(45.40%)  | 10.243(38.88%) | 10.397(36.39%) | 10.569(33.92%) |  |  |  |\n|               |               | MSE ↓  | 0.135(100.00%)  | 0.121(61.70%)    | 0.106(45.40%)  | 0.099(38.88%)  | 0.096(36.39%)  | 0.092(33.92%)  |  |  |  |\n|               | Dataset-level | FID ↓  | 353.581         | 340.666          | 354.830        | 472.561        | 395.229        | 408.607        |  |  |  |\n| Inv-Alignment |               | SSIM ↑ | 0.248(100.00%)  | 0.278(51.45%)    | 0.263(22.56%)  | 0.280(12.49%)  | 0.310(8.29%)   | 0.325(7.35%)   |  |  |  |\n|               | Sample-level  | PSNR ↑ | 11.171(100.00%) | 12.329(51.10%)   | 13.308(21.78%) | 13.559(13.04%) | 14.105(8.89%)  | 14.404(7.61%)  |  |  |  |\n|               |               | MSE ↓  | 0.086(100.00%)  | 0.064(51.10%)    | 0.049(21.78%)  | 0.046(13.04%)  | 0.040(8.89%)   | 0.037(7.61%)   |  |  |  |\n|               | Dataset-level | FID ↓  | 327.883         | 323.892          | 316.452        | 319.774        | 318.895        | 315.227        |  |  |  |\n| Bias-Rec      |               | SSIM ↑ | 0.039(38.20%)   | 0.040(35.55%)    | 0.043(30.88%)  | 0.044(27.50%)  | 0.045(25.44%)  | 0.047(24.28%)  |  |  |  |\n|               | Sample-level  | PSNR ↑ | 9.783(3.20%)    | 10.211(0.75%)    | 10.249(0.80%)  | 10.311(0.53%)  | 10.222(0.60%)  | 10.354(0.57%)  |  |  |  |\n|               |               | MSE ↓  | 0.105(3.20%)    | 0.096(0.75%)     | 0.095(0.80%)   | 0.093(0.53%)   | 0.095(0.60%)   | 0.093(0.57%)   |  |  |  |\n|               | Dataset-level | FID ↓  | 117.688         | 107.536          | 101.380        | 94.490         | 95.751         | 93.720         |  |  |  |\n| KEDMI         |               | SSIM ↑ | 0.117(100.00%)  | 0.121(57.25%)    | 0.141(33.54%)  | 0.159(25.58%)  | 0.164(22.20%)  | 0.169(19.29%)  |  |  |  |\n|               | Sample-level  | PSNR ↑ | 9.386(100.00%)  | 9.782(55.05%)    | 10.817(29.94%) | 11.290(21.14%) | 11.481(17.86%) | 11.665(14.80%) |  |  |  |\n|               |               | MSE ↓  | 0.126(100.00%)  | 0.112(55.05%)    | 0.087(29.94%)  | 0.078(21.14%)  | 0.075(17.86%)  | 0.072(14.80%)  |  |  |  |\n|               | Dataset-level | FID ↓  | 117.919         | 114.870          | 89.353         | 81.834         | 84.404         | 81.208         |  |  |  |\n| PLGMI         |               | SSIM ↑ | 0.111(100.00%)  | 0.126(61.40%)    | 0.147(37.86%)  | 0.151(26.97%)  | 0.155(22.27%)  | 0.159(19.51%)  |  |  |  |\n|               | Sample-level  | PSNR ↑ | 9.706(100.00%)  | 10.001(57.05%)   | 10.673(32.36%) | 11.066(20.47%) | 11.321(16.83%) | 11.502(13.80%) |  |  |  |\n|               |               | MSE ↓  | 0.116(100.00%)  | 0.106(57.05%)    | 0.090(32.36%)  | 0.082(20.47%)  | 0.077(16.83%)  | 0.074(13.80%)  |  |  |  |\n|               | Dataset-level | FID ↓  | 182.862         | 184.168          | 181.001        | 181.065        | 188.434        | 331.510        |  |  |  |\n| Updates-Leak  |               | SSIM ↑ | 0.182(26.00%)   | 0.181(27.00%)    | 0.191(31.00%)  | 0.178(27.00%)  | 0.188(31.00%)  | 0.166(20.00%)  |  |  |  |\n|               | Sample-level  | PSNR ↑ | 12.671(20.00%)  | 12.779(17.00%)   | 12.708(20.00%) | 12.630(22.00%) | 12.697(18.00%) | 12.838(11.00%) |  |  |  |\n|               |               | MSE ↓  | 0.055(20.00%)   | 0.054(17.00%)    | 0.055(20.00%)  | 0.056(22.00%)  | 0.055(18.00%)  | 0.053(11.00%)  |  |  |  |\n|               | Dataset-level | FID ↓  | 385.713         | 380.721          | 364.624        | 370.025        | 372.819        | 378.168        |  |  |  |\n| Deep-Leakage  |               | SSIM ↑ | 0.047(55.00%)   | 0.051(44.00%)    | 0.039(52.00%)  | 0.045(55.00%)  | 0.038(45.00%)  | 0.034(47.00%)  |  |  |  |\n|               | Sample-level  | PSNR ↑ | 11.186(9.00%)   | 11.246(11.00%)   | 11.149(4.00%)  | 11.232(6.00%)  | 11.181(4.00%)  | 11.132(3.00%)  |  |  |  |\n|               |               | MSE ↓  | 0.076(9.00%)    | 0.076(11.00%)    | 0.077(4.00%)   | 0.076(6.00%)   | 0.076(4.00%)   | 0.077(3.00%)   |  |  |  |\n\nTable 18: Evaluation results of existing reconstruction attacks. The target model is ResNet-18 trained on CelebA with 6 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance.\n\n| Attack        | Metrics       |        | Target Data Size |                |                |                |                |                |  |  |\n|---------------|---------------|--------|------------------|----------------|----------------|----------------|----------------|----------------|--|--|\n|               |               |        | 100              | 500            | 1,000          | 5,000          | 10,000         | 20,000         |  |  |\n|               | Memorization  |        | 0.512            | 0.339          | 0.289          | 0.140          | 0.090          | 0.080          |  |  |\n|               | Dataset-level | FID ↓  | 402.753          | 392.789        | 384.108        | 384.251        | 392.885        | 389.169        |  |  |\n| MI-Face       |               | SSIM ↑ | 0.038(58.00%)    | 0.048(54.20%)  | 0.054(48.70%)  | 0.058(40.64%)  | 0.060(38.09%)  | 0.064(35.38%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 10.924(11.00%)   | 11.240(4.00%)  | 11.334(1.90%)  | 11.487(0.64%)  | 11.364(0.41%)  | 11.474(0.26%)  |  |  |\n|               |               | MSE ↓  | 0.083(11.00%)    | 0.076(4.00%)   | 0.074(1.90%)   | 0.071(0.64%)   | 0.073(0.41%)   | 0.071(0.26%)   |  |  |\n|               | Dataset-level | FID ↓  | 466.706          | 411.230        | 421.903        | 313.134        | 322.337        | 337.856        |  |  |\n| DeepDream     |               | SSIM ↑ | 0.391(7.00%)     | 0.459(2.00%)   | 0.501(1.00%)   | 0.614(0.24%)   | 0.653(0.12%)   | 0.688(0.07%)   |  |  |\n|               | Sample-level  | PSNR ↑ | 16.442(9.00%)    | 18.796(3.80%)  | 19.278(2.00%)  | 21.654(0.56%)  | 22.012(0.45%)  | 22.620(0.33%)  |  |  |\n|               |               | MSE ↓  | 0.032(9.00%)     | 0.017(3.80%)   | 0.018(2.00%)   | 0.008(0.56%)   | 0.007(0.45%)   | 0.007(0.33%)   |  |  |\n|               | Dataset-level | FID ↓  | 289.379          | 206.482        | 161.931        | 145.367        | 129.795        | 131.545        |  |  |\n| DeepInversion |               | SSIM ↑ | 0.143(51.00%)    | 0.176(38.60%)  | 0.190(35.20%)  | 0.195(19.16%)  | 0.210(18.59%)  | 0.208(19.58%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 12.259(34.00%)   | 12.646(22.60%) | 12.705(18.80%) | 12.303(8.24%)  | 12.919(5.62%)  | 13.428(2.82%)  |  |  |\n|               |               | MSE ↓  | 0.063(34.00%)    | 0.059(22.60%)  | 0.058(18.80%)  | 0.061(8.24%)   | 0.053(5.62%)   | 0.047(2.82%)   |  |  |\n|               | Dataset-level | FID ↓  | 263.823          | 194.916        | 173.920        | 148.330        | 142.888        | 142.958        |  |  |\n| Revealer      |               | SSIM ↑ | 0.176(51.00%)    | 0.205(41.40%)  | 0.215(36.00%)  | 0.254(27.16%)  | 0.264(23.60%)  | 0.280(20.13%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 11.127(43.00%)   | 11.191(35.00%) | 11.200(34.00%) | 11.713(24.32%) | 11.724(21.60%) | 11.933(18.96%) |  |  |\n|               |               | MSE ↓  | 0.083(43.00%)    | 0.083(35.00%)  | 0.084(34.00%)  | 0.075(24.32%)  | 0.075(21.60%)  | 0.071(18.96%)  |  |  |\n|               | Dataset-level | FID ↓  | 404.703          | 406.386        | 353.662        | 383.719        | 357.323        | 345.400        |  |  |\n| Inv-Alignment |               | SSIM ↑ | 0.278(6.00%)     | 0.361(1.00%)   | 0.362(0.50%)   | 0.421(0.16%)   | 0.453(0.06%)   | 0.445(0.03%)   |  |  |\n|               | Sample-level  | PSNR ↑ | 16.326(6.00%)    | 18.294(1.00%)  | 18.556(0.70%)  | 19.124(0.14%)  | 19.648(0.11%)  | 19.857(0.06%)  |  |  |\n|               |               | MSE ↓  | 0.024(6.00%)     | 0.016(1.00%)   | 0.015(0.70%)   | 0.013(0.14%)   | 0.011(0.11%)   | 0.011(0.06%)   |  |  |\n|               | Dataset-level | FID ↓  | 413.533          | 373.631        | 365.228        | 357.940        | 361.057        | 367.615        |  |  |\n| Bias-Rec      |               | SSIM ↑ | 0.052(51.00%)    | 0.073(44.40%)  | 0.080(38.60%)  | 0.089(31.32%)  | 0.092(28.75%)  | 0.093(27.77%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 11.732(4.00%)    | 11.556(4.00%)  | 11.318(3.90%)  | 11.664(2.76%)  | 11.690(2.75%)  | 12.057(1.19%)  |  |  |\n|               |               | MSE ↓  | 0.068(4.00%)     | 0.071(4.00%)   | 0.075(3.90%)   | 0.069(2.76%)   | 0.069(2.75%)   | 0.063(1.19%)   |  |  |\n|               | Dataset-level | FID ↓  | 276.379          | 195.312        | 171.608        | 152.892        | 132.250        | 128.840        |  |  |\n| KEDMI         |               | SSIM ↑ | 0.175(34.00%)    | 0.221(27.80%)  | 0.255(18.60%)  | 0.240(18.30%)  | 0.252(15.75%)  | 0.265(12.46%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 11.340(25.00%)   | 12.614(19.80%) | 12.682(14.90%) | 12.763(13.24%) | 12.709(12.58%) | 12.678(10.67%) |  |  |\n|               |               | MSE ↓  | 0.082(25.00%)    | 0.058(19.80%)  | 0.058(14.90%)  | 0.056(13.24%)  | 0.057(12.58%)  | 0.058(10.67%)  |  |  |\n|               | Dataset-level | FID ↓  | 247.514          | 203.480        | 182.907        | 128.939        | 123.629        | 123.018        |  |  |\n| PLGMI         |               | SSIM ↑ | 0.207(39.00%)    | 0.212(10.80%)  | 0.235(5.70%)   | 0.252(4.56%)   | 0.268(3.28%)   | 0.271(2.66%)   |  |  |\n|               | Sample-level  | PSNR ↑ | 12.891(25.00%)   | 14.093(5.60%)  | 13.598(4.50%)  | 13.965(5.22%)  | 13.932(2.96%)  | 14.053(2.88%)  |  |  |\n|               |               | MSE ↓  | 0.056(25.00%)    | 0.041(5.60%)   | 0.045(4.50%)   | 0.043(5.22%)   | 0.042(2.96%)   | 0.042(2.88%)   |  |  |\n|               | Dataset-level | FID ↓  | 268.582          | 298.789        | 367.103        | 343.944        | 269.650        | 267.234        |  |  |\n| Updates-Leak  |               | SSIM ↑ | 0.173(47.00%)    | 0.153(29.00%)  | 0.110(11.00%)  | 0.169(9.00%)   | 0.180(39.00%)  | 0.182(49.00%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 12.623(30.00%)   | 11.772(22.00%) | 9.684(10.00%)  | 12.159(7.00%)  | 12.576(39.00%) | 12.791(25.00%) |  |  |\n|               |               | MSE ↓  | 0.057(30.00%)    | 0.075(22.00%)  | 0.110(10.00%)  | 0.065(7.00%)   | 0.058(39.00%)  | 0.056(25.00%)  |  |  |\n|               | Dataset-level | FID ↓  | 417.883          | 416.383        | 409.578        | 418.149        | 416.713        | 413.789        |  |  |\n| Deep-Leakage  |               | SSIM ↑ | 0.053(54.00%)    | 0.054(49.00%)  | 0.057(59.00%)  | 0.054(54.00%)  | 0.054(51.00%)  | 0.054(53.00%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 12.627(2.00%)    | 12.642(2.00%)  | 12.761(6.00%)  | 12.659(2.00%)  | 12.637(2.00%)  | 12.654(2.00%)  |  |  |\n|               |               | MSE ↓  | 0.055(2.00%)     | 0.054(2.00%)   | 0.053(6.00%)   | 0.054(2.00%)   | 0.055(2.00%)   | 0.054(2.00%)   |  |  |\n\nTable 19: Evaluation results of existing reconstruction attacks. The target model is VGG16 trained on CIFAR10 with 6 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance.\n\n|               | Metrics       |                           | Target Data Size                                 |                                                  |                                                  |                                                  |                                                  |                                                  |  |  |\n|---------------|---------------|---------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--|--|\n| Attack        |               |                           | 100                                              | 500                                              | 1,000                                            | 5,000                                            | 10,000                                           | 20,000                                           |  |  |\n|               | Memorization  |                           | 0.613                                            | 0.369                                            | 0.319                                            | 0.140                                            | 0.090                                            | 0.020                                            |  |  |\n|               | Dataset-level | FID ↓                     | 422.825                                          | 399.889                                          | 402.926                                          | 390.659                                          | 392.791                                          | 397.124                                          |  |  |\n| MI-Face       | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.037(60.00%)<br>10.368(15.00%)<br>0.093(15.00%) | 0.049(51.80%)<br>11.117(3.40%)<br>0.078(3.40%)   | 0.049(49.20%)<br>11.015(1.50%)<br>0.079(1.50%)   | 0.057(40.60%)<br>11.358(0.72%)<br>0.073(0.72%)   | 0.060(37.94%)<br>11.333(0.42%)<br>0.074(0.42%)   | 0.062(34.78%)<br>11.287(0.29%)<br>0.075(0.29%)   |  |  |\n|               | Dataset-level | FID ↓                     | 408.101                                          | 371.711                                          | 355.894                                          | 287.888                                          | 287.213                                          | 291.196                                          |  |  |\n| DeepDream     | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.326(7.00%)<br>16.004(9.00%)<br>0.034(9.00%)    | 0.515(2.60%)<br>18.526(5.40%)<br>0.016(5.40%)    | 0.508(1.70%)<br>18.370(3.00%)<br>0.016(3.00%)    | 0.636(0.26%)<br>20.720(0.50%)<br>0.010(0.50%)    | 0.624(0.19%)<br>21.495(0.47%)<br>0.008(0.47%)    | 0.662(0.13%)<br>22.929(0.18%)<br>0.006(0.18%)    |  |  |\n|               | Dataset-level | FID ↓                     | 321.617                                          | 253.638                                          | 243.467                                          | 150.192                                          | 140.885                                          | 139.596                                          |  |  |\n| DeepInversion | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.071(51.00%)<br>11.817(16.00%)<br>0.067(16.00%) | 0.112(38.00%)<br>11.816(19.20%)<br>0.068(19.20%) | 0.126(28.10%)<br>12.722(14.30%)<br>0.056(14.30%) | 0.200(17.76%)<br>12.433(8.70%)<br>0.058(8.70%)   | 0.203(18.32%)<br>12.818(5.18%)<br>0.054(5.18%)   | 0.195(24.82%)<br>13.038(6.38%)<br>0.051(6.38%)   |  |  |\n|               | Dataset-level | FID ↓                     | 277.745                                          | 200.123                                          | 174.551                                          | 147.235                                          | 144.788                                          | 147.336                                          |  |  |\n| Revealer      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.153(52.00%)<br>10.703(44.00%)<br>0.092(44.00%) | 0.198(40.40%)<br>10.925(31.80%)<br>0.088(31.80%) | 0.211(37.80%)<br>10.956(32.80%)<br>0.088(32.80%) | 0.246(28.28%)<br>11.487(25.46%)<br>0.078(25.46%) | 0.257(24.36%)<br>11.502(22.44%)<br>0.078(22.44%) | 0.273(21.45%)<br>12.219(18.57%)<br>0.067(18.57%) |  |  |\n| Inv-Alignment | Dataset-level | FID ↓                     | 367.057                                          | 374.952                                          | 364.608                                          | 354.472                                          | 374.362                                          | 382.299                                          |  |  |\n|               | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.280(6.00%)<br>16.266(5.00%)<br>0.025(5.00%)    | 0.345(1.00%)<br>17.797(1.00%)<br>0.018(1.00%)    | 0.361(0.50%)<br>18.422(0.70%)<br>0.016(0.70%)    | 0.420(0.10%)<br>18.889(0.14%)<br>0.013(0.14%)    | 0.439(0.07%)<br>19.521(0.08%)<br>0.012(0.08%)    | 0.434(0.04%)<br>19.484(0.03%)<br>0.011(0.03%)    |  |  |\n|               | Dataset-level | FID ↓                     | 413.533                                          | 373.631                                          | 365.228                                          | 357.940                                          | 361.057                                          | 367.615                                          |  |  |\n| Bias-Rec      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.052(51.00%)<br>11.732(4.00%)<br>0.068(4.00%)   | 0.073(44.40%)<br>11.556(4.00%)<br>0.071(4.00%)   | 0.080(38.60%)<br>11.318(3.90%)<br>0.075(3.90%)   | 0.089(31.32%)<br>11.664(2.76%)<br>0.069(2.76%)   | 0.092(28.75%)<br>11.690(2.75%)<br>0.069(2.75%)   | 0.093(27.77%)<br>12.057(1.19%)<br>0.063(1.19%)   |  |  |\n|               | Dataset-level | FID ↓                     | 263.430                                          | 186.165                                          | 174.264                                          | 140.003                                          | 128.682                                          | 124.387                                          |  |  |\n| KEDMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.174(38.00%)<br>11.615(26.00%)<br>0.075(26.00%) | 0.215(29.00%)<br>11.689(21.00%)<br>0.073(21.00%) | 0.246(19.90%)<br>12.551(16.30%)<br>0.059(16.30%) | 0.247(20.34%)<br>12.681(14.44%)<br>0.057(14.44%) | 0.246(16.59%)<br>12.273(12.93%)<br>0.064(12.93%) | 0.277(10.90%)<br>12.893(9.45%)<br>0.055(9.45%)   |  |  |\n|               | Dataset-level | FID ↓                     | 259.366                                          | 189.079                                          | 211.715                                          | 209.681                                          | 218.543                                          | 231.811                                          |  |  |\n| PLGMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.192(37.00%)<br>12.481(29.00%)<br>0.065(29.00%) | 0.228(11.20%)<br>13.432(8.00%)<br>0.048(8.00%)   | 0.233(3.10%)<br>14.113(2.30%)<br>0.040(2.30%)    | 0.250(0.50%)<br>14.612(0.34%)<br>0.036(0.34%)    | 0.271(0.34%)<br>14.916(0.36%)<br>0.033(0.36%)    | 0.268(0.10%)<br>14.652(0.09%)<br>0.035(0.09%)    |  |  |\n|               | Dataset-level | FID ↓                     | 281.515                                          | 381.355                                          | 346.614                                          | 282.818                                          | 326.718                                          | 349.654                                          |  |  |\n| Updates-Leak  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.150(42.00%)<br>11.327(30.00%)<br>0.077(30.00%) | 0.158(6.00%)<br>11.599(4.00%)<br>0.074(4.00%)    | 0.155(9.00%)<br>11.270(10.00%)<br>0.076(10.00%)  | 0.179(37.00%)<br>12.759(25.00%)<br>0.056(25.00%) | 0.159(8.00%)<br>11.279(13.00%)<br>0.079(13.00%)  | 0.157(9.00%)<br>12.672(10.00%)<br>0.057(10.00%)  |  |  |\n|               | Dataset-level | FID ↓                     | 418.202                                          | 412.582                                          | 419.318                                          | 415.702                                          | 419.419                                          | 416.522                                          |  |  |\n| Deep-Leakage  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.053(57.00%)<br>12.647(2.00%)<br>0.054(2.00%)   | 0.054(55.00%)<br>12.689(2.00%)<br>0.054(2.00%)   | 0.053(51.00%)<br>12.662(2.00%)<br>0.054(2.00%)   | 0.054(56.00%)<br>12.674(3.00%)<br>0.054(3.00%)   | 0.055(54.00%)<br>12.639(2.00%)<br>0.054(2.00%)   | 0.054(53.00%)<br>12.632(2.00%)<br>0.055(2.00%)   |  |  |\n\nTable 20: Evaluation results of existing reconstruction attacks. The target model is MobileNetV2 trained on CIFAR10 with 6 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance.\n\n|               | Metrics       |                           | Target Data Size                                 |                                                  |                                                  |                                                  |                                                  |                                                  |  |  |\n|---------------|---------------|---------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--|--|\n| Attack        |               |                           | 100                                              | 500                                              | 1,000                                            | 5,000                                            | 10,000                                           | 20,000                                           |  |  |\n|               | Memorization  |                           | 0.504                                            | 0.305                                            | 0.349                                            | 0.140                                            | 0.100                                            | 0.050                                            |  |  |\n|               | Dataset-level | FID ↓                     | 410.967                                          | 390.645                                          | 388.878                                          | 395.207                                          | 392.996                                          | 396.056                                          |  |  |\n| MI-Face       | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.038(58.00%)<br>10.628(16.00%)<br>0.087(16.00%) | 0.051(50.40%)<br>11.295(3.00%)<br>0.075(3.00%)   | 0.054(49.00%)<br>11.378(1.70%)<br>0.073(1.70%)   | 0.057(40.72%)<br>11.355(0.66%)<br>0.074(0.66%)   | 0.060(37.79%)<br>11.339(0.43%)<br>0.074(0.43%)   | 0.062(34.56%)<br>11.360(0.28%)<br>0.073(0.28%)   |  |  |\n|               | Dataset-level | FID ↓                     | 487.742                                          | 456.497                                          | 407.365                                          | 313.767                                          | 316.199                                          | 285.730                                          |  |  |\n| DeepDream     | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.376(5.00%)<br>14.794(9.00%)<br>0.036(9.00%)    | 0.469(2.00%)<br>18.432(4.60%)<br>0.020(4.60%)    | 0.523(1.00%)<br>18.962(2.40%)<br>0.015(2.40%)    | 0.624(0.32%)<br>21.145(0.74%)<br>0.009(0.74%)    | 0.661(0.12%)<br>21.687(0.42%)<br>0.008(0.42%)    | 0.685(0.69%)<br>22.583(1.82%)<br>0.006(1.82%)    |  |  |\n|               | Dataset-level | FID ↓                     | 270.052                                          | 192.866                                          | 164.677                                          | 130.302                                          | 122.753                                          | 128.291                                          |  |  |\n| DeepInversion | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.140(49.00%)<br>11.896(29.00%)<br>0.069(29.00%) | 0.183(37.20%)<br>12.576(22.60%)<br>0.059(22.60%) | 0.200(34.20%)<br>12.593(16.30%)<br>0.058(16.30%) | 0.212(16.58%)<br>12.852(5.96%)<br>0.053(5.96%)   | 0.221(20.44%)<br>12.984(7.60%)<br>0.052(7.60%)   | 0.229(46.04%)<br>13.989(13.64%)<br>0.042(13.64%) |  |  |\n|               | Dataset-level | FID ↓                     | 280.891                                          | 195.987                                          | 175.441                                          | 149.223                                          | 146.884                                          | 146.078                                          |  |  |\n| Revealer      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.167(48.00%)<br>11.238(41.00%)<br>0.082(41.00%) | 0.201(39.60%)<br>11.315(37.20%)<br>0.081(37.20%) | 0.216(36.20%)<br>11.178(32.40%)<br>0.084(32.40%) | 0.250(27.84%)<br>11.647(24.34%)<br>0.075(24.34%) | 0.265(23.88%)<br>11.857(20.20%)<br>0.072(20.20%) | 0.279(20.19%)<br>12.106(18.19%)<br>0.068(18.19%) |  |  |\n|               | Dataset-level | FID ↓                     | 395.286                                          | 360.147                                          | 352.534                                          | 373.990                                          | 370.574                                          | 373.378                                          |  |  |\n| Inv-Alignment | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.269(7.00%)<br>15.781(5.00%)<br>0.027(5.00%)    | 0.362(1.00%)<br>18.352(1.00%)<br>0.016(1.00%)    | 0.364(0.70%)<br>18.105(0.80%)<br>0.017(0.80%)    | 0.409(0.10%)<br>18.642(0.16%)<br>0.014(0.16%)    | 0.429(0.06%)<br>19.224(0.09%)<br>0.012(0.09%)    | 0.440(0.03%)<br>19.649(0.05%)<br>0.011(0.05%)    |  |  |\n|               | Dataset-level | FID ↓                     | 413.533                                          | 373.631                                          | 365.228                                          | 357.940                                          | 361.057                                          | 367.615                                          |  |  |\n| Bias-Rec      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.052(51.00%)<br>11.732(4.00%)<br>0.068(4.00%)   | 0.073(44.40%)<br>11.556(4.00%)<br>0.071(4.00%)   | 0.080(38.60%)<br>11.318(3.90%)<br>0.075(3.90%)   | 0.089(31.32%)<br>11.664(2.76%)<br>0.069(2.76%)   | 0.092(28.75%)<br>11.690(2.75%)<br>0.069(2.75%)   | 0.093(27.77%)<br>12.057(1.19%)<br>0.063(1.19%)   |  |  |\n|               | Dataset-level | FID ↓                     | 261.661                                          | 202.890                                          | 190.255                                          | 146.832                                          | 140.382                                          | 125.205                                          |  |  |\n| KEDMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.170(30.00%)<br>10.949(20.00%)<br>0.088(20.00%) | 0.207(28.60%)<br>12.110(19.60%)<br>0.066(19.60%) | 0.235(18.50%)<br>12.470(12.60%)<br>0.060(12.60%) | 0.239(19.04%)<br>12.890(13.32%)<br>0.055(13.32%) | 0.266(13.85%)<br>12.504(11.09%)<br>0.060(11.09%) | 0.263(10.30%)<br>12.491(6.98%)<br>0.059(6.98%)   |  |  |\n|               | Dataset-level | FID ↓                     | 272.401                                          | 162.365                                          | 191.645                                          | 209.561                                          | 180.915                                          | 168.016                                          |  |  |\n| PLGMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.172(21.00%)<br>10.596(19.00%)<br>0.109(19.00%) | 0.220(27.00%)<br>13.296(16.20%)<br>0.049(16.20%) | 0.213(5.50%)<br>13.454(4.40%)<br>0.049(4.40%)    | 0.249(0.38%)<br>14.643(0.30%)<br>0.035(0.30%)    | 0.249(0.76%)<br>14.794(0.75%)<br>0.035(0.75%)    | 0.270(0.54%)<br>14.728(0.68%)<br>0.035(0.68%)    |  |  |\n|               | Dataset-level | FID ↓                     | 271.478                                          | 311.422                                          | 348.529                                          | 319.388                                          | 308.801                                          | 265.218                                          |  |  |\n| Updates-Leak  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.168(46.00%)<br>12.745(29.00%)<br>0.056(29.00%) | 0.197(34.00%)<br>13.955(23.00%)<br>0.046(23.00%) | 0.140(15.00%)<br>12.632(8.00%)<br>0.056(8.00%)   | 0.179(21.00%)<br>12.954(14.00%)<br>0.052(14.00%) | 0.103(28.00%)<br>11.368(20.00%)<br>0.078(20.00%) | 0.177(44.00%)<br>12.618(32.00%)<br>0.058(32.00%) |  |  |\n|               | Dataset-level | FID ↓                     | 418.566                                          | 408.233                                          | 417.587                                          | 418.974                                          | 418.767                                          | 410.465                                          |  |  |\n| Deep-Leakage  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.053(55.00%)<br>12.630(2.00%)<br>0.055(2.00%)   | 0.056(58.00%)<br>12.661(2.00%)<br>0.054(2.00%)   | 0.055(58.00%)<br>12.654(2.00%)<br>0.054(2.00%)   | 0.053(61.00%)<br>12.646(2.00%)<br>0.054(2.00%)   | 0.054(55.00%)<br>12.643(2.00%)<br>0.054(2.00%)   | 0.054(58.00%)<br>12.655(2.00%)<br>0.054(2.00%)   |  |  |\n\nTable 21: Evaluation results of existing reconstruction attacks. The target model is ResNet-18 trained on CIFAR10 with 6 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance.\n\n|               | Metrics       |        | Target Data Size |                |                |                |                |                |  |  |\n|---------------|---------------|--------|------------------|----------------|----------------|----------------|----------------|----------------|--|--|\n| Attack        |               |        | 100              | 500            | 1,000          | 5,000          | 10,000         | 20,000         |  |  |\n|               | Memorization  |        | 0.078            | 0.028          | 0.009          | 0.000          | 0.000          | 0.000          |  |  |\n|               | Dataset-level | FID ↓  | 415.896          | 397.556        | 392.129        | 354.141        | 375.405        | 351.019        |  |  |\n| MI-Face       |               | SSIM ↑ | 0.031(68.00%)    | 0.043(48.80%)  | 0.046(49.60%)  | 0.054(41.66%)  | 0.054(39.33%)  | 0.069(33.95%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 5.956(34.00%)    | 6.418(21.80%)  | 6.476(21.40%)  | 6.554(15.12%)  | 6.319(12.78%)  | 6.734(13.96%)  |  |  |\n|               |               | MSE ↓  | 0.255(34.00%)    | 0.229(21.80%)  | 0.226(21.40%)  | 0.222(15.12%)  | 0.234(12.78%)  | 0.214(13.96%)  |  |  |\n|               | Dataset-level | FID ↓  | 333.082          | 348.128        | 369.028        | 309.106        | 308.409        | 298.882        |  |  |\n| DeepDream     |               | SSIM ↑ | 0.022(14.00%)    | 0.139(11.20%)  | 0.145(5.40%)   | 0.044(3.00%)   | 0.048(3.54%)   | 0.052(5.70%)   |  |  |\n|               | Sample-level  | PSNR ↑ | 7.726(6.00%)     | 7.266(4.40%)   | 8.292(4.20%)   | 9.793(4.80%)   | 8.221(1.41%)   | 8.905(1.61%)   |  |  |\n|               |               | MSE ↓  | 0.201(6.00%)     | 0.261(4.40%)   | 0.224(4.20%)   | 0.189(4.80%)   | 0.156(1.41%)   | 0.137(1.61%)   |  |  |\n|               | Dataset-level | FID ↓  | 179.603          | 120.952        | 99.503         | 96.454         | 77.297         | 64.678         |  |  |\n| DeepInversion |               | SSIM ↑ | 0.376(55.00%)    | 0.414(42.40%)  | 0.433(41.60%)  | 0.420(27.50%)  | 0.458(27.80%)  | 0.471(25.91%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 9.735(44.00%)    | 10.404(31.20%) | 10.423(35.00%) | 10.493(22.70%) | 10.894(21.65%) | 11.050(20.55%) |  |  |\n|               |               | MSE ↓  | 0.117(44.00%)    | 0.102(31.20%)  | 0.099(35.00%)  | 0.101(22.70%)  | 0.092(21.65%)  | 0.087(20.55%)  |  |  |\n|               | Dataset-level | FID ↓  | 135.806          | 126.251        | 104.455        | 99.131         | 91.107         | 96.890         |  |  |\n| Revealer      |               | SSIM ↑ | 0.367(54.00%)    | 0.402(41.40%)  | 0.429(39.20%)  | 0.445(30.96%)  | 0.460(29.48%)  | 0.458(25.67%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 9.348(45.00%)    | 9.606(40.00%)  | 10.149(35.10%) | 10.141(29.48%) | 10.363(27.80%) | 10.289(23.99%) |  |  |\n|               |               | MSE ↓  | 0.120(45.00%)    | 0.114(40.00%)  | 0.102(35.10%)  | 0.101(29.48%)  | 0.097(27.80%)  | 0.098(23.99%)  |  |  |\n| Inv-Alignment | Dataset-level | FID ↓  | 341.583          | 409.051        | 354.247        | 360.600        | 388.330        | 324.958        |  |  |\n|               |               | SSIM ↑ | 0.060(9.00%)     | 0.054(2.00%)   | 0.043(1.80%)   | 0.064(0.28%)   | 0.067(0.21%)   | 0.054(0.12%)   |  |  |\n|               | Sample-level  | PSNR ↑ | 7.849(7.00%)     | 8.243(1.40%)   | 6.639(0.80%)   | 7.092(0.18%)   | 8.276(0.08%)   | 5.995(0.07%)   |  |  |\n|               |               | MSE ↓  | 0.165(7.00%)     | 0.150(1.40%)   | 0.217(0.80%)   | 0.196(0.18%)   | 0.149(0.08%)   | 0.252(0.07%)   |  |  |\n|               | Dataset-level | FID ↓  | 412.807          | 396.300        | 391.460        | 370.548        | 382.453        | 381.468        |  |  |\n| Bias-Rec      |               | SSIM ↑ | 0.051(40.00%)    | 0.062(44.00%)  | 0.067(43.00%)  | 0.077(32.40%)  | 0.078(32.65%)  | 0.081(30.37%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 4.956(32.00%)    | 4.941(31.80%)  | 5.013(27.50%)  | 5.060(21.94%)  | 5.066(17.74%)  | 5.130(17.79%)  |  |  |\n|               |               | MSE ↓  | 0.320(32.00%)    | 0.321(31.80%)  | 0.316(27.50%)  | 0.313(21.94%)  | 0.312(17.74%)  | 0.307(17.79%)  |  |  |\n|               | Dataset-level | FID ↓  | 123.429          | 98.651         | 99.726         | 89.443         | 90.610         | 86.584         |  |  |\n| KEDMI         |               | SSIM ↑ | 0.459(38.00%)    | 0.508(33.80%)  | 0.461(8.00%)   | 0.547(25.12%)  | 0.541(22.22%)  | 0.531(16.57%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 10.366(35.00%)   | 11.244(32.20%) | 10.769(6.60%)  | 11.952(21.62%) | 11.891(16.70%) | 11.669(13.64%) |  |  |\n|               |               | MSE ↓  | 0.099(35.00%)    | 0.081(32.20%)  | 0.095(6.60%)   | 0.071(21.62%)  | 0.070(16.70%)  | 0.074(13.64%)  |  |  |\n|               | Dataset-level | FID ↓  | 120.630          | 108.092        | 91.574         | 83.864         | 74.951         | 82.940         |  |  |\n| PLGMI         |               | SSIM ↑ | 0.242(34.00%)    | 0.261(22.20%)  | 0.265(26.20%)  | 0.376(16.12%)  | 0.347(16.08%)  | 0.342(15.17%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 10.266(40.00%)   | 11.106(25.40%) | 11.408(25.10%) | 12.999(15.64%) | 12.940(15.12%) | 12.600(14.93%) |  |  |\n|               |               | MSE ↓  | 0.109(40.00%)    | 0.093(25.40%)  | 0.085(25.10%)  | 0.060(15.64%)  | 0.061(15.12%)  | 0.062(14.93%)  |  |  |\n|               | Dataset-level | FID ↓  | 272.213          | 250.294        | 261.341        | 261.160        | 267.537        | 261.076        |  |  |\n| Updates-Leak  |               | SSIM ↑ | 0.239(50.00%)    | 0.275(42.00%)  | 0.272(48.00%)  | 0.275(44.00%)  | 0.225(48.00%)  | 0.262(50.00%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 10.314(33.00%)   | 10.892(27.00%) | 10.803(34.00%) | 10.843(32.00%) | 10.076(33.00%) | 10.682(36.00%) |  |  |\n|               |               | MSE ↓  | 0.102(33.00%)    | 0.089(27.00%)  | 0.090(34.00%)  | 0.091(32.00%)  | 0.107(33.00%)  | 0.094(36.00%)  |  |  |\n|               | Dataset-level | FID ↓  | 446.372          | 446.842        | 430.602        | 452.186        | 456.572        | 452.086        |  |  |\n| Deep-Leakage  |               | SSIM ↑ | 0.037(57.00%)    | 0.036(59.00%)  | 0.032(64.00%)  | 0.036(57.00%)  | 0.036(59.00%)  | 0.036(54.00%)  |  |  |\n|               | Sample-level  | PSNR ↑ | 5.231(35.00%)    | 5.278(40.00%)  | 6.557(38.00%)  | 5.220(37.00%)  | 5.262(40.00%)  | 5.225(40.00%)  |  |  |\n|               |               | MSE ↓  | 0.300(35.00%)    | 0.298(40.00%)  | 0.244(38.00%)  | 0.301(37.00%)  | 0.298(40.00%)  | 0.300(40.00%)  |  |  |\n\nTable 22: Evaluation results of existing reconstruction attacks. The target model is VGG16 trained on MNIST with 6 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance.\n\n|               | Metrics       |                           | Target Data Size                                 |                                                  |                                                  |                                                  |                                                  |                                                  |  |  |\n|---------------|---------------|---------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--|--|\n| Attack        |               |                           | 100                                              | 500                                              | 1,000                                            | 5,000                                            | 10,000                                           | 20,000                                           |  |  |\n|               | Memorization  |                           | 0.059                                            | 0.038                                            | 0.040                                            | 0.009                                            | 0.000                                            | 0.000                                            |  |  |\n|               | Dataset-level | FID ↓                     | 443.258                                          | 418.851                                          | 414.673                                          | 405.400                                          | 394.948                                          | 397.093                                          |  |  |\n| MI-Face       | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.031(54.00%)<br>5.908(32.00%)<br>0.257(32.00%)  | 0.038(56.40%)<br>6.044(21.60%)<br>0.249(21.60%)  | 0.043(53.60%)<br>6.425(18.50%)<br>0.229(18.50%)  | 0.050(45.44%)<br>6.342(13.42%)<br>0.233(13.42%)  | 0.052(39.79%)<br>6.389(11.55%)<br>0.230(11.55%)  | 0.055(38.14%)<br>6.299(9.26%)<br>0.235(9.26%)    |  |  |\n|               | Dataset-level | FID ↓                     | 341.891                                          | 325.367                                          | 359.402                                          | 254.154                                          | 293.305                                          | 331.156                                          |  |  |\n| DeepDream     | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.035(42.00%)<br>7.116(19.00%)<br>0.248(19.00%)  | 0.034(22.20%)<br>6.947(13.40%)<br>0.218(13.40%)  | 0.027(11.10%)<br>6.233(4.90%)<br>0.244(4.90%)    | 0.041(4.96%)<br>10.530(0.84%)<br>0.100(0.84%)    | 0.029(2.68%)<br>10.697(0.62%)<br>0.102(0.62%)    | 0.040(2.82%)<br>7.391(1.33%)<br>0.187(1.33%)     |  |  |\n|               | Dataset-level | FID ↓                     | 335.001                                          | 234.491                                          | 203.541                                          | 87.943                                           | 139.181                                          | 90.703                                           |  |  |\n| DeepInversion | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.064(49.00%)<br>7.776(41.00%)<br>0.182(41.00%)  | 0.098(47.00%)<br>8.942(28.60%)<br>0.139(28.60%)  | 0.185(34.20%)<br>9.620(30.00%)<br>0.119(30.00%)  | 0.387(33.76%)<br>10.353(25.56%)<br>0.099(25.56%) | 0.342(24.17%)<br>10.303(20.50%)<br>0.100(20.50%) | 0.429(26.20%)<br>10.934(21.38%)<br>0.091(21.38%) |  |  |\n|               | Dataset-level | FID ↓                     | 148.591                                          | 114.421                                          | 111.391                                          | 96.699                                           | 94.908                                           | 94.310                                           |  |  |\n| Revealer      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.365(54.00%)<br>9.180(45.00%)<br>0.125(45.00%)  | 0.399(39.60%)<br>9.622(34.40%)<br>0.113(34.40%)  | 0.417(39.50%)<br>9.864(35.20%)<br>0.107(35.20%)  | 0.445(32.06%)<br>10.207(29.46%)<br>0.100(29.46%) | 0.450(29.60%)<br>10.221(27.16%)<br>0.100(27.16%) | 0.460(26.63%)<br>10.397(24.33%)<br>0.096(24.33%) |  |  |\n| Inv-Alignment | Dataset-level | FID ↓                     | 339.009                                          | 344.593                                          | 359.705                                          | 362.586                                          | 348.022                                          | 354.774                                          |  |  |\n|               | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.043(9.00%)<br>6.108(8.00%)<br>0.247(8.00%)     | 0.047(2.40%)<br>6.878(1.40%)<br>0.205(1.40%)     | 0.043(1.60%)<br>6.367(1.30%)<br>0.233(1.30%)     | 0.062(0.36%)<br>7.490(0.18%)<br>0.179(0.18%)     | 0.061(0.20%)<br>7.070(0.08%)<br>0.197(0.08%)     | 0.064(0.07%)<br>6.905(0.05%)<br>0.205(0.05%)     |  |  |\n|               | Dataset-level | FID ↓                     | 412.807                                          | 396.300                                          | 391.460                                          | 370.548                                          | 382.453                                          | 381.468                                          |  |  |\n| Bias-Rec      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.051(40.00%)<br>4.956(32.00%)<br>0.320(32.00%)  | 0.062(44.00%)<br>4.941(31.80%)<br>0.321(31.80%)  | 0.067(43.00%)<br>5.013(27.50%)<br>0.316(27.50%)  | 0.077(32.40%)<br>5.060(21.94%)<br>0.313(21.94%)  | 0.078(32.65%)<br>5.066(17.74%)<br>0.312(17.74%)  | 0.081(30.37%)<br>5.130(17.79%)<br>0.307(17.79%)  |  |  |\n|               | Dataset-level | FID ↓                     | 125.799                                          | 98.177                                           | 94.378                                           | 90.395                                           | 87.378                                           | 84.559                                           |  |  |\n| KEDMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.428(45.00%)<br>10.140(40.00%)<br>0.104(40.00%) | 0.450(30.20%)<br>10.493(25.60%)<br>0.098(25.60%) | 0.488(32.00%)<br>11.248(26.70%)<br>0.086(26.70%) | 0.515(23.44%)<br>11.708(19.68%)<br>0.078(19.68%) | 0.562(19.60%)<br>12.444(16.62%)<br>0.067(16.62%) | 0.547(15.06%)<br>12.359(12.49%)<br>0.070(12.49%) |  |  |\n|               | Dataset-level | FID ↓                     | 124.465                                          | 87.185                                           | 95.126                                           | 85.172                                           | 89.048                                           | 77.150                                           |  |  |\n| PLGMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.199(44.00%)<br>10.194(43.00%)<br>0.112(43.00%) | 0.214(30.80%)<br>10.838(24.60%)<br>0.091(24.60%) | 0.254(24.00%)<br>10.802(23.20%)<br>0.094(23.20%) | 0.379(19.54%)<br>12.581(18.26%)<br>0.070(18.26%) | 0.307(15.44%)<br>12.271(14.38%)<br>0.067(14.38%) | 0.348(12.87%)<br>12.869(12.01%)<br>0.062(12.01%) |  |  |\n|               | Dataset-level | FID ↓                     | 259.074                                          | 259.886                                          | 259.870                                          | 256.958                                          | 262.431                                          | 272.638                                          |  |  |\n| Updates-Leak  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.254(49.00%)<br>10.532(33.00%)<br>0.096(33.00%) | 0.251(49.00%)<br>10.389(39.00%)<br>0.103(39.00%) | 0.258(46.00%)<br>10.474(30.00%)<br>0.100(30.00%) | 0.258(52.00%)<br>10.577(31.00%)<br>0.096(31.00%) | 0.257(37.00%)<br>10.637(34.00%)<br>0.095(34.00%) | 0.224(49.00%)<br>10.035(31.00%)<br>0.104(31.00%) |  |  |\n|               | Dataset-level | FID ↓                     | 450.400                                          | 453.644                                          | 456.467                                          | 438.643                                          | 448.958                                          | 456.391                                          |  |  |\n| Deep-Leakage  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.034(57.00%)<br>5.698(37.00%)<br>0.270(37.00%)  | 0.036(60.00%)<br>5.223(37.00%)<br>0.301(37.00%)  | 0.036(60.00%)<br>5.237(43.00%)<br>0.300(43.00%)  | 0.033(65.00%)<br>6.043(42.00%)<br>0.265(42.00%)  | 0.034(53.00%)<br>5.412(44.00%)<br>0.289(44.00%)  | 0.036(56.00%)<br>5.229(38.00%)<br>0.299(38.00%)  |  |  |\n\nTable 23: Evaluation results of existing reconstruction attacks. The target model is MobileNetV2 trained on MNIST with 6 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance.\n\n| Attack        | Metrics       |                           | Target Data Size                                 |                                                  |                                                  |                                                  |                                                  |                                                  |  |  |\n|---------------|---------------|---------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--|--|\n|               |               |                           | 100                                              | 500                                              | 1,000                                            | 5,000                                            | 10,000                                           | 20,000                                           |  |  |\n|               | Memorization  |                           | 0.045                                            | 0.031                                            | 0.025                                            | 0.016                                            | 0.009                                            | 0.008                                            |  |  |\n|               | Dataset-level | FID ↓                     | 422.880                                          | 428.743                                          | 411.834                                          | 356.822                                          | 371.393                                          | 380.956                                          |  |  |\n| MI-Face       | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.036(59.00%)<br>6.250(37.00%)<br>0.238(37.00%)  | 0.043(52.00%)<br>6.305(21.60%)<br>0.235(21.60%)  | 0.047(50.50%)<br>6.425(19.60%)<br>0.228(19.60%)  | 0.061(38.18%)<br>6.475(15.48%)<br>0.226(15.48%)  | 0.063(36.32%)<br>6.491(13.22%)<br>0.225(13.22%)  | 0.063(35.86%)<br>6.544(11.50%)<br>0.222(11.50%)  |  |  |\n|               | Dataset-level | FID ↓                     | 362.900                                          | 332.212                                          | 342.366                                          | 314.468                                          | 301.383                                          | 318.317                                          |  |  |\n| DeepDream     | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.025(10.00%)<br>7.095(8.00%)<br>0.294(8.00%)    | 0.035(5.60%)<br>9.428(2.80%)<br>0.143(2.80%)     | 0.120(3.20%)<br>8.582(3.00%)<br>0.201(3.00%)     | 0.152(1.06%)<br>12.336(0.32%)<br>0.096(0.32%)    | 0.035(1.08%)<br>10.763(0.21%)<br>0.095(0.21%)    | 0.095(1.16%)<br>12.237(0.35%)<br>0.077(0.35%)    |  |  |\n|               | Dataset-level | FID ↓                     | 182.303                                          | 153.178                                          | 143.682                                          | 136.023                                          | 114.825                                          | 100.255                                          |  |  |\n| DeepInversion | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.316(59.00%)<br>9.581(53.00%)<br>0.121(53.00%)  | 0.362(46.40%)<br>10.183(36.00%)<br>0.106(36.00%) | 0.356(43.20%)<br>10.408(31.70%)<br>0.100(31.70%) | 0.335(31.00%)<br>10.512(24.60%)<br>0.096(24.60%) | 0.398(31.28%)<br>11.003(23.37%)<br>0.088(23.37%) | 0.438(27.94%)<br>11.354(21.63%)<br>0.082(21.63%) |  |  |\n|               | Dataset-level | FID ↓                     | 145.634                                          | 125.281                                          | 109.221                                          | 95.974                                           | 92.075                                           | 93.991                                           |  |  |\n| Revealer      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.379(51.00%)<br>9.314(49.00%)<br>0.122(49.00%)  | 0.410(42.80%)<br>9.697(36.80%)<br>0.111(36.80%)  | 0.423(40.80%)<br>9.888(36.20%)<br>0.107(36.20%)  | 0.450(33.34%)<br>10.237(30.90%)<br>0.099(30.90%) | 0.457(28.63%)<br>10.317(27.14%)<br>0.098(27.14%) | 0.463(26.63%)<br>10.403(24.80%)<br>0.096(24.80%) |  |  |\n| Inv-Alignment | Dataset-level | FID ↓                     | 383.438                                          | 313.183                                          | 327.018                                          | 363.263                                          | 390.299                                          | 349.382                                          |  |  |\n|               | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.040(10.00%)<br>6.366(5.00%)<br>0.231(5.00%)    | 0.054(1.60%)<br>6.797(1.80%)<br>0.209(1.80%)     | 0.057(0.70%)<br>5.743(1.70%)<br>0.267(1.70%)     | 0.062(1.02%)<br>7.155(0.32%)<br>0.193(0.32%)     | 0.063(0.31%)<br>7.291(0.13%)<br>0.187(0.13%)     | 0.087(0.07%)<br>8.580(0.06%)<br>0.139(0.06%)     |  |  |\n|               | Dataset-level | FID ↓                     | 412.807                                          | 396.300                                          | 391.460                                          | 370.548                                          | 382.453                                          | 381.468                                          |  |  |\n| Bias-Rec      | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.051(40.00%)<br>4.956(32.00%)<br>0.320(32.00%)  | 0.062(44.00%)<br>4.941(31.80%)<br>0.321(31.80%)  | 0.067(43.00%)<br>5.013(27.50%)<br>0.316(27.50%)  | 0.077(32.40%)<br>5.060(21.94%)<br>0.313(21.94%)  | 0.078(32.65%)<br>5.066(17.74%)<br>0.312(17.74%)  | 0.081(30.37%)<br>5.130(17.79%)<br>0.307(17.79%)  |  |  |\n|               | Dataset-level | FID ↓                     | 134.646                                          | 108.765                                          | 101.268                                          | 83.017                                           | 81.788                                           | 83.390                                           |  |  |\n| KEDMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.434(50.00%)<br>10.232(43.00%)<br>0.105(43.00%) | 0.457(34.60%)<br>10.739(26.00%)<br>0.090(26.00%) | 0.515(33.20%)<br>11.176(27.70%)<br>0.082(27.70%) | 0.557(22.60%)<br>12.077(20.54%)<br>0.070(20.54%) | 0.525(5.77%)<br>11.692(4.49%)<br>0.076(4.49%)    | 0.549(19.76%)<br>12.085(16.92%)<br>0.069(16.92%) |  |  |\n|               | Dataset-level | FID ↓                     | 188.569                                          | 181.531                                          | 137.936                                          | 80.253                                           | 87.943                                           | 71.808                                           |  |  |\n| PLGMI         | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.160(11.00%)<br>9.555(12.00%)<br>0.127(12.00%)  | 0.188(4.20%)<br>10.503(3.20%)<br>0.168(3.20%)    | 0.256(9.00%)<br>9.900(10.40%)<br>0.168(10.40%)   | 0.389(11.50%)<br>11.897(11.76%)<br>0.076(11.76%) | 0.371(12.67%)<br>12.764(13.29%)<br>0.067(13.29%) | 0.371(14.32%)<br>12.242(13.68%)<br>0.070(13.68%) |  |  |\n|               | Dataset-level | FID ↓                     | 262.660                                          | 269.299                                          | 267.962                                          | 257.977                                          | 259.761                                          | 253.278                                          |  |  |\n| Updates-Leak  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.251(48.00%)<br>10.598(32.00%)<br>0.094(32.00%) | 0.241(49.00%)<br>10.290(37.00%)<br>0.101(37.00%) | 0.229(54.00%)<br>10.271(40.00%)<br>0.101(40.00%) | 0.253(51.00%)<br>10.470(28.00%)<br>0.099(28.00%) | 0.242(41.00%)<br>10.282(38.00%)<br>0.108(38.00%) | 0.257(42.00%)<br>10.735(32.00%)<br>0.091(32.00%) |  |  |\n|               | Dataset-level | FID ↓                     | 458.451                                          | 455.759                                          | 458.373                                          | 450.504                                          | 456.811                                          | 409.554                                          |  |  |\n| Deep-Leakage  | Sample-level  | SSIM ↑<br>PSNR ↑<br>MSE ↓ | 0.037(57.00%)<br>5.239(43.00%)<br>0.299(43.00%)  | 0.036(53.00%)<br>5.232(37.00%)<br>0.300(37.00%)  | 0.035(57.00%)<br>5.218(31.00%)<br>0.301(31.00%)  | 0.035(56.00%)<br>5.326(37.00%)<br>0.294(37.00%)  | 0.035(56.00%)<br>5.212(34.00%)<br>0.301(34.00%)  | 0.026(57.00%)<br>9.409(10.00%)<br>0.122(10.00%)  |  |  |\n\n<span id=\"page-34-0\"></span>Table 24: Evaluation results of existing reconstruction attacks. The target model is ResNet-18 trained on MNIST with 6 different sizes. Attacks with a gray background belong to the dynamic training type. For FID and MSE, a lower score indicates better reconstruction quality; while for SSIM and PSNR, a higher score indicates better performance.，分析其研究动机、核心方法与公式推导细节。请结合摘要与正文信息，提取论文背景、问题定义、方法核心流程与理论基础。\n",
        "agent": "论文解读专家\n",
        "status": "started"
    }
]
